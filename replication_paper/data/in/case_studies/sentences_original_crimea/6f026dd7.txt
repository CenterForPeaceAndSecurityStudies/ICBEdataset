INFORMATION MANIPULATION A Challenge for Our Democracies A report by the Policy Planning Staff and the Institute for Strategic Research AUTHORS Jean-Baptiste Jeangene Vilmer, Alexandre Escorcia, Marine Guillaume, Janaina Herrera.
ABOUT CAPSAND IRSEM The Policy Planning Staff , known as CAPS, created in 1973, reports to the Minister for Europe and Foreign Affairs.
Composed of around twenty experts, diplomats and academics, CAPS produces trans-disciplinary and forward-looking analyses of mediumand long-term developments in the international arena for the benefit of the French Foreign Minister and French authorities.
It also proposes foreign policy recommendations and strategic options drawn from its own analysis and interactions with the world of think tanks and academic research in the field of international relations.
The Institute for Strategic Research , known as IRSEM, created in 2009, is the research institute of the Ministry for the Armed Forces.
Composed of around forty people, including both civilians and military personnel, the majority of whom hold doctoral degrees, IRSEM's primary mission is to promote French research on defense and security issues.
CAPS and IRSEM each produce independent analyses that do not constitute official positions.
Therefore, the opinions expressed in this report are only those of the authors and are in no way those of the Ministry for Europe and Foreign Affairs, the Ministry for the Armed forces or, a fortiori, the French government.
A Challenge for Our Democracies, report by the Policy Planning Staff of the Ministry for Europe and Foreign Affairs and the Institute for Strategic Research of the Ministry for the Armed Forces, Paris, August 2018.
INFORMATION MANIPULATION A Soviet tradition .51 The evolution of the Russian approach .53 The “newgeneration warfare” .55 “Information warfare” .57 b.
Raise users’ awareness of the risks and challenges of information manipulation.
Contain the dissemination and impact of information manipulation campaigns .148 4.
BIBLIOGRAPHY.195 PRESENTATION OF THE AUTHORS .205 FOREWORD Our Study Our investigation is the product of an awareness of the existential danger that information manipulation poses to our democracies.
Second, the attempted interference in the 2017 French presidential election, culminating with the so-called “Macron Leaks” incident, captured French attention and demonstrated to us the importance of studying this subject.
In September 2017, acting on our own initiative, we decided to set up a joint working group bringing together members of the Policy Planning Staff of the Ministry for Europe and Foreign Affairs and the Institute for Strategic Research of the Ministry for the Armed Forces.
Initially, the purpose of this working group was to explore the possibility of creating an inter-agency task force to combat information manipulation.
But, above all, this group gave itself the task studying the underlying problem, its causes, its consequences and the solutions that can be brought to bear.
This working group was intended to be inter-agency in order to respond to the inherently interdisciplinary nature of information manipulation, which lies at the crossroads between international relations, war studies.
INFORMATION MANIPULATION intelligence studies, media studies, sociology and social psychology.
This working group was designed to study the problem from an international perspective, taking into account not only our national interests, but also the transnational nature of information manipulation, which transcends sovereignty and individual State legal systems.
While certain cases are more well-known than others, the issue of information manipulation is universal.
It is an issue that affects civil society as well as the governments of many States, not only in Europe and North America, but also in Asia, in the Middle East, in Africa and in Latin America.
However, information manipulation is also diverse in nature and each case is different and is tailored to fit different target audiences.
It is important to distinguish between exogenous manipulations, which originate from outside the targeted State, and endogenous manipulations, which originate from within a State.
We must also distinguish between attempts that are caused by state actors and those that are caused by nonstate actors.
Given that it was impossible to cover everything, and given our particular focus on foreign affairs and defense, we chose to limit the scope of this report to the study of information manipulation orchestrated by foreign States, or in other words, foreign interferences.
We produced a dozen internal memos within the relevant ministries and departments, a published research paper 1 and organized several public events including a seminar cycle at the IRSEM on “Information Warfare” and an international symposium organized by the CAPS on 4 April 2018, which had opening remarks given by the Minister for Culture and closing 1.
The Foreign Minister’s closing speech is the longest and most detailed French official statement on the subject of information manipulation to date.
In his speech, the Minister mentioned the present report, which was then in the making, and is the principal result of our research.
It must be noted however that this report does not and should not be considered as reflecting an official position of the French government.
CAPS and IRSEM enjoy a degree of autonomy within their respective ministries.
The 2016 United States and 2017 French elections have shed a critical light on this phenomenon, its mechanisms and consequences.
Yet, the impact of information manipulation—and, in some cases, its very existence—is sometimes called into question.
They require in-depth examination in order to identify as clearly as possible what actually constitutes information manipulation.
The latter term is INFORMATION MANIPULATION 12 understood as the intentional and massive dissemination of false or biased news for hostile political purposes.
Working off of this definition of information manipulation and drawing on our numerous interviews and a thorough review of the abundant literature on the subject, this report proceeds as follows.
Firstly, the report explores the causes of information manipulation, which exist partly at the level of the individual.
After having analyzed each of these causes, we then proceed to identify the beneficiaries of these activities,e. the actors carrying out information manipulation.
We focus specifically on States that manipulate information outside their territory or, in other words, who interfere in the internal affairs of other States.
We also explore information manipulation in regions other than the post-Soviet space, Europe and North America— which are the best known—by turning our attention to several case studies in the Middle East, Africa and Latin America.
We start with a case study on the “Macron Leaks,” which stand apart from the recent history of election meddling precisely because they failed to achieve their intended purpose.
It is, therefore, important to understand why and to draw lessons from this isolated incident.
To conclude, we identify future challenges—technological challenges, future trends in Russian “information warfare” and possible future SUMMARY scenarios.
Lastly, we propose 50 recommendations operating on the assumption that information manipulation will remain a problem in the future and that it will constitute a long-term challenge for our democracies.
In the face of this challenge, democracies must provide a participatory, liberal response that respects fundamental rights.
In closing, we anticipate some of the criticism that our recommendations will receive and note some of our responses to these objections.
Information is increasingly seen as a common good, the protection of which falls into all citizens concerned with the quality of public debate.
Above all, it is the duty of civil society to develop its own resilience.
They should not be in the lead, but their role is nonetheless crucial, for they cannot afford to ignore a threat that undermines the foundations of democracy and national security.
Historian Robert Darnton has shown how fake news benefited from the development of print media, including French and English sensationalist pamphlets (known in France as Parisian canards) of the seventeenth and eighteenth centuries.
The Protocols of the Elders of Zion constitutes one of the first famous examples.
The rise of totalitarianism served as a catalyst and the Cold War, too, had its share of well-known episodes.
Vladimir Volkoff, Petite Histoire de la disinformation , Ed. du Rocher, 1999.
For this reason, clarifying the terminology is an essential prerequisite to our analysis.
In this introduction, we will review the existing terminology and then justify our decision to use the term “information manipulation,” which we believe is best suited to the context.
Our goal is both to define the terms of the debate and to demonstrate the importance of this issue, bearing in mind the effectiveness of and the challenges posed by these kinds of manipulation.
To ensure the study had solid foundations, it was imperative first of all to identify and set aside the most vague and ambiguous of them and find a more precise definition of the phenomenon under consideration. — “Fake news” is the most commonly used expression, even in French, where it is sometimes translated into “fausses informations ” (false information) although it might be more accurate to speak of falsified, counterfeit.
Soviet Bloc Intelligence and Its AIDS Disinformation Campaign,” Studies in Intelligence , 53:4, 2009, 1-24.
The term was popularized in 1999 by The Daily Show , a satirical American show that tampers with information and news for comedic purposes, much like The Onion news journal.
This first generation of fake news, a humoristic one, lasted for around fifteen years.
Secondly, the term has become so overused that it is not uncommon, even for certain heads of State, to use it for all news that they dislike, ultimately enabling a form of populism that is hostile to the freedom of the press. — The notion of “political warfare,” which applies to all non-military and non-lethal operations, and even the sub-field of “information warfare,” are simply too broad.
Furthermore, these terms entail a militarization of information and of the academic literature devoted to this phenomenon.
This critique also extends to “hybrid warfare,” a widespread but confusing notion, which in reality refers to war waged across the full spectrum—from conventional means to information and cyber means, from clandestine operations to nuclear intimidation.
It is, therefore, even broader than the preceding two categories, because hybrid warfare associates non-kinetic elements with kinetic ones. — “Propaganda,” defined as “an attempt to influence the opinion and behavior of society in order for people to adopt a particular opinion and behavior,” * 11 is also too vague.
Above all, it does not apply to our subject because the term propaganda implies the defense of an alternative world view.
This is an element that the current observed phenomena—essentially centered on the denigration of others—appear to lack. — “Influence” and “public diplomacy” are also very broad and, above all, they are not in themselves problematic.
European Commission, A Multi-Dimensional Approach to Disinformation, Report of the Independent High Revel Group on Fake News and Online Disinformation, March 2018, 10.
The distinction proves useful when responding to the line of argument that views RT and Sputnik as merely the Russian equivalents of mainstream media.
The problem, of course, remains that the intention is rarely clear, 13 and can only be assumed.
This definition of the subject is probably the least bad from among the most commonly used terms.
It is too broad in the sense that it includes benign information that lacks a hostile intention.
At the same time, the concept of disinformation is too narrow because the problems that we encounter are not all, strictly speaking, forms of disinformation.
Sometimes information is not false, but simply exaggerated, biased or presented in a very emotional way, such as in a tabloid.
These processes do not all imply a dichotomy between truth and falsehood.
Most of the time, the manipulator does not position himself relative to the truth; he or she is simply trying to produce an effect.
To account for this complexity, some experts, including the European Group of Experts, define disinformation as “false, inaccurate, or 12.
Terms for Problematic Information, Data & Society Research Institute, 2017, 4.
INTRODUCTION misleading information designed, presented and promoted to intentionally cause public harm or for profit” 14 — a definition that has been reused in various publications, including a report by the Irish government and a report from the Belgian Group of Experts.
To us, it seems preferable to use the generic term, “manipulation,” because it is more inclusive.
The notion of a “coordinated campaign” refers less to the idea of an orchestrated operation with certain actors giving orders and others carrying them out.
By selecting this definition, we have chosen to highlight the political intent behind information manipulation campaigns as a defining criterion of the phenomenon.
It does not mean that the field is limited to political or national affairs.
The campaign may seek to undermine the legitimacy of an electoral process, ruin the reputation of a large corporation abroad or create a hostile environment for an external military operation.
We are de facto excluding the numerous information manipulation attempts whose intention is neither political nor hostile from the scope of this study.
However, we must not overemphasize the distinction, as we sometimes do, between commercial manipulation, whose intention is to turn a profit and is thus often depoliticized by those who analyze it, and political manipulation, which is of interest to us here.
Not only can the former have real political consequences, whether intended or not, but the latter may also be used to make money, for the media, digital platforms and even 14.
European Commission, A Multi-Dimensional Approach to Disinformation, op. cit ., 10.
In other words, there is an overlap between political and economic interests.
We have been advocating for the use of the expression “information manipulation” in our internal memoranda since the beginning of 2018.
In 2013, the World Economic Forum listed online “misinformation” as one of the ten trends to watch in 2014' —which proved to be premonitory, given the non-negligible role that information manipulation played in the 22 Ukrainian crisis.
All the polls confirm that it is now a major concern for populations, journalists, NGOs and governments around the world, who recognize the damages these types of manipulations can cause to society.
However, there is also a common tendency to underestimate the effectiveness of information manipulation, and thus the importance of the subject.
This tendency is less pronounced in countries that are traditionally more aware of the issue (Central, Eastern and Northern 16.
One investigation revealed how the city of Veles, in Macedonia, had become a breeding ground for fake news and how the youth had, sometimes without any political motivation, de facto supported Trump in the American campaign, simply after having observed it was that most profitable choice (the pro-Trump content was shared more times, and therefore generated more publicity revenue.
Today, some of them continue to mass produce fake news, but they earn much less because after the whole operation was discovered they were no longer able to sell to Google.
See for example the latest online public consultation conducted by the European Commission on fake news and disinformation online from November 2017 to February 2018 {Synopsis report, 26 April 2018) and Reuters Institute Digital News Report 2018, which surveyed over 74,000 people in 37 States.
However, countries who believe they are insulated, or who know they are targeted but have succeeded in fending off past attempts—such as France in the so-called “Macron Leaks” affair —are susceptible to underestimating the threat.
Strong persuasion is then necessary, sometimes within government itself, and in public debates, to bring about the realization that this is no minor issue.
In order to do so, it could be helpful to state as a reminder, that information manipulation, although appearing virtual, has numerous, very real effects.
In the last few years alone, it has interfered in the democratic processes of multiple states, including the presidential elections of the world’s major powers, and destabilized large digital companies.
Information manipulation has divided public opinion, and sowed doubt as to the veracity of the information provided by the media and reinforced a rejection of traditional media.
It played a role in various diplomatic crises , and has also contributed to the saturation of digital spaces with troll communities that harass and intimidate internet users.
On a smaller scale, in only two months in India, dozens of people were lynched after having false information circulated about them.
As a result, authorities decided to temporarily cut access to some online platforms.
The fact that many States are mobilizing and civil society is launching numerous initiatives to protect itself, as the disinformation economy continues to develop in parallel, with its troll factories, click farms and millionaire entrepreneurs, 22 is an indication of both the gravity and the effectiveness of information manipulation.
Annie Gowen and Max Bearak, “Fake News on Facebook Fans the Flames of Hate Against the Rohingya in Burma,” The Washington Post , 8 December 2017.
May be Indians should not be allowed to use WhatsApp,” India Today , 2 July 2018.
See for example the case of Mexican Carlos Merlo, who claims to control millions of bots and dozens of sites.
His entreprise, Victory Lab provides services such as “managing bots, containment, cyberattacks, and the creation of fake news sites” at prices ranging from 49,000 pesos to a six-month contract at one million pesos per.
See Ben 23 INFORMATION MANIPULATION 24 Nevertheless, the assessment of the effectiveness of information manipulation remains a challenge.
During and after the Cold War, United States intelligence agencies commissioned meticulous surveys in an attempt to accurately measure the permeability of targeted groups to Moscow’s disinformation campaigns.
Another limitation on the methodology is the reliance on textual analysis, whereas information manipulation can also occur through images, which are much more difficult to analyze automatically.
Therefore, while it is crucial that attention be drawn to the role of platforms like Facebook, other networks must also be examined.
Disinformation through images also raises the issue of manipulation aimed at children.
Measuring the effectiveness of information manipulation is almost impossible because the link between a broadcast message and subsequent behavior involves too many other variables.
However, we can distinguish the impact in the digital environment, which is relatively measurable and quantifiable (that is, if we manage to separate the real accounts from increasingly sophisticated bots), from the broader effect, which can merely be hypothesized.
Trending Beyond Borders in Mexico,” Atlantic Council’s Digital Forensic Research Lab, Medium.com, 28 June 2018.
The Special Reports S issued by the intelligence community by the United States Information Agency.
The Department of Defense , the State Department and the CIA, like the USIA previously, consider social science studies an essential tool for the implementation of their respective strategies.
The Office of Research and Intelligence produces dozens of “Special S reports,” and collaborates with many departments and academic research laboratories.
We know that it works, we have seen it at work abroad and also in France.
Democratic process is thus profoundly distorted because the indignation that stems from this fake news is explosive and prevails over reflection.
And that is the somewhat anthropological gamble made by those who manipulate these channels.
The question is whether information manipulation can succeed in generating new opinions or merely reinforce existing ones.
From our investigation, it appears that manipulation does not generate new opinions, but sows doubt and confusion and, sometimes, facilitates action.
In other words, sometimes information manipulation transforms a passive conviction into an active conviction—in a way that is similar to the process of radicalization.
Information manipulation may tempt heads of government to infringe upon civil liberties.
This ultimately contributes to the deepening of divisions within society.
Therefore, it is important for the State to properly monitor its efforts against disinformation so as to respect civil liberties.
It appears, therefore, essential to have the means to conduct independent research into the science of information and communication, which will allow for the evaluation of the reception of information manipulation campaigns.
In the meantime, immediate action is necessary, given the very real effect of these phenomena.
Fighting effectively against information manipulation requires first and foremost to identify the roots of the problem.
There are cognitive weaknesses and a crisis of knowledge that makes us particularly vulnerable to information manipulation.
There are also collective causes, related to the dynamics of social life, a crisis of trust in institutions, a crisis of the press and disillusionment with the digital world.
Indeed, although the internet was supposed to liberate us, we instead find ourselves confined by it.
After analyzing each of these causes, we will identify the beneficiaries,e. the actors conducting information manipulation, focusing in particular on state actors.
Information manipulation is particularly prolific in times of war—and thus benefits all the more from the “despecification” of war, that is, from the increasing ambiguity between times of war and times of peace.
INFORMATION MANIPULATION 30 Marc Bloch on the causes of fake news “The masses are aroused by false stories.
Items of false news, in all the multiplicity of their forms—simple gossip, deceptions, legends—have filled the life of humanity.
The historian who seeks to understand the origin and development of false news, disappointed by the reading of documents, will naturally think of turning to the laboratories of psychologists.
The error propagates itself, grows, and ultimately survives only on one condition—that it finds a favorable cultural broth in the society where it is spreading.
Through it, people unconsciously express all their prejudices, hatreds, fears, all their strong emotions.
Only great collective states of mind—I will have occasion to return to this later—have the power to transform a misperception into a legend. among the questions of social psychology that the events of the last few years can elucidate, those relating to false news are at the forefront.
For four and a half years, everywhere, in every country, at the front as in the rear, we saw them being born and proliferating.
They troubled minds, sometimes overstimulating them and sometimes sapping them of their courage.
Cicero and Quintilian have more disciples in editorial bureaux than we commonly believe. an item of false news always arises from preexisting collective representations.
It is fortuitous only in appearance, or, more correctly, all that is fortuitous about it is the initial incident, something that sets the imagination in motion.
But this setting in motion occurs only because imaginations have already been prepared and are secretly fermenting.
An event or misperception, for example, that does not go in the direction where all minds are already tending can at most constitute the origin of an individual error, not a popular and widespread instance of false news.
Indeed, Bloch’s text is worth revisiting because it shows the extent to which the fundamental elements of the “fake news” debate have not changed.
Causes at the individual level Targeting the individual and the collective at the same time, “modern propaganda is based on scientific analyses of psychology and sociology.
Cognitive failings Disinformation exploits a natural intellectual laziness, characterized by the failure to systematically exercise critical thinking and choosing to relay information naively without looking for evidence to support that information.
Conspiracy theorists demand that we provide evidence that their theories are incorrect and far-fetched, which is contrary to journalistic standards.
As Pascal Engel writes, “reasoning has not evolved in order to establish reality, but only as a means to win against our adversaries.
A recent study showed that fake news spreads faster than accurate news for psychological reasons.
Real news is often not that new; it is often merely a confirmation of what we already know or suspect.
It merely adds to the accumulation of knowledge, and is quickly forgotten.
By contrast, fake news surprises; fake news is written to surprise and to go against the doxa.
It is written in a spectacular, emotional and often alarmist style, playing on fear and anxiety, elements that are generally not as prioritized in the realm of real news.
It is therefore our cognitive biases that contribute in large part to the spread of fake news.
Pascal Engel, “Si on ne peut pas prouver que le monstre du Loch Ness n’existe pas, c’est qu’il existe...,” Liberation , 19 February 2018.
Soroush Vosoughi, Deb Roy and Sinan Aral, “The Spread of true and false news online,” Science , 9 March 2018, 1146-1151.
An epistemological crisis Information manipulation is only one of the manifestations of a much larger phenomenon that integrates pseudoscience—particularly in the fields of medicine and biology—, historical revisionism, and conspiracy theories.
Some countries have been particularly affected, such as Kazakhstan and Indonesia.
Stephane Foucart and David Larousserie, “Alerte mondiale a la fausse science,” Le Monde, 19 July 2018.
How we surrendered to conspiracy theories, quack medicine, bogus science andfake history , Atlantic, 2008.
Understanding and Coping with the ‘Post-Truth’ Era,” Journal of Applied Research in Memory and Cognition 6:4, 2017, 360.
The third is deconstructionism, an approach which, according to the writings of Derrida, seeks to unveil unsaid things.
All in all, the contemporary epistemological crisis lies in the wrong interpretation, the diversion and the simplification of otherwise legitimate approaches.
If the majority of studies, in the United States at least, show that the rightwing is most often—but of course not exclusively—the source of fake and biased news, it is due to the fact that progressive citizens generally consult a larger variety of sources of information and are more trusting towards professional journalism, while conservatives have a stronger tendency to consult sources and networks that conform with their political opinions, and tend to maintain an anti-media, anti-intellectual bias.
According to this theory, “there is a view, which I shall call the conspiracy theory, which holds that the explanation of any social phenomenon consists in finding out who is interested in the occurrence of this phenomenon.
For this reason, conspiracy theories are inevitable and feed mostly on crises and violent events.
But others can have destabilizing effects, even if they are only shared by a very small percentage of the population, as long as that small minority is ready to take violent action.
Radicalism and world views fueled by conspiracy theories often go hand-in-hand.
Most conspiracy theorists are neither foolish nor irrational but simply lack good sources of information.
The theories they defend are unjustified in light of all the information available, but not according to the sources they consult, which make these arguments seem plausible.
The cause of the problem is the epistemic poverty of their environments.
Conspiracy theorists pose a particular difficulty because they are highly resistant to debunking, especially if attempted by the State.
As conspiracy theories hold that certain people have disproportionate power with which to conceal their actions, these attempts can become absorbed into the narrative of the plot.
Hence, we are experiencing a crisis of knowledge, an epistemological crisis—although this is nothing new.
Causes and Cures,” The Journal of Political Philosophy, 17:2, 2009, 219.
Never before today has the distinction between episteme and doxa been so threatened—and through it, the very possibility of knowledge.
The era in which we currently live is very different from the epistemological crises of the past.
We find ourselves not in an ideological era, in which we replace one truth with another, but rather in a skeptical or relativist era, where we question the very existence of truth.
On 16 November 2016, the Oxford dictionaries declared the term “post-truth” to be the word of the year.
The post-truth imposes no particular truth and it is precisely in this way that it sows confusion and doubt.
The crisis of confidence in institutions A survey of more than 33,000 people in 28 countries in November 2017 showed that distrust of institutions was on the rise, with the media ranking first in terms of the least trusted institution, while trust in social media platforms was decreasing, but trust in journalism was increasing.
In addition, almost 70% of the population reported feeling worried about the weaponization of fake news.
It speaks of the treachery of elected officials, the media’s appropriation of public discourse and a number of 21.
Sabrina Tanquerel, “Quand l’exigence de verite devient secondaire,” The Conversation , 12 February 2017.
Sebastian Dieguez, Total Bullshit lAu cceurde la post-verite , op. cit, ., 307.
The depreciation of the truth is one of the manifestations of this crisis of confidence—but at the same time, the devaluation of truth propagates the crisis of confidence.
From the United States to the Philippines to Hungary, hatred for the establishment seems to be a passion that is shared by all populist outsiders, real or purported.
This tendency includes phenomena such as the erection of walls (Israel, the United States, Hungary), the expansion of gated urban communities, the imposition of refugee quotas, etc.
The crisis of the economic model is not new and is largely the result of the decrease in press advertising revenues, due to the competition it first faced from the advent of television, and then later on with the invention of the internet.
Many agencies and news organizations have had to lay off journalists, doll out severance payments and terminate certain news outlets.
This precarious situation renders the press even more vulnerable to information manipulation, because there are less people and less time to detect them and because of the premium placed on quantity rather than quality.
Nevertheless, there is room for hope in this case too, namely because these excesses generate fatigue within the population, which prompts more serious media outlets, eager to demonstrate their added value, to strengthen journalistic norms and to value investigative journalism, which produces papers that are longer and more complex, and sometimes collaborative.
The internet, in particular, became even more of an accelerant in the last decade with the rise of social media.
The digital revolution—in particular, the accessibility of broadband in the last fifteen years—and the subsequent development of social media have changed the playing field.
Social media has become omnipresent in the lives of billions of individuals.
Social networks are used as a source of information by 62% of American adults and 48% of Europeans.
Google and Facebook now account for more than 70% of web traffic, which means that other sites, including news organizations, get most of their audience from these platforms.
Initially, the dramatic growth of social networks—and in particular, the Web 2.0, blogs and citizen journalism—has brought many to believe in the emancipation of the people from their States.
This optimism was followed by disappointment several years later, beginning with the Snowden Affair which revealed that States had not renounced their strong grip over society.
As Ben Nimmo concludes, “the spread of digital publishing technologies has made it easier to create false stories.
It took the KGB nearly four years to spread the rumor that the AIDS virus was created by the Pentagon.
As of 2010, the search results in Google are not the same for all users; they depend on the preferences of the user as deduced from his or her search history and geolocation.
Originally, these algorithms served a commercial purpose by offering the user results as close to their expectations as possible.
But this practice also had the perverse effect of cocooning internet users “in closed cognitive spaces where they were only exposed to content that supported their beliefs.
The problem is that for most users, these platforms are the “gatekeepers” of the web, the access routes to the rest of the internet.
These personalization algorithms close people in cocoons, comfortable cognitive spaces that confirm prejudices rather than confront them with the prejudices of others.
As already stated, we do not like to be contradicted, and these platforms’ content-creating algorithms ensure that we are not, by providing us with information that bolsters our opinions.
Paradoxically, the digital revolution may actually be pushing us to close us back in on ourselves.
We thank the General Secretariat of the French Digital Council for its contribution to the following analysis.
In a famous article entitled “How technology disrupted the truth,” Katharine Viner, the Editor-in-Chief of the Guardian , demonstrated how this occurred in the case of Brexit.
This also creates the phenomenon of “cascading information:” users relay information posted by their close contacts without necessarily checking or even considering whether that information is true.
The more the information is shared, the more we tend to trust it and the less we use critical thinking to assess it.
This phenomenon favors the most interesting or scandalous content because that is most likely to make us react, regardless of truth or accuracy.
This model contributes to the polarization of public opinion by reducing the visibility of nuanced content, which is considered to be less engaging.
Platforms invest huge amounts of money in studies of what it is that attracts our attention and produces weaknesses of will.
For all of these reasons, journalistic ethics, the traceability of sources and the verification of facts are sacrificed in order to make something go viral.
This race to the number of page views, to increase advertising revenues and to attract investors corrupts press companies at the expense of serious journalism.
It encourages lurid titles, sensationalism, clickbait to the detriment of the truth.
The vulnerabilities identified in the previous pages constitute fertile ground for information manipulation.
These vulnerabilities are used by actors who perceive them as opportunities to defend their strategic interests.
Katharine Viner, “How technology disrupted the truth,” The Guardian , 12 July 2016.
See also Matthew Hughes, “How the Internet tricked you into thinking Trump wouldn’t win,” The Next Web , 9 November 2016.
However, we will begin by examining other scenarios of information manipulation.
Non-state actors The report concerns itself with non-state actors primarily in their role as a relay, or sometimes a stimulus, of information manipulation by States.
However, information manipulation techniques are also used by non-state actors acting on their own behalf to promote their own agendas.
The case of nationalist and/or populist movements within our Western democracies, which played a role in Brexit, the election of Donald Trump and the recent French presidential election, are a separate category.
Those efforts stemmed from a different logic as the agenda of these movements overlapped with those of state actors.
As the so-called “Macron Leaks” will be the subject of a separate chapter , we will first consider the examples of the first two instances.
In 2005, Ayman al-Zawahiri said, “we are in a battle and more than half of this battle is in the media.
The propaganda used by ISIS is multidimensional, multi-vectored and carefully targeted.
It is multidimensional firstly because it is based on a simple, conspiratorial vision of a Manichean world to explain our social lives.
It also involves BBC-type reports presented by John Cantlie, a hostage-reporter, speaking about such things as the good living conditions in Mosul.
It further includes theology courses, based on extremist readings of religious texts.
ISIS propaganda pairs speeches that are supposedly truth-oriented with more emotional elements—a combination which allows the group to acquire discourse credibility and conquer “the hearts and minds” of their disciples.
The group communicates through its media center Al-Hayat 4 ' and the “jihadosphere” has experienced significant development since the official proclamation of the caliphate in 2014.
ISIS claimed responsibility for the attacks of November 2015 in Paris via an official written statement, which was also repeated in song to a much younger audience.
In a video published by the Islamic State in January 2015, the reporter proposed a tour of the city.
The video appeared to be a direct response to an article by The Guardian, which stated that the residents of Mosul lacked water, food and electricity.
John Cantlie presented the city as pleasant despite the conflict, repeating that there were no power outages in the area.
Kierat Ranautta-Sambhi, in NATO Stratcom COE and the King’s College London, Fake News.
These audiences are subject to meticulous targeting that seeks primarily to exploit the social, economic, political and cultural vulnerabilities of certain communities.
The multiplication of memes and terrorist videos suggests that young people are the principal target of the conspiracy theories spun by the so-called Islamic State.
ISIS offers these vulnerable young people answers to the challenges that come with entering the professional world and building an adult identity.
The problem also arose in 2012 and more recently in 2017, during the gubernatorial elections in Jakarta, which sparked numerous disinformation campaigns attempting to set Muslims against Indonesians of Chinese origin—an old antagonism in the country.
One of the main targets was the then governor, Basuki Tjahaja Purnama (the first Christian of Chinese origin to hold this post), who was accused of blasphemy.
The following year, in West Kalimantan, an innocent man was beaten to death by a mob following a false rumor—but a more sophisticated one as it used the police logo—that a child kidnapping scheme had taken place.
By the time the group was arrested in August 2017, its members had already succeeded in exacerbating antiChinese sentiment.
The president has since declared “war” on fake news and civil society has launched a number of initiatives to detect fake news, such as TurnBackHoax.
The government has also created a National Cyber Encryption Agency to counter religious extremism and fake news online.
States In the face of the social movements launched by or with the aid of digital platforms, in particular Twitter and Facebook, authoritarian governments have had two successive reactions.
They initially reacted with a strategy of information scarcity, by censuring content and blocking access, as was witnessed in numerous examples from the early 2000s in China, North Africa and the Middle East.
These authoritarian governments nevertheless quickly became aware of the potential for these technologies to surveil and influence their own citizens.
Therefore, the second generation in internet control of the population, in contrast to the first, actually benefitted from the overabundance of information.
The NGO denounced the actions of 30 governments (compared to only 23 countries denounced in the 49.
How Governments Are Deploying Disinformation as Tart of Broader Digital Harassment Campaigns, Institute for the Future, 2018, 8.
The case of the United States was peculiar in that it revealed that another State, Russia, had used manipulation to advance its interests and influence abroad.
Manipulation targeting the local population In most cases, governments manipulate the information given to their own people in order to strengthen and solidify their power.
They use control techniques such as those developed by China and Russia which have become a “global phenomenon,” according to the President of Freedom House.
See also Michael Riley, Lauren Etter and Bibhudatta Pradhan, /\ Global Guide to State-Sponsored Trolling, Bloomberg, 19 July 2018.
Bots are waging a dirty war in Mexican Social media,” media.ccc.de, August 2015.
Inside, the Secret World of the BJP’s Digital Army, Juggernaut Publication, 2016.
The object of this report is not to identify all the information manipulation implemented by States against their own population—that falls to the lot of human rights NGOs—but to analyze those attempts aimed at foreign populations targeted, above all, at our democracies.
How governments endorse hate campaigns against critics,” The Guardian, 12 July 2017.
Manipulation targeted at a foreign population This category concerns far fewer States.
This does not mean, of course, that only these two States manipulate information outside of their borders.
Other States also do it, or attempt to do it, but with much less success and far fewer means in the international arena than the two mentioned above. a.
Russia There is no “Russophobia”inthe observation that all recent interference attempts in referenda and elections are tied, directly or indirecdy, to Russia.
Our interlocutors among European authorities attribute 80% of influence efforts in Europe to Russia.
For a number of countries that have been exposed for many years— Baltic and Scandinavian countries.
These States have long been the targets of information manipulation campaigns.
However, until 2014, their concerns were not heard, as attempts to bring attention to this phenomenon were only met with indifference and even irritation from the “great” powers of Western Europe.
Western political leaders no longer hesitate to call out Russia actions.
The Role and Impact of Mon-Traditional Publishers in the French Elections 2017, 19 April 2017, 18.
These tactics have been integrated into Russian official doctrine, whose strategy is to weaken the West as will be demonstrated in the subsequent pages.
To be exact, and as recommended by the CSIS, we should speak about the “Kremlin” rather than “Russia” so as not to conflate the governing power with its people.
Sw'eden, Price Minister’s office, National Security Strategy , 2017, 12.
Andrea Shalal, “Germany Challenges Russia over alleged cyberattacks,” Reuters, 4 May 2017.
The other elements are of a political, diplomatic, military, economic and cultural nature.
A Soviet tradition Russian disinformation—including interviews with fake experts, counterfeit documents, and retouched photos and videos—has a long tradition dating back to the Soviet period.
The first significant operation was Operation Trust , which targeted White Russians in exile.
How the Kremlin Weaponi^es Information, Culture and Monty, The Interpreter, a project of the Institute of Modern Russia, 2014, 10.
Implications for United States National Security, A Minority Staff Report prepared for the use of the Committee on Foreign Relations, United States Senate, 10 January 2018, 37.
On the history of Soviet disinformation operations, see the notes of Vasili Mitrokhim, who was an archivist for the KGB for thirty years before going to the West in 1992.
This Soviet rumor remains popular today and is used by the Kremlin to defend itself from certain accusations, namely by claiming that these accusations are merely false flag operations.
Among other famous fake news of the Soviet era were stories alleging American responsibility for the 1961 putsch of French generals, the assassination attempt on Pope John Paul II in 1981, and the “creation” of the AIDS virus.
These strategies include disinformation, infiltration or manipulation of youth organizations or trade unions, the use of agents of influence, and the use of pro-Russian or mainstream foreign media to disseminate information.
For a time, the White House refused to respond directly to these operations.
It was only in 1981 that the Reagan Administration created an inter-agency group (bringing together the CIA, USIA, FBI and the State Department) to analyze and organize a means of response in the form of reports presented before Congress and briefings to major media outlets.
This is the sole example of a coordinated and effective response by the American institutional apparatus to the threat of Soviet influence.
On April 12, 1982, KGB Director Yuri Andropov ordered all agents to take “active measures” to prevent Ronald Reagan from being reelected.
Moscow also launched a “massive propaganda campaign” to see Helmut Kohl defeated in the 1983 federal election in Germany, but to no avail.
There is a mimetic component to the Russian approach, which draws inspiration from the latest Western communication and public relations techniques.
Information warfare in the light of Russia’s Military Doctrine”, Point of View, 50, OSW, May 2015, 7.
The evolution of the Russian approach One major difference with the Soviet period is that contemporary Russian interferences have given up any pretense of ideology.
The continuation and adaptation of Soviet techniques occurred in several stages.
Moscow observed that its narrative of the Chechnya War, from 1999 onward, was not adopted by international public opinion, even as the majority of the Russian population rallied around the new President Putin and his stated desire to exterminate Chechen terrorism.
Russia then worked to strengthen and redefine its tools of international influence.
First, it attempted a classic approach to soft power, based on attraction, by creating the Valdai club and by recruiting communicators.
It was to this end that Russia Today was created in December 2005 with the aim of improving Russia’s image abroad.
But the fruits of these efforts took time and the 2008 Georgian War confirmed the weaknesses of the Russian information system.
Despite its efforts, Russia Today failed to influence the international public’s perception of the conflict.
Peter Pomerantsev and Michael Weiss, The Menace of Unreality , op. cit ., 18.
It was after this episode that the Kremlin incorporated “the protest potential of the population” into its military doctrine, perceiving it as one of the most important variables in armed conflict.
This policy advocates targeting Russians first in order to quell protests through the repression of NGOs under the law on foreign agents and through purges in the media; in September 2012, the Kremlin closed USAID, which it described as an agent of influence.
The Kremlin perceived both Euromaidan and the fall of the Yanukovych regime as serious setbacks.
They were, for Moscow, a worrying signal of the success of the regime change approach,e. a Western-led idea.
Adding to the anxiety, these events took place near Russian borders, and more importantly, in Ukraine.
This trauma pardy explains Russia’s subsequent military intervention in Ukraine—first in Crimea and later in the Donbass.
It also explains the intensity of the information war waged by Russia from the start of the Ukrainian crisis.
After having implemented a series of measures aimed at curbing the protest potential of the Russian population, the Kremlin has, since the Ukrainian crisis in particular, strengthened its information offensive both towards the States of the “near abroad” and Western States.
Since 2016, the sophistication of these techniques has advanced with the adoption of a new information doctrine and, in 2017, a strategy for the development of the information society and the creation of “cyberbrigades,” the extension of the National Guard’s jurisdiction over informational and cyber fields, etc.
It considers itself the victim of an information war waged by the West, especially by the United States.
The defense of democratic and liberal values and support for civil society are seen as subversive acts whose purpose is regime change.
The perception of Western dominance in the information field (based on the observation that major American and British media have much larger audiences than RT, for example) puts Russia on the defensive.
It will however be noted that no proof was provided in support of this claim and that, if the central electoral commission observed these cyberattacks on the day of the vote, they did not identify any perpetrators.
The term “Gerasimov doctrine” is common in the West, named after the Chief of the General Staff of the Russian Armed Forces.
In the twenty-first century we have seen a tendency toward blurring the lines between the states of war and peace.
The role of nonmilitary means of achieving political and strategic goals has grown, and, in 55 86.
INFORMATION MANIPULATION 56 many cases, they have exceeded the power of force of weapons in their effectiveness.
In Ukraine in 2014, Russia reproduced what it thought Westerners had done during the color revolutions, the Arab Spring and Euromaidan.
Gerasimov likes to recall that the concept of “hybrid warfare” was first written in 2005 in the United States from the pen of a certain General James Maths, now Secretary of Defense.
Valery Gerasimov, “The Value of Science Is in the Foresight”, originally published in Military-Industrial Kurier, 27 February 2013, translated from Russian by Robert Coalson and republished in the United States A.rmy Military Review, January-February 2016, 24.
Moscow, 2010, quote in Peter Pomerantsev and Michael Weiss, The Menace of Unreality , op. cit. , 12.
The Military Academy of the General Staff of the Armed Forces of Russia devotes an entry to this term in a glossary, with the purpose to distinguish the Russian meaning, applicable at all times, from the Western meaning, which limits informational operations to a period of hostilities.
This confirms the Russian continuum between war and peace and the conviction, on the Russian side, of their difference , compared to the Western approach.
In March 2018, several members of the Duma evoked the possibility of introducing the concept of “information war” into Russian legislation.
Mikhail Degtyarev, for example, declared that “it is the continuation of the information war launched against Russia.
First, the democratization of information through the internet, especially in democratic countries, creates fierce competition for major Russian media.
In terms of the number of viewers, on television and even on social networks, RT remains well below BBC, CNN and 92.
Sergei Chekinov and Sergei Bogdanov, “Asymmetrical Actions to Maintain Russia’s Military Security,” op. cit.
Keir Giles, The Next Phase of Russian Information Warfare, NATO Strategic Communications Centre of Excellence, 2016, 4.
Cited by Keir Giles, The Next Phase of Russian Information Warfare, op. cit., 2.
Dar’ja Rynochnova, “AerrapeB npeAAOKHA BHecra b 3aKOHOAaTeAtcTBo nomrrae ‘HH^opMaijHOHHOH BoHHH,’” Parlamentskaja Ga^eta, 13 March 2018.
However, this same process of democratizing information is also ambivalent and creates more relays and means of reaching certain audiences.
In addition, the Kremlin does not create crises so much as it exploits existing vulnerabilities, divisions, and political or inter-community tensions.
China The People’s Republic of China has a long history of ideological struggle and the use of propaganda.
Today, this know-how is at the service of Chinese interests on a global scale.
Beyond the maintenance and improvement of its image, Beijing develops tools of influence and interference that are specifically geared towards offensive intentions.
The organs of propaganda and influence are placed high up in the political hierarchy.
The regime’s number five, Wang Huning, heads the “Central Guidance Commission on Building Spiritual Civilization,” which determines ideological content and manages its dissemination at the national and international level.
These vectors are complemented by networks that broadcast cultural content for educational and academic purposes, such as the Confucius Institutes, which are the preferred relays of influence and propagation of official messages.
In the media field, China created a state-owned global broadcasting group called the China Global Television Network in 2016.
The programs broadcast content in a coordinated manner that was, for the most part, provided by the Xinhua State News Agency.
In pursuit of these same objectives, the main English-language Chinese newspapers, the People's Daily, the China Daily and the Global Times , are similarly distributed through digital media.
In recent years, the nature of the content broadcast by these media has changed dramatically.
In the context of China’s rise to the rank of superpower and its involvement in the strategic and security issues of the day, Chinese critiques of Western powers—the United States, in particular—have become more regular and more elaborate.
This is the case, for example, with the media’s treatment of the Syrian crisis.
At the international level, these two bodies rely on a dense network of broadcasters and operators whose supervision is essentially shared by the Information Office of the State Council and the Ministry of Foreign Affairs.
This is part of a counter-influence attempt aimed primarily at Eastern European and African audiences.
These operations are part of a global propaganda campaign that seeks to counter and reduce the influence of democratic, liberal values and messages.
The establishment of the Belt and Road Media Community contributes not only to promoting Chinese interests, but also to countering or extinguishing the influence of external media.
The ideological content is not only used to seduce or influence, but also to guide public opinion and interfere if needed.
This proactive dimension is less aggressive than that used by Russia today, but Chinese tactics are growing in number and sophistication.
Information warfare is an integral dimension of China’s strategy of influence and intimidation.
Since the beginning of the 2000s, Chinese strategists have been working on the implementation of the “three wars” in the field of information.
Combining the war for public opinion, psychological warfare and legal warfare, this approach is intended—in peacetime as in wartime—to control the dominant discourse and influence beliefs and perceptions so as to serve the interests of the PRC, while also reducing the ability of adversaries to respond.
This strategy, which explicitly targets public opinion in democracies, exploits the vulnerabilities of open societies.
The intelligence services (the Ministry of State Security, the Ministry of Public Safety, the second department of the People’s Liberation Army and the department of international liaisons of the PLA in particular) and certain departments of the Central Committee of the CCP (United Front Work Department—UFWD) have engaged in a similar reflection in recent years.
Henceforth, the concerted efforts to reinforce the Chinese narrative are now associated with clandestine operations aimed at boosting China’s influence capabilities.
This development, which is closely linked to China’s rise to power in the international arena and subsequent sense of self-assurance, hinges on the combination of both the Russian example, from which Beijing draws its inspiration, and the 100.
Beijing also organizes numerous conferences that seek to shape the content of media stories and influence a new generation of journalists.
See Lu Anqi, “Chinese and African Media Discuss how to tell good stories,” Chinafrica , 14 August 2016.
American retreat from the international sphere, initiated by the Trump administration.
Although there were some heated discussions brought about by Chinese interference in Australia and New Zealand, Europe continues to show comparatively little concern for this threat even as Chinese operations continue to visibly increase.
The implementation of this multi-sectoral influence has already led to policy responses and the build-up of new security measures in several democracies, including Australia, from wDch Europe may draw inspiration.
Today, Beijing enjoys systematic and multi-vector counter-influence and information-control capabilities.
CDnese content that is broadcast in French-speaking Africa often conveys positions and principles that are contrary to French interests.
TDs dimension goes well beyond the framework of the Franco-Chinese bilateral relationsDp and is part of a strategic global competition.
INFORMATION MANIPULATION 62 Chinese interference in Australia Australia is a prime target of Chinese influence.
The CCP exploits vulnerabilities linked to the Australian model of financing universities, media and election campaigns in order to expand its influence and “buy” itself access to the country’s political and scientific communities.
There are certain corrupt political figures who, in exchange for funding from Chinese donors, advance the CCP’s positions on international issues.
Some Australian universities for example have equally become vehicles of Chinese propaganda.
Australia has become aware of these concerning developments, which are partly the result of the premium placed on counter-terrorism over counter-espionage in the Australian Security Intelligence Agency in the post-9/11 years.
Canberra is currently in the process of rebalancing its priorities, and has significantly increased its legal arsenal in order to monitor foreign investment in its territory, including in the media.
In June 2018, the Parliament passed news laws against espionage and foreign interference.
From our analysis of attempted information manipulation in some twenty countries, certain common features have emerged, in terms of vulnerability factors and the means employed.
This section of the report exposes those commonalities and then explores other incidents of information manipulation outside of the well-documented cases in Europe and North America.
The presence of minorities Manipulation attempts are facilitated by the presence of minorities, as they exploit the feeling of non-belonging that these communities might have with regard to their integration within the national community.
The large Russian-speaking minority, particularly in Latvia , 1 is not in itself a threat to national cohesion because these communities are diverse.
In fact, there are several Russian-speaking communities of different nationalities (Lithuanian, Latvian or Estonian, with or without 1.
The Russian-speaking minority in Latvia is the largest in the Baltic States as compared with 29% in Estonia and 6% in Lithuania.
Furthermore, Russian communities in the Baltic States enjoy a better quality of life and greater freedom of movement (in Russia and the Schengen Zone if they are permanent residents of Latvia) than they would otherwise enjoy in Russia.
After disappearing for 30 hours, a 13-year-old girl belonging to the Russlanddeutsche community claimed to have been kidnapped, beaten and raped by three men who appeared to be of Arab descent.
Russia immediately picked up the story, first on the major national channel, then in Russian media outlets abroad and on social networks, where the story was relayed notably by far-right groups.
Through Facebook, demonstrations were organized, involving both the Russlanddeutsche and neo-Nazi groups.
The Russian Foreign Minister Sergei Lavrov made two public statements in which he accused the German authorities of concealing the reality of the situation behind political correctness for interior political reasons and challenged the competence of the German police and judicial system.
His German counterpart Frank-Walter Steinmeier accused Russia of political propaganda.
It also showed that the Russlanddeutsche community is one of Germany’s vulnerabilities and that Berlin must put in place a mechanism with which to react to these challenges as soon as possible.
The incident was allowed to develop as much as it did because the story was refuted too late.
In Germany, alongside a small, but important local Russian-speaking community, there is a community of Kusslanddeutsche.
They are nationals of the German-speaking Soviet space, descendants of the Germans of the Volga transfered there by Catherine II in the 18 th century, who were deported by Stalin to Central Asia and later repatriated to Germany after reunification.
The existence of this community provides fertile ground for manipulation, the primary purpose of which is to accentuate the divisions between these Kusslanddeutsche and other Germans in an environment of suspicion toward immigrant communities.
Internal divisions Even where there is no significant diaspora or easily exploitable minority group, attempts at information manipulation can have an effect on social and political divisions within our democracies in an even more insidious manner.
At first glance, it seems that the country would offer little opportunity for Russian manipulation attempts.
In addition, 70 years of communism has immunized the population to Russian propaganda.
Furthermore, the country has neither a Russian-speaking minority nor a Russophile political party and anti-Russian sentiment is widespread.
Moscow is taking advantage of the political divisions, which have lately been increasing.
An Oxford researcher studied Polish social networks and showed that only a few days after the Euromaidan movement in Kiev, a large number of fake accounts appeared on Facebook, along with different platforms which began spreading Russian propaganda.
The researcher also pointed to the growing difficulty of detecting and attributing these actions to Russia, which has increasingly succeeded in normalizing them.
External divisions Tensions between neighboring countries are also exploited.
Moscow is working to sow discord between Poland and its neighbors, Belarus, Lithuania, Germany and, above all, Ukraine.
The objective is to make Poland the outcast of Europe, by weakening its relationship with both its immediate neighbors and the EU as an institution.
In Lithuania in 2017, authorities observed a strong resurgence of messages directed at the local Polish community which were aimed at exacerbating inter-community tensions and degrading diplomatic relations between the two countries.
At the European level, Moscow tries to isolate the Baltic States by portraying them as paranoid, Russophobic hysterics compared to the more “moderate” States of Western Europe.
One of the Kremlin’s prime objectives is to maintain and caricaturize the existing divisions between European countries on Russian matters.
A vulnerable media ecosystem One of the reasons why the Macron Leaks failed to have an effect on the 2017 French presidential elections is that the French media ecosystem is relatively healthy.
By this we mean that the population relies mainly on traditional media sources with high journalistic standards.
Their efforts are all the more likely to succeed if the media landscape is fragmented and conventional media outlets are weak, which can arouse distrust in part of the population and accelerate a variety of populist and conspiracy theories’ vectors.
State Security Department of the Republic of Lithuania, National Threat Assessment 2018, 42.
A few years ago, Latvia and Lithuania temporarily blocked the Russian channel RTR-Planeta, accusing it of inciting hatred towards Ukrainians.
Local channels have a hard time competing with Russian channels, which have considerable resources and very popular entertainment programs.
In Latvia, authorities rejected the idea of creating a Russian-language channel, while Estonia launched ETV+ in 2015.
Furthermore, the idea of a single, common Russian-speaking channel is not feasible anymore because of rivalries and differences in perception.
On the internet, the Latvian authorities refused to register Sputnik on the .lv domain.
The most effective sites for spreading Kremlin propaganda are not those that are most obviously associated with Moscow, such as Sputnik; rather, they are sites which appear local, such as Vesti.lv, or regional, such as Baltnews or Rubaltic.
Launched in 2014 in three distinct editions for each of the Baltic States, Baltnews belongs to Rossiya Segodnya through front companies.
The content of its sites is partly determined by Russian diplomats in Riga, Vilnius and Tallinn.
There is also the IMHO Club, a network of Russian-language blogs owned by Latvian proRussian activist Yuri Alexeyev, who, after successfully establishing himself in Latvia is trying to gain footholds in Belarus and Ukraine.
Contested institutions The first part of this report demonstrated how distrust in institutions was one of the main reasons for the rise and effectiveness of attempts at information manipulation.
It makes democratic institutions and public policies easy targets by constantly causing doubt, either of a government’s so-called “hidden agenda” or of the effectiveness of government action.
Thus one of the narratives propagated by manipulation attempts in Baltic States is that these States are failed States, brimming with corruption that would collapse without the support of Western powers.
This narrative is all the more dangerous because there is always a segment of the population, in every democracy, that is critical of and disappointed with its government.
Since 2014, the Kremlin has systematically challenged authorities in Ukraine by accusing them of inefficiency, corruption, etc.
These social networks use pro-Ukrainian symbols and nationalist rhetoric to call for the organization of a third Euromaidan.
One could add a fifth category of individuals abroad who serve as pawns in the manipulation of public opinion or local relays of foreign policy.
These “useful idiots,” an expression attributed to Lenin, may be intellectuals whom the Kremlin is trying to “capture” through forums such as the Valdai club.
They may be political figures from far right or far left movements or activists from various movements.
There is also “black” propaganda, that is, propaganda that can be reasonably denied.
Proxy Groups in the Contested Neighbourhood, Chatham House, April 2016.
A case study in how Russia’s propaganda machine works,” DFRLab Medium.com, 23 January 2018.
INFORMATION MANIPULATION 72 2014 to replace the Voice of Russia) and RT, established under the name Russia Today in 2005 and renamed RT in 2009.
However, the Kremlin quickly realized that Rossiya Segodnya and RT did not succeed in this endeavor.
These media receive little attention in the international arena, where CNN, BBC and Al-Jazeera dominate the media environment.
They are, however, successful with audiences in the far left and far right political camps or those easily taken in by conspiracy theories.
In the Baltic States, most of the Russian “journalists” who publish Russia-oriented reports are working there illegally, having entered on tourist or business visas often issued in Europe.
These “journalists” interview small, local personalities who are opposed to the governing party.
They also film protest movements, exaggerating their scale in order to give the impression of a divided country on the brink of civil war.
When Russian channels invent reality In February 2017, a team from the Russian television channel NTV made their way to Rinkeby, a Stockholm suburb, to cover clashes with the police.
Rape and sexual crimes in general are one of the most favorited pro-Kremlin disinformation stories because they arouse passions all the while illustrating a state of insecurity, of moral decadence and of the “barbarization” of Europe.
The year before, the television channel Russia 1 also aired a report in France on islamization that is riddled with lies—the journalist who went around Parisian streets notably insisted that “practically nobody speaks French.
At the market, they only sell Halal meat One out of two women wears a burqa or a niqab.
Allyson Jouin-Claude, “Le Petit Journal denonce les manipulations d’une chaine publique russe,” FeFigaro.fr, 21 May 2016.
It is still too early to predict the effect that this new player will have on the Russian disinformation arsenal.
Even if information manipulation is spread into the virtual world (see below), it also has real-life effects.
Depending on the countries, information manipulation relies on various mechanisms of influence, in the political, media, economic, and even cultural spheres.
One of Moscow’s biggest achievements is to have individuals or groups that are not Kremlin-related endorse its narratives.
These individuals or groups all share a common suspicion or disdain for democratic institutions.
They also defend ideas that are in line with Moscow’s interests, even in those countries where no significant political force is explicitly supporting Moscow’s policies.
The story of Russia’s new private propaganda oudet,” EUvsDisinfo, 16 April 2018.
In Sweden, associations with no outward connections to Russia hold positions that coincide with those of Moscow.
Two examples are the Swedish Peace Council, on the far left, and Swedish Doctors for Human Rights, which denies that chemical attacks occurred in Syria.
In addition, other alternative movements, such as the anti-vaccination movement, are progressively becoming pro-Russian.
Finally, different research centers and institutes that have more or less overt links to Russia propagate Russian positions on foreign policy.
As far as the economy is concerned, in the Baltic States, strategic infrastructure projects that are likely to advance either European integration or energy independence are targeted.
The rail project Rail Baltica was presented as economically unsustainable and evidence of aggression, as its purpose was allegedly to transport NATO troops.
The Polish project to link the Vistula to the Baltic Sea by a canal was also criticized for being ecologically irresponsible, economically unsustainable and militarily aggressive.
Calibrated narratives The Kremlin’s objective is not to persuade us to doubt in an alternative truth, but to believe that objective truth even exists, in order to sow confusion and paralyze any action.
As such, it does not have to defend an ideological view point—this is, as already mentioned, a major change as compared to the Soviet era.
The Kremlin can simultaneously support far right and far left movements, so long as they are in competition with one another.
The UK authorities have so far identified about 2,800 Twitter accounts which are likely to be bot-managed and have reached 7. million users.
They spread stories of a Fentanyl overdose, Russophobia, a British experiment, a plot to justify Russian sanctions, an attempt to influence the Russian elections, an attempt to justify the boycott of the Football World Cup in Russia, an American plot, then a Ukrainian plot, then the fault of Yulia Skripal’s future mother-in-law, then a British attempt to divert attention from rampant pedophilia in the country, that Skripal was trafficking in chemical weapons, a NATO toxin, etc.
Ofcom has announced that it has opened seven investigations against RT, which it suspects of partiality in the handling of the case.
The narratives primarily target the most populous and influential States.
The messages are tailor-made, adapted to specific audiences, depending not only on the region but also on the socio-economic profile, age, etc. of the individual.
The vectors are similarly adapted to the media ecosystem of each country.
The topics are diverse, but certain recurring topics emerge (immigration, crime, American or NATO hegemony, moral decadence, etc.) which is no coincidence.
The Kremlin first targets divisive, fear-inducing topics in our societies.
Aleksander Krol, “Russian Information Warfare in the Baltic States—Resources and Aims,” op. cit, ., 61.
This pathocentered tactic rightly assumes that many people are less rational and more easily manipulated when it comes to emotional subjects.
These subjects do not materialize out of thin air nor do Russian secret services invent them.
These tensions often have a firm and legitimate basis, which makes their manipulation all the more credible.
This was liked by 360,000 people, which is more than the official account of the Black Lives Matter movement.
The account helped mobilize, incite and accelerate the movement with messages encouraging action against the police.
The account was revealed to be Russian and indeed part of the Russian effort to divide communities in the United States by tapping into racial tensions.
The lesson for the online black American “community” to be extremely vigilant and double check the source of the information and the identity of the account holders before forwarding any messages.
In particular, one of the accounts created a counter-protest to a gathering of white nationalists which was planned to be held in Washington DC in August.
Flistory museums in the region place the Soviet and Nazi occupations on a similar footing, which is difficult for Russians to accept.
Russians tend to describe the Soviet period as positive for Baltic States and try to minimize the crimes committed against Balts.
For example, there is the idea circulating that the economic situation in the Baltic States has declined, that they were in fact much wealthier under the USSR—a “Soviet Silicon Valley”—and that their integration into the West allegedly crippled their high-tech industry.
In July 2017, NATO posted a video honoring the “Forest Brothers”, who were Estonians, Latvians and Lithuanians who fought against Soviet occupation.
The spokeswoman of the Russian Foreign Ministry reacted by saying that these “Forest Brothers” were “fascists” and “collaborators” with the Nazi regime.
According to the narrative, sooner or later, the Baltic States will return to Russia’s sphere of influence.
In light of this, the Continental Hockey League can also be seen as a way of reinvigorating old Soviet ties while demonstrating that, in this area at least, the Balts remain in the Russian zone of influence.
For example, there is a memorial dedicated to the victims of communism in Tallinn that is currently under construction at the site of the Soviet memorial to the victims of the Great Patriotic War.
This has generated controversy and debate in the Russian-speaking community and has been subject to complains from Moscow.
Privileged places and mechanisms Information manipulation derives its effectiveness from the viral character of its diffusion on the internet, by various relays, automated or otherwise.
It is clear, however, that their viral character owes nothing to chance.
It is the result of a thoughtful, coordinated and meticulously implemented strategy that relies on a chain of actors, culminating not only in the mass dissemination of manipulated information, but also on a sort of information “laundering” as it is taken up by actors from the media and various other institutions.
See also “The Nazi-obsession of pro-Kremlin propagandist,” EUvsDisinfo, 21 July 2017.
The places The most advantageous place for manipulation is the digital platform.
Through the algorithms that these sites use, the platforms rank content and set the conditions for the diffusion of that content, which is then shared and published.
An attack against national defense and security “Digital platforms, including social networks, can shape opinion more insidiously and are often vectors of values that are not those of the French Republic.
In certain cases, they can be used for the purposes of disinformation and spreading propaganda to French citizens, particularly to the yougest ones.
This feature is very often underemphasized by the platforms themselves, who prefer to consider themselves as “technology companies” that host, more than edit, the information and content that is exchanged.
Therefore, even the way in which these platforms present their features, which is critical to defining their status, is a challenge for them.
Such platforms have long been presented as technology companies that host, without editing, information and content.
French National Digital Council, La Neutrality desplateformes^ June 2014.
Platforms today have, in large part because of outside pressure, modified their position and manner of approaching this subject.
Mark Zuckerberg’s statement to the United States Congress in April 2018 perfectly illustrates this change of stance, especially when he acknowledged that Facebook is responsible for its content, even if it does not produce the content itself.
This statement was surprising because it goes against the strategy employed by this particular actor in prior years.
These platforms benefit from networks effect, “these positive externalities of the information economy,” 29 which ensure them a large number of subscribers and a high rate of retention.
They are therefore the most advantageous location for information manipulation campaigns, which by definition are massive and large-scale.
In this respect, other digital platforms, which are restricted to smaller circle, are not the focus of this report.
The term “social network” does not adequately cover the variety of actors involved in such activities and runs the risk of conflation with unrelated actors.
It does, however, play a significant role in the dissemination of fake news as it has the power to increase or decrease the visibility of that information.
They show that the French “Russosphere” is “neither homogenous in terms of the individual profiles that compose it nor in terms of their political orientations.
On the contrary, the French ‘Russosphere’ is a diverse galaxy of which a significant portion could exist even without Russia playing any role.
INFORMATION MANIPULATION 82 Large digital platforms are not the only ones to relay and amplify information manipulation campaigns.
It was on 4Chan that the “Macron Leaks” were first published and attracted the attention of internet surfers.
Indeed, discussion forums often serve as a launching pad for manipulation campaigns and for the propagation of rumors.
Nevertheless, if discussion forums are susceptible to the spread of fake news, their users are often conscious of the controversial nature of the content exchanged and of the fact that the content is potentially untrue.
The debate often takes place among a small group of users who exchange their opinions anonymously.
Even if these forums eventually allow this fake news to achieve high visibility and affect public opinion, they are used, first and foremost, to launder false information, giving the impression that the information actually originated from actors who have no ties to the political aim pursued.
With the amount of mobile equipment in use being high and constantly growing, and the barriers to entry being minimal , applications are a means of achieving a high threshold of exposure while enjoying a complete absence of moderation.
Information manipulation campaigns can involve the use of stolen documents.
Sometimes these documents are auctioned off and broadcast on the Darknet, a sphere that, because it does not abide by the rules governing the internet, is particularly conducive to the exchange of illegally acquired information.
Bots Firstly, automated or semi-automated actors, like bots or netbots.
This tactic involves fake Twitter or Facebook accounts that allow for the rapid diffusion of fake news through biased retweets and likes.
Ben Nimmo, In his hearing before the Singaporean committee, 22 February 2018.
Rachel Lavin and Roland Adorjani, “L’lrlande a deja trouve la parade aux fake news (mais on ne pourra pas la reproduire),” Slate, 13 June 2018.
For example, during the electoral campaign for the 2018 Colombian presidential election, the amplification process was essentially human, originating from politicians or notable supporters from both political camps. b.
This activity is pardy institutionalized , but is also carried out autonomously by individuals of all nationalities.
A large number of Kremlin supports suddenly appeared on these networks to create controversy, sow discord, and ultimately weaken the adversarial communities.
Petersburg The Internet Research Agency is a Russian company located in St.
In reality, it is a “troll factory” financed by the Kremlin, whose existence was discovered in 2013 by Russian journalists pretending to be candidates applying for jobs there.
The regional press, including Finnish and Polish news outlets, 37 then got hold of the story, followed by 34.
Everyday Misinformation in Colombia: Humans, not bots, were the main vectors of misinformation,” @DFRLab, Medium.com, 20 July 2018.
A tribute to the Russian journalists who exposed the ‘troll factory,’” DFRLab Medium.com, 7 March 2018.
Propaganda and Trolling as Warfare Tools,” European View , 10 May 2016.
In the midst of his investigation into Russian interference, in February 2018, Mueller indicted the IRA, two companies owned by Yevgeny Prigozhin who created the IRA (Concord Catering and Concord Management and Consulting), as well as 13 individuals, one of whom was Prigozhin himself.
The IRA is accused of having led an operation to influence the American electoral campaign.
Registered in July 2013, the IRA would have begun targeting the United States around April 2014 and was receiving funding from 14 affiliated companies in Concord.
The IRA alone controlled 3,814 human accounts and 50,258 hots on Twitter, with which 1. million Americans interacted.
The indictment of the United States Special Prosecutor has provided detailed information on the operations of the agency.
It does not, however, accuse the Russian government of anything nor does it acknowledge that the IRA succeeded in influencing the vote.
While this agency showcases this phenomenon, it draws attention away from other troll factories present elsewhere on the Russian territory, as well as abroad.
However, the IRA is not an isolated case, and must not become the tree that hides the forest.
Office of the Director of National Intelligence , Assessing Russian Activities and Intentions in Recent United States Elections, Washington DC, January 2017.
Ben Popken & Kelly Cobiella, “Russian Troll Describes Work in the Infamous Misinformation Factory,” NBC News, 16 November 2017.
In 2015, Russian trolls and bots began exacerbating racial tensions , fear of jihadism , the debate on firearms , homophobia , etc., and began attacking Hillary Clinton.
It was during 2016 that the first coordinated operation took place, involving cyberattacks and information manipulation, against Hillary Clinton and in favor of Donald Trump.
If no one challenges the message, the troll may do so himself by posing as a third person who challenges the post or, alternatively, supports the post but in so exaggerated a manner that he provokes a reaction and draws others into the exchange.
When a user has “bitten” the hook by engaging in the discussion, the troll reels him in by systematically challenging his comments.
To keep the “discussion” going, the troll varies the characters involved and the tone of the comments from insult to irony.
Among them, the aggressive trolls that proceed through intimidation, brutality and even harassment are the most effective means of saturating the debate and silencing opposition voices.
A number of investigative journalists and well-known personalities, who are opposed to Russian interests, have been the victims of such attacks.
This was the case for Finnish journalist Jessikka Aro who dissected the intimidation techniques 42.
Robert Szwed, Framing of the Ukraine-Russia Conflict in Online and Social Media , NATO Strategic Communications Centre of Excellence, May 2016.
Results of the study , NATO Strategic Communications Centre of Excellence, 2016.
Since journalists are particularly targeted, the NGO Reporters Without Borders dedicated a report to this issue.
In order to discredit someone, trolls often accuse the person of colluding with foreign intelligence services and/or of committing treason.
In this way, a few trolls can, by posting a number of comments, give the impression of a majority opinion even when it is not at all the case—it is enough to have a paralyzing effect on others.
The trolls thereby participate in a wider phenomenon, which is the brutalization of online public debate.
This is particularly true of Ben Nimmo, a researcher at the Atlantic Council, who regularly uncovers networks of trolls and provides detailed descriptions on their functioning.
These analyses are extremely useful for detecting and, in fine , countering them.
In fact, around forty cases were identified from 2006 to 2017 50 (the most known cases being the 2010 American diplomatic telegrams on Wikileaks, the 45.
From Tags tTo Trolling: How tweets to a small group precede attacks on critics of the Syrian and Russian regimes,” @DFRLab, Medium, com, 27 June 2018 and “#TrollTracker: Russia’s Other Troll Team: Mueller points to existence of second Russian troll operation focused on activist groups and foreign policy,” @DFRLab, Medium.com, 2 August 2018.
Pierre Gastineau and Philippe Vasset, Armes de destabilisation massive.
However, this method is increasingly used to serve political or economic interests.
In this way, leaks may form part of an information manipulation campaign, such as is evidenced by the American presidential election and French presidential election (2017 Macron Leaks), As such, they can be used to discredit a target, who may be either a victim of hacking, or a third party.
Journalists would have a lot of trouble with verifying these documents because they do not in general have access to the original source.
In Sweden, there are several famous cases, such as the image of a car on fire which supposedly illustrated the rising crime rate caused by migrants.
Disinformation and Phishing With a Russian Nexus,” The Citizen Tab , 25 May 2017.
Eliot Higgins, “The Russian Ministry of Defence Publishes Screenshots of Computer Games as Evidence of United States Collusion with ISIS,” Bellingcat, 14 November 2017.
In reality, the photo was of a young Welsh girl attacked by her dog in 2008.); • articles in reputable newspapers.
In France, in March 2017, during the presidential campaign, a fake article appeared in a Belgian newspaper Le Soir alleging that Emmanuel Macron was the preferred candidate of Saudi Arabia.
This article was circulated by a number of people, including Marion Marechal-Le Pen, before being identified as a fake news story.
This particular article imitated the web layout of Le Soir but used a different address, lesoir.info instead of lesoir.be); • internet sites.
When the falsification involves text—or images, such as “memes” which have become very popular—the quality of the language can be a useful means of detecting distortion as manipulators usually rely on automatic translators like Google Translate.
Electoral interference Electoral interference can target systems (electronic voting, voting lists), which consequently affects the population’s confidence in the results, or the voters themselves in order to influence their vote.
After studying dozens of cases of interference in cyber-democratic processes in nearly 40 countries on five continents in the past ten years, Canada’s Communications Security Establishment concluded that three-quarters of the activities involved sophisticated methods (i.e. were probably orchestrated by States) for strategic objectives.
Only a quarter of these activities employed less sophisticated means for criminal 53.
The first component concerns elections, beginning with voter registration.
At the very least, this interference slows down the election process and leads voters to question the integrity of the election’s outcome.
Even manual voting is also vulnerable if there are counting machines that could be modified before the vote to falsify or erase data.
In addition, dissemination of the election results online makes those results vulnerable to interception and modification by a third party.
Finally, at any moment in the process, the aggressor may also target critical infrastructures that are needed for organizing elections, such as the power grid.
For this reason, they are choice targets for commercial purposes as much as for strategic purposes.
There is also the risk of collusion, that is to say, of foreign powers providing illegal financial or logistical support to certain candidates in order to try and influence the campaign and the outcome of the vote.
After the 2014 referendum on Scottish independence showed that a majority wanted to remain in the UK, the Russian media tried to discredit the results, as they did not suit their liking.
They interviewed so-called “experts” who claimed that the vote did not respect international standards and supported a petition.
Although ultimately in vain, it nonetheless acquired over 100,000 signatures.
Other significant instances of information manipulation include the 2016 referenda on the approval of the Association Agreement between Ukraine and the EU in the Netherlands and on the United Kingdom EU membership.
Ben Nimmo interviewed in Severin Carrell, “Russian cyber-activists ‘tried to discredit Scottish independence vote’,” The Guardian , 13 December 2017.
It is the spread of data, more than the theft itself, that has an effect on the population, provided that one knows where—or rather, to whom—and when exactly to disseminate the data.
Boris Toucas underlines the tertiary role played by whistleblowers, who use their “critical credibility” to pass the information on to the mainstream media where it is further developed.
The first among them, WikiLeaks, has been relatively discredited since the American elections 58 but it remains popular among a certain demographic and retains a large number of followers.
The interference in the American election, which served as a model for the author, passed through these five stages.
The meddling in the French election did not exceed stage three because the traditional media did not succumb , while the interference in the German election did not exceed stage two.
By edging in the direction of the American alt-right and becoming an ally of the Kremlin, Wikileaks disappointed many of its initial supporters.
The midterm elections of 2018 The American presidential election of 2016 was marked by the hacking of Democrat servers and the dissemination of thousands of documents on Wikileaks , among other measures relating to the information manipulation campaign that was underway.
The investigation led by Special Prosecutor Robert Mueller since May 2017 has highlighted the role of Russia in what appears to be interference.
In the current context, with daily revelations about the scope of the 2016 operations, American authorities are particularly concerned with avoiding repetition of the same incident during the midterm elections which will be held on 6 November 2018.
The reaction of the authorities is however stunted by a lack of political unity, between Democrats and Republicans and within each party; a lack of coordination between numerous administrative structures dedicated to the fight against information manipulation (the Department of State, the Department of Justice, the Department of Homeland Security), intelligence services, etc., see below); the reluctance to share information with the private sector.
The first meeting between federal agencies and digital platforms in preparation for the elections took place only six months before the elections, at the end of May at Facebook’s head office 61 and at the request of private companies, not the government.
Several voices highlighted the importance of this public-private cooperation and the necessity for sharing information with digital platforms if we expect them to effectively engage in the fight against these threats.
Daniel R Coats, Worldwide Threat Assessment of the United States Intelligence Community, Statement for the record, 13 February 2018.
Joshua A Geltzer and Dipayan Ghosh, “How Washington Can Prevent Midterm Election Interference,” Foreign Affairs, 25 July 2018.
Catalonia is not and has never been a concern for Moscow The vote was an opportunity for the Kremlin to divide—and thereby weaken—European States.
During the Catalan crisis, these outlets effectively served as relays for Moscow.
Their geolocation confirms that Venezuela is the second most common origin of these messages after Spain.
Personalities from the digital world such as Julian Assange and Edward Snowden suddenly became passionate about the Catalan issue.
WikiLeaks went so far as to ask El Pais to fire David Alandete, who was investigating Russian interference.
The Integrity Initiative, Framing Russian meddling in the Catalan question , October 2017.
David Alandete, “Russian network used Venezuelan accounts to deepen Catalan crisis,” FI Pais, 11 November 2017.
Other regions affected by information manipulation While the post-Soviet space, Europe and North America are, for the moment, the areas where the main examples of information manipulation are taking place, other areas of concern are emerging, especially in the Middle East, Africa and Latin America.
Democratic transitions and elections also provide fertile ground for information manipulation, as does the rapid growth in digital connectivity, particularly in rural zones where the population is less educated and has a stronger tendency to believe in online rumors.
On the other hand, the organization is alleged to be responsible for several “false flags,” whose purpose was to incriminate Damascus and provoke Western strikes.
These accusations have been made five times against the White Helmets since 2013.
Olivia Solon, “How Syria’s White Helmets Became Victims of an Online Propaganda Machine,” The Guardian , 18 December 2017.
Other regions, notably Latin America through TeleSur and anti-imperialist networks, echo this disinformation.
The strategic alliance between these various actors in Syria doubles as a united and coordinated front on social media.
Ultimately, any initiative to counter impunity in Syria can be invalidated if it is based on the testimony of the White Helmets; 3) it accuses the White Helmets of staging fake chemical attacks and generates uncertainty about the responsibility of the Syrian regime for such attacks.
The Douma chemical attack of 7 April 2018, which sparked outrage from the international community and caused American, French and British strikes a week later, led to the publication of a wide variety of fake news stories in the Russian media.
This information manipulation campaign is, therefore, a central element of the combined strategy of Russian, Iranian and pro-Syrian regime networks in the propagation of a narrative which seeks to discredit all forms of opposition or action against impunity for the war crimes committed in Syria.
Sarah Oh and Travis L Adkins, Disinformation Toolkit , InterAction, June 2018.
The Gulf Other States have been able to orchestrate foreign information manipulation operations, or in other words, information interferences.
On May 23, 2017, the eve of President Trump’s official visit to Saudi Arabia, the Qatar News Agency published a statement online by Emir Tamim bin Hamad al-Thani which was directed at the Trump Administration.
The Emir criticized the “negative ambitions” of its Gulf neighbors, calling Hamas “the legitimate representative of the Palestinian people” and announcing that Qatar had “excellent” relations with Israel.
A few minutes later, the QNA’s Twitter account posted three messages revealing the existence of a plot against Qatar, attributed to Saudi Arabia, Kuwait, the United Arab Emirates, Bahrain and Egypt.
It further announced the recall of Qatari diplomats from these five countries and the dismissal of these countries’ ambassadors to Doha.
Major media outlets in the region, including those in Saudi Arabia and the UAE, quickly spread these statements, which triggered a crisis.
Qatari technicians took more than nine hours to regain control of them.
Upon request from Doha, the FBI conducted an investigation and concluded that the QNA had indeed been hacked.
The Qataris blame Yousef al-Otaiba, the UAE’s influential ambassador to Washington, for having orchestrated this virulent anti-Qatari media campaign.
As of June 5, Saudi Arabia, the UAE, Egypt and Bahrain have worked to isolate Qatar, by recalling ambassadors from Doha, imposing a trade embargo, refusing to allow Qatari planes to enter their airspace, and launching an offensive, international media campaign.
These States quickly submitted an ultimatum to Qatar with a list of thirteen conditions for lifting the sanctions (such as limiting relations with Iran, shutting down Al-Jazeera and other media, closing the Turkish military base under construction, and severing ties with a list of “terrorist” organizations, including the Muslim Brotherhood and 97 70.
Nabil Ennasri, “Reprise de la guerre froide du Golfe,” Orient XXI, 31 May 2017.
Karen De Young and Ellen Nakashima, “UAE orchestrated hacking of Qatari government sites, sparking regional upheaval, according to United States intelligence official,” The Washington Post , 16 July 2017.
These conditions were immediately deemed unacceptable by the Qatari government.
There are several indications that Africa could be the next playground for Russian “information warfare,” especially since French and English are easy languages with which they penetrate the African continent.
The work of a French research team revealed the growing spread of Russian content through the French-speaking African web.
The African public often views Russia through the lens of its anti-colonial Soviet past.
In some countries, such as the Cote d’Ivoire, this rhetoric fuels local political debates.
Thus pro-Gbagbo movements find the information and narratives produced by Russian media to be quite opportune.
The choice made by RT and Sputnik editors to strongly publicize certain issues of direct interest to the African public, such as those affecting the future of the CFA franc, naturally aggravates matters.
In Senegal, for example, many of Sputnik’s articles on Africa are picked up by seneweb.com, the fourth most visited site in Senegal.
In addition, digital marketing strategies used by Russian agencies on social networks are particularly well-suited to the African context, where many users rely on Facebook as a source of information.
The conspiracy theories and other sensationalist news that the Russian media are fond of publishing allow them to increase their audience as tabloid-style media is very popular in Africa.
Until now, the activity of Russian platforms on the African continent was unstructured and their popularity could be understood as a collateral effect of the efforts directed towards French public opinion.
But now, both RT and Sputnik plan to expand their network of correspondents in Africa.
The vast majority of these new viewers are young men from the Maghreb and Sub-Saharan Africa.
The anti-French campaign in Goma On 2 January 2018 in Goma, the Democratic Republic of Congo , a digital campaign called #BoycottFrance was launched.
Two caricatures published on Twitter and widely shared on social media accompanied this campaign.
Under the headings “The sponsors of barbarity in the DRC” and “The heirs of Leopold II in the DRC,” these images intended to denounce alleged French support for Joseph Kabila.
These references are mixed with real facts that cannot be linked to French diplomatic actions, such as the ongoing Total negotiations or the presence of the Themiis training institute.
Key to this campaign was the reinterpretation or clustering of facts that have no causal link with one another.
This had the effect of both inspiring new rumors and feeding existing ones, and it only took the circulation of some fake news to trigger the campaign.
The paradox is that this attack was carried out by actors with whom the French Embassy is in contact.
The reflection on the ways to get rid of such fake news or to counter the viral nature of such caricatures and come up with alternative methods of communication, should take into account the unique character of online communication today on the African continent.
Latin America The relative neglect toward Latin America by many Western States has presented several geopolitical entrepreneurs with the opportunity to build new partnerships at little expense, under the banner of post-Western multipolarity.
The channel now has offices in Argentina, Venezuela, Cuba, Nicaragua and Madrid as well as in North American urban centers where Latino communities are concentrated, like Miami and Los Angeles.
Playing a Geopolitical Game in Latin America,” Carnegie Endowment for International Peace, 3 May 2018.
This investment in Spanish-language channels has been accompanied by a strategy of creating partnerships and amplifying information on social networks.
The broadcasting of RT programs has been facilitated by hundreds of specific agreements with national media and some programs are being jointly produced with the Venezuelan channel TeleSur.
It should be emphasized that this worldview corresponds to important trends in Latin American public opinion and that this editorial orientation does not preclude the production of good quality programs.
Brazil is the third country after India and the United States in terms of Facebook users, followed by Mexico.
INFORMATION MANIPULATION networks, and of the extreme polarization of exchanges.
McMaster, 78 then President Trump’s National Security Advisor, publicly denounced the implementation of a sophisticated strategy of Russian influence in favor of leftist candidate Andres Manuel Lopez Obrador.
The Kremlin would, in this scenario, be interested in seeing an ally rise to power in Mexico and destabilize its large northern neighbor.
At this point, there was indeed an editorial line from RT and Sputnik that openly supported Obrador.
It then examines the responses employed by other States on the institutional, legislative, and educational levels.
Finally, this section provides an overview of the responses prescribed by a variety of international organizations, civil society and private actors.
A common question is whether it is better to respond to an information manipulation attack or to simply ignore it and, if the choice is to respond, whether it is sufficient to correct it or if the opportunity should be used to promote an alternative message.
This answer is a combination of ignorance and both defensive and offensive measures.
Jakub Janda, “Why the West is Failing to Counter Kremlin Disinformation Campaigns,” The Observer ; 30 December 2016.
The rhythm of the media is such that few events outlive the daily flow of information, and public opinion tends to forget quickly.
Yet, this also comes with the risk of allowing such false and potentially dangerous ideas to sink into the minds of the population.
If they are not contradicted from the outset, these ideas may continue to grow with time.
Ignorance as a strategy should therefore only be reserved for minor and inoffensive forms of information manipulation.
Reacting defensively by correcting false information has the advantage of not allowing the ideas to spread and be left unchallenged, by quickly cutting them short.
But the task requires time and human resources— to monitor social networks, detect manipulation attempts, formulate a response, disseminate it, and analyze its reception.
There is also a risk that it will have the opposite effect, as the response could spark a debate, and even involuntarily give new wind to the trolls.
The most efficient solution would therefore combine a defensive strategy with an offensive one, by providing new information that will help to take back control over the discussion.
These targeted actions against presidential candidate Emmanuel Macron neither succeeded in interfering with the election nor in antagonizing French society and, as such, is of particular interest to our study.
By “Macron Leaks,” we refer not only to the release on Friday, May 5, 2017—just two days before the second and final round of the presidential elections—of 9 gigabytes of data that were hacked from Emmanuel Macron’s campaign team.
We refer more generally to the orchestrated campaign against him that started several months earlier, through numerous information manipulation operations.
This section will provide an analysis of the “Macron Leaks,” the actors who orchestrated the attacks, how it was successfully countered and, finally, the lessons that can be learned.
Jean-Baptiste Jeangene Vilmer, The Macron Teaks: A PostMortem Analysis^ CSIS Europe Program, Washington D.C., Fall 2018.
The leak itself was only the culmination of a long-running campaign orchestrated against the presidential candidate.
It began with the diffusion of rumors and insinuations that grew in January and February 2017.
Some attacks came from Marine Le Pen’s “foreign legion” of American alt-right trolls.
Two hours before the final televised debate between Emmanuel Macron and Marine Le Pen, on Wednesday, 3 May at 7 pm, 5 a user with a Latvian IP address posted two fake documents on the US-based forum 4Chan, suggesting that Macron had a secret offshore account.
It was quickly retweeted by some 7,000 Twitter accounts, mostly pro-Trump, often with the #MacronGate and #MacronCacheCash hashtags.
During the debate, Le Pen herself mentioned the existence of a hidden account.
The rumor was quickly debunked as several investigative journal pieces proved that these documents were fake.
Curiously, the same people who posted the fake documents on 4Chan on Wednesday announced on Friday morning that more was coming.
By this declaration, those responsible for the “MacronGate” inadvertently provided evidence that they were the same people responsible for the “Macron Leaks” that came out later that day.
The operation started with a series of phishing attacks several months earlier.
Macron’s team confirmed that their party had been targeted since January 2017.
In total, the professional and personal email accounts of at least five of Macron’s close collaborators were hacked, including his speechwriter, his campaign treasurer and two MPs.
Josh Harkinson, “Inside Marine Le Pen’s ‘Foreign Legion’ of American Alt-Right Trolls,” Mother Jones, 3 May 2017.
Michel Rose, Eric Auchard, “Macron campaign confirms phishing attempts, says no data stolen,” Tenters, 26 April 2017.
The files were initially posted on Archive.org, then on PasteBin and 4Chan.
Pro-Trump accounts (William Craddick and Jack Posobiec) were the first to share the link on Twitter, using the hashtag #MacronLeaks, which was soon after picked up by WikiLeaks.
Third, it is spread through political communities, notably the United States alt-right and French far-right with catalyst accounts or “gurus” and finally the content is retweeted by both real people * 11 and bots.
It is indeed easy to see the connection between the information component—that is, the spread of rumors and fake news during the presidential campaign—and Russian interests, particularly since the Russian media, with Sputnik and RT at the head, played a non-negligible role in the diffusion of this information.
Alt-right attacks Macron in last ditch effort to sway French election,” DFRLab Medium.com, 6 May 2017.
At the time of writing, more than a year after the incident, France still has not publicly attributed the attacks to any particular perpetrator.
Sean Gallagher, “Evidence suggests Russia behind hack of French president-elect,” Ars Technica, 8 May 2017.
Feike Hacquebord, “Pawn Storm Targets German Christian Democratic Union,” TreudLabs Security Intelligence Blog, 11 May 2016.
Sean Gallagher, “Evidence suggests Russia behind hack of French president-elect,” op. cit.
Given the well-known alliance that exists between Russia and the American far-right movements, 23 these two hypotheses are not incompatible.
On June 1, 2017, Guillaume Poupard, the head of the French National Cybersecurity Agency , declared that “the attack was so generic and simple that 18.
The fake news on Emmanuel Macron offshore account looks too professional,” 5 May 2017.
Konstantin Rykov in a mediametrics.ru interview, in the Paul Moreira’s documentary, “Guerre de l’info ,” Arte thema, 2018.
Russia used Facebook to try to spy on Macron campaignsources,” Reuters, 27 July 2017.
Casey Michel, “America’s neo-Nazis don’t look to Germany for inspiration.
In fine, the leak did not significandy influence French voters, despite the efforts of the aforementioned actors.
French success resulted from a combination of structural factors, luck, as well as the effective anticipation and reaction of the Macron campaign staff, the government and civil society, especially the mainstream media.
Structural reasons Compared with other countries, especially the United States and the UK, France presents a less vulnerable political and media environment for a number of reasons.
First, the election of the president is direct, malting any attempt at interference in the election more obvious.
Furthermore, the French election has two rounds, which creates an additional difficulty for meddlers, as they do not know in advance who will make it to the second round.
This also permits the population to shift their support to another candidate and correct an unexpected result after the first round.
They overestimated their ability to shock and mobilize online communities.
Andrew Rettman, “Macron Leaks could be ‘isolated individual’, France says,” EU Observer, 2 June 2017.
They also overestimated the interest of the population in a leak that ultimately revealed nothing.
They assumed that the creation of confusion would be enough and that the content of the leaks would somehow be secondary.
But, as it became obvious that the thousands of emails and other data were, at best, boring and, at worst, totally ludicrous, the public lost interest.
Then, the idea to launch the offensive just hours before the purdah period was a double-edged sword.
The goal was certainly to render Macron unable to defend himself and to mute the mainstream media.
Perhaps the hackers expected to attract attention with the announcement of the leaks rather than the content of those leaks because the content did not contain anything interesting.
Regardless, the timing of the release did not leave provocateurs long enough to spread the information, and it made the leaks appear suspicious.
Most of the catalyst accounts were in English because the leaks were first spread by the American alt-right community.
This was not an effective means of penetrating the French population, which is known for not having the best foreign language skills.
First, there is the National Commission for the Control of the Electoral Campaign for the Presidential Election 25.
Jean-Yves Le Drian , interviewed in he Journal du Dimanche , 8 January 2017.
United States warned France about Russian hacks before Macron leak,” Politico , 9 May 2017.
THE RESPONSES , a special body set up in the months preceding every French presidential election to serve as a campaign watchdog.
ANSSI and CNCCEP alerted the media, political parties and the public to the risk of cyberattacks and disinformation during the presidential campaign.
ANSSI was proactive, offering to meet with and educate all campaign staff at very early stages of the election.
From the start of the electoral campaign, the French government signaled both publicly and through more discrete, diplomatic channels, its determination to prevent, detect and, if necessary, respond to foreign interference.
This was evidently not enough to stop the attack, which is why it would be an exaggeration to call it deterrence, however it is possible that the remarks helped to contain the operations, which could have otherwise posed a bigger threat.
ANSSI heightened security at every step of the electoral process in order to ensure the integrity of the vote.
Following the recommendations set forth by ANSSI, the Foreign Minister announced, at the beginning of March 2017, the end of electronic voting for citizens abroad because of the extremely high risk of cyberattacks.
Ten days before the vote, Facebook announced that it had deleted 30,000 suspicious accounts in France.
Jean-Yves Le Drian , interviewed in Le Journal du Dimanche , 8 January 2017.
Russia used Facebook to try to spy on Macron campaignsources,” op. cit.
At 11:56 pm on Friday, 5 May, only hours after the documents were dumped online and 4 minutes before the purdah went into effect, the Macron campaign issued a press release.
This diversionary tactic, which involves the creation of fake documents to confuse attackers with irrelevant and even deliberately ludicrous information, is called cyber or digital blurring.
Thanks to this tactic, the Macron campaign staff did not have to justify potentially compromising information contained in the Macron Leaks; rather, the hackers had to justify why they stole and leaked information which seemed, at best, useless and, at worst, false or misleading.
The forceful presence of the Macron campaign staff on social media enabled them to respond quickly to the spread of information.
The campaign’s injection of humor and irony into their responses to the hackings increased the visibility, popularity, and rate of diffusion of those responses across different platforms.
On 27 April, Macron’s campaign confirmed that it had denied RT and Sputnik accreditations to cover the rest of the campaign.
Even after the election, both outlets have been denied access to the Elysee Presidential Palace and Foreign Ministry press 30.
If there is nothing scandalous to be found in the leaked emails, it is because Macron’s campaign staff was aware that everything they wrote in their emails could one day be made public.
Some have even gone one step further by denouncing an electoral interference attempt and calling upon their readers to not let themselves be manipulated.
Emmanuel Macron, joint press conference with Vladimir Poutine, Versailles, 29 May 2017.
See Marc de Boni, “Devant Poutine, Macron denonce la ‘propagande’ des medias pro-russes,” Le Figaro , 29 May 2017.
There was a disinformation campaign, data hacking, large scale leaking, but no laundering or mainstreaming.
Finally, structural factors as well as an effective, responsive strategy allowed the French to successfully mitigate the damage of the Macron Leaks.
More countries are coming to grips with the problem and have come to realize that the most dangerous activities are not the most visible ones—such as the “Lisa Case” in Germany or instances of electoral interference in the United States and France—but the dayto-day undermining of trust in institutions and liberal, democratic values.
Indeed the peril of foreign interference lies less in targeted operations than in the long-term alteration of the political environment.
Accordingly, States have taken a number of corrective actions, or are in the process of doing so.
SafeguardingDemocraticElections Against Cyber-Enabled Autocratic Meddling, op. cit.
Created in December 2014, it is an informal network in that it was not officially appointed, however it was approved by the Security Committee.
Its mission is to identify, analyze and respond to hostile foreign interference attempts.
This network comprises about thirty government experts holding key positions within their respective Ministries as well as ICRC and other NGO representatives, who all meet once a month.
The Danish Ministry of Foreign Affairs also has its own internal task force that involves three departments— namely Public Diplomacy, Security Policy and European Neighborhood & Russia.
This cross-disciplinary team of about a dozen members is placed under the direct authority of the political director, which speeds up the decision-making process.
The purpose of this unit is to ward off information manipulation narratives through the identification of their source and the examination of their effects, as well as build up the resilience capacity of those third States that are particularly exposed to information manipulation.
The unit also develops partnerships with the media, technological actors and civil society so as to establish a network of fact-checkers.
Initially established to counter ISIS propaganda, the Center’s mission was broadened the following year to include threats from foreign States, primarily those from Russia.
Sometimes described as coordinating the work of a variety of agencies (in particular the Department of Defense, the intelligence community, USAID, the BBG and the Department of State), the GEC mostly comprises Pentagon staff, who are more numerous and better trained on these issues than Department of State personnel.
There are also numerous task forces dedicated to the fight against disinformation and/or foreign influence in other departments, such as the Department of Justice and the Department of Homeland Security.
It is for this reason that some Democratic Senators deem it necessary to go a step further and they have urged the President, in a recently published report, to set up a high-level inter-agency “fusion cell” modeled on the National Counter-Terrorism Center to coordinate all elements of United States policy in response to Russian influence operations.
THE RESPONSES information manipulation, it does not produce counter-narratives but merely issues refutations of false information.
This new authority could absorb the counter-influence section of the MSB, which is one of the most well-organized and innovative structures that we have visited.
The efficacy of these structures depends on the means devoted to them, that is, on human and financial resources, which are themselves dependent on political will.
The involvement of Parliaments The United States and the United Kingdom have undertaken in-depth parliamentary inquiries to establish responsibility for the interference operations of which they were victims.
The public nature of those inquiries, which are the focus of close media scrutiny, has the advantage of increasing public awareness, providing them with very precise information as well as, arguably, having a deterrent effect.
Certain committees have produced authoritative reports, which greatly contribute to our knowledge of the problem.
The Singaporean parliament addressed the issue and, to introduce a new law against disinformation, they created in January 2018 a Select Committee on Deliberate Online Falsehoods— Causes, Consequences and Countermeasures, which has since conducted a great number of hearings, including with international experts.
This booklet is sent to every household in the country—these dedicated pages used to appear in the phone directory, however the shift away from print has reduced the dissemination of information, particularly in rural areas); • training of civil servants, journalists and companies (the MSB has already trained 11,000 civil servants); • media literacy (from 2018 onwards, all primary schools in Sweden will teach the basics of programming and develop pupils’ capacity to distinguish between reliable and unreliable information; Latvia 39.
Parliament of Singapore, Select Committee on Deliberate Online Falsehoods—Causes, Consequences and Countermeasures.
To qualify as disinformation, an informational activity must 1) contain deceptive elements; 2) have the intention to harm; 3) be disruptive; and 4) constitute an interference.
Such a diagnostic offer the possibility for both its users as well as public opinion to differentiate information manipulation from more sincere operations of influence.
Media Outreach The four main state measures relating to the media consist of registration, prohibition, regulation and denunciation.
Registration The United States uses the Foreign Agent Registration Act , a law originally adopted in the 1930s to counter Nazi propaganda, which requires that any entity engaging in political information and receiving foreign funds openly identify as such and disclose the nature of its foreign financial connection.
Christopher Livesay, “Italy Takes Aim At Fake News With New Curriculum For High School Students,” NPR, 31 October 2017; and interviews conducted in Rome on 30 th November 2017.
The FARA legislation is useful in that no ruling needs to be made on the content of the messages put forward by these “foreign agents” (it is not an instrument of censorship); it seeks, rather, to increase transparency regarding these actors’ sponsors, leaving it to citizens to form their own conclusions as to the credibility of the published messages.
However this system is less effective when faced with hybrid actors, who are neither companies nor registered lobbyists, and whose financial links with foreign powers are not easily demonstrable.
The difference, however, is that the American legislation aims at transparency and does not undermine the freedom of the media to conduct their work.
In Russia, by contrast, it is feared that the Russian law, under the guise of transparency, will exercise enough pressure on some media as to force them to shut down.
Ukrainian-language content quotas were also imposed for radio and television.
The Ukrainian government also created a Ministry of Information Policy, for which Kiev was widely criticized, not just by Moscow but also by the 42.
Jack Stubbs and Ginger Gibson, “Russia’s RT America registers as ‘foreign agent’ in U.S,” Reuters, 13 November 2017.
Regulation It is the middle-of-the-road option and the preference of most liberal democracies.
Numerous States have or are currently trying to introduce such legislation.
The most well-known is undoubtedly the German law known as “NetzDG” (for Netgn’erkdurchsetgungsgeset^), in effect since January 2018, which obliges digital platforms of more than two million members (Facebook, YouTube, Twitter) to delete “blatantly illegal” content within 24 hours or face fines up to 50 million euros.
Defining the object also poses certain challenges (especially if it is placed under a category as vague as “fake news,” as is often the case), as does finding an equilibrium with the protection of civil liberties and the freedom of the press.
In democratic countries, civil society, namely NGOs and associations of journalists, along with a certain number of parliamentarians, are often skeptical of the need and the effectiveness of introducing new legislation.
Daniel Funke, “A Guide to anti-misinformation actions around the world,” The Poynter Institute, 2 July 2018.
In Italy, for example, there is a portal that allows anyone with an e-mail and a link to the incriminated information to get the attention of the Polizia Postale, the police unit in charge of cybercrime.
The Thai government, through the Ministry of Public Health, launched a mobile application, “Media Watch,” developed by the Fund for Development of Safe and Creative Media for Mental Health, that allows anyone to report fake news.
The case of the United States Throughout the Cold War, the American institutional system developed a very sophisticated architecture with which to respond to Soviet information manipulation.
Countering disinformation and the Kremlin’s “active measures” became a priority for American national security at the beginning of the 1980s.
This architecture was dismantled after the fall of the Berlin Wall before being militarized after 9/11, in the context of the long War on Terror.
Since the attacks of September 2001, and in the absence of any public diplomacy arsenal comparable to that of the Cold War years with which to approach the ideological war waged against the United States by Jihadist groups like A1 Qaeda and ISIS, American counter-propaganda capabilities have largely centered around IOs and military counter-propaganda.
These actions are deemed all the more necessary in light of shrinking resources and the “bunkerization” or disappearance, in some operational theatres, of public affairs officers who are the main agents in the field of public diplomacy.
Directive 77 gave public diplomacy a key place in the foreign policy decision making process, and it affirmed a multidirectional strategy aimed at weakening Soviet influence by supporting the actions of dissidents across Eastern Europe.
The sole goal of this significant reorganization of foreign affairs was to successfully complete Project Truth, designed in 1981 by President Reagan and his advisors, to counter the effects of Soviet propaganda.
THE RESPONSES both national and regional commandment levels (Centcom, Africom, Pacom, etc.), can appear redundant or even counter-productive, which generates inter-agency controversies as to the distribution of responsibility, efficiency and the cost of both military and civilian “public diplomacy” activities.
Following Russian interference in the American electoral process of 2016 (characterized by the targeted use of internet platforms and social networks) and the establishment of the Russian state media Sputnik and RT in the United States, the American media have voiced deep concerns regarding what the political world perceives to be new strategies of Russian influence.
Such concerns reflect a deeper anxiety, widely shared across political, diplomatic and military circles in the U.S., of a deficiency in preparedness and coordination to respond adequately and proportionally to this new threat.
The atmosphere in Washington is redolent of McCarthyism and the official responses of the new “warriors of disinformation” seem to draw on the Cold War experience.
Already in January 2017, the former National Intelligence Director James Clapper, with the support of, among others.
Clapper’s call for a USIA “on steroids” only reinforced the confusion between influence diplomacy and counter-propaganda.
The failure of the various congressional endeavors initiated as of 2017 49 to articulate a coherent doctrine for American counter-propaganda results from the sheer diversity of actors engaged in the counter-offensive, 47.
Carlo Muno 2 , “Clapper calls for United States Information Agency ‘on steroids’ to counter Russian propaganda,” The Washington Times , 5 January 2017.
Coordination is rendered all the more difficult, for both internal organization and external policy, by the competing—and even sometimes conflicting—endeavors of agencies.
The Department of Homeland Security therefore created an anti-disinformation task force which became operational in January 2018 and thus far comprises a dozen staff Its mission is to better coordinate the various actions undertaken by different agencies, as well as to build capacity and involve private actors.
One of its roles is to coordinate actions with other agencies, including the DHS, the Department of State, the NSA and the CIA, while establishing relationships with the federal and local authorities, the private sector and digital platforms.
In July 2018, the NSA and the Cyber Command announced they would begin working together to fight against the threat of Russian interference, in light of the November midterm elections.
Acknowledging the lack of coordination, general Paul Nakasone, commander of Cyber Command and director of the NSA, declared he was doing everything he could in the absence of an “overall approach directed by the president” or the White House.
Most recently, studies conducted by several American think tanks and by the London School of Economics have endeavored to draw lessons from the Cold War legacy and apply those lessons to contemporary challenges.
As a matter of fact, the need to re-establish a para-governmental agency modeled on the USIA or an inter-agency coordination committee had become obvious long before suspicions arose of Russian interference in the 2016 American elections.
In particular, the need for greater efficiency was already sensed in the struggle against jihadist propaganda.
United States Department of Justice, Report of the Attorney General’s Cyber-Digital Task Force, 2018, P-8.
Ellen Nakashima, “NSA and Cyber Command to coordinate actions to counter Russian election interference in 2018 amid absence of White House guidance,” The Washington Post, 17 July 2018.
THE RESPONSES currently being floated in Washington, supported by the new “warriors of disinformation,” tend to rely too heavily on the Cold War experience of the 1980s in their interpretation of contemporary challenges.
They also seem to disregard some of the more worrying aspects that underpinned the Cold War era, notably in times of high tension, when it was deemed acceptable to use the weapons of the enemy and thus to overtly wage a war of information.
In 2016, Congress authorized the replacement of the Center for Strategic Counter-Terrorism Communications with the Global Engagement Center within the Department of State.
The purpose was mainly to counter ISIS propaganda and to endorse a strategy commensurate with the new information environment.
The driving idea was to foster cooperation between a greater number of public and private actors at both the national and international level.
The new Center’s most enthusiastic supporters wanted to quickly turn it into the main organ responsible for combatting the Kremlin’s subversive activities.
However, they quickly came up against the complexity of the American bureaucratic system.
The 2017 National Defense Authorization Act was designed to extend the prerogatives and mission of the GEC so as to include activities aimed at offsetting state propaganda, be it from Russia, China, Iran or North Korea.
Yet this inter-agency entity represents but one layer of the inherently multi-dimensional response to those challenges.
Contemporary debates on American influence and counterpropaganda capacities sometimes present an overly compartmentalized vision of public diplomacy programs, on the one hand, and military information operations , on the other.
However, public diplomacy and IOs are only two distinct facets of the overall United States strategic communication apparatus.
The Active Measure Working Group is the latest instance of an effective coordination on the part of the United States.
Based on this model, civil and military public diplomacy professionals have made the 52.
The National Defense Authorization Act was intended to expand the GEC’s mission by making it an organization fighting against “state propaganda,” whether Russian, Chinese, Iranian or North Korean.
The digital turn announced by the Department of State at the end of Georges W Bush’s second term was not successfully completed and the public-private partnerships initiated in those years and then substantially developed through the many initiatives undertaken under Hillary Clinton , while she headed to the State Department, must be carried on.
On this basis, diplomatic and military operational actors recommend turning the GEC into an equivalent of the Office of the Director of National Intelligence, so as to coordinate inter-agency work and synchronize operations.
In order to organize countermeasures in information warfare 3.0, they also deem it necessary to foster a global approach bringing together the whole range of institutional actors around a common strategy.
The recommendations drawn up during congressional hearings as well as those found in think tank reports emphasize the crucial role assumed by the chairperson of this interagency entity in the National Security Council’s decision-making process.
This chairperson could be granted a role at the highest federal level as Deputy-Secretary General or Special Advisor.
Indeed, without such close cooperation between the President and the chief of this entity responsible for the elaboration of public diplomacy strategies, responses to the Kremlin’s ongoing “active measures” risk remaining incoherent and ineffective.
Michael Lumpkin, coordinator appointed by Barack Obama in 2016, ex-Deputy Secretary of State for Special Operations.
THE RESPONSES Finally, the appointment of former CIA Director Mike Pompeo as Secretary of State could provide a good opportunity to reinforce interagency cooperation and the sharing of information in the fight against information manipulation.
This has allowed the creation of the Information Access Fund (since February), a support fund for citizen, entrepreneurial or paragovernmental initiatives.
The European Union At the European level, the rise of information manipulation has triggered a progressive response, which was dispersed at first between various institutions.
The issue was initially apprehended through the prism of external relations and the need to protect the EU’s image in the Eastern neighborhood.
European Council, Cover note from the General Secretariat of the Council Delegations on the subject of European Council meeting , EUCO 11/15, 20 March 2015.
By 2017, they had already identified over 2,500 instances of disinformation in 18 languages.
Despite its good results, this task force is under-resourced in terms of both staff and funding.
Comprising only eleven personnel, it exists and survives thanks to the good will of a handful of Member States that fund it and lend it members of their own staff.
Until recently, European institutions did not appear to be very involved and the EEAS has sometimes been criticized for not taking the Russian disinformation threat seriously.
Above all, these difficulties betray an absence of European unity on the issue.
Indeed, national approaches to disinformation remain quite varied across Europe, due to diverging perceptions of the threat.
They are followed by Czech Republic, Denmark, Finland, France, the Netherlands, Poland, Romania, and Spain.
For reasons that are both diverse and sometimes recent, these countries are most aware of this 56.
Philippe Regnier, “Tacler la desinfo russe,” Le Soir, 24 November 2016, 12.
Lastly there are Cyprus and Greece, who not only take no action to fight against the threat, but who even systematically block any effort to deal with the issue at the EU level.
The Commissioner established a high-level expert group which released its report on 12 March 2018, calling for increased transparency for online content, enhanced media literacy, the development of research and a closer partnership with civil society.
A multi-dimensional approach to disinformation, Report of the independent High-Revel Group on fake news and online disinformation, March 2018.
The Forum’s advisory committee will hand in a first draft at the beginning of September 2018 and will adopt a final version at the end of the same month.
The Commission intends to assess the Code’s implementation at the end of 2018 and it makes provisions for possible additional measures.
The Commission also states its commitment to safeguarding electoral processes, notably in anticipation of the 2019 European parliamentary elections, in cooperation with Member States, with whom primary responsibility for these matters lies.
However, up to the present day, no regulatory action has been announced, and the Commission is unlikely to put forward any legislative proposal before the end of the current term of office.
Furthermore, part of the funding allocated to the Horizon Europe program will be channeled into research on artificial intelligence and algorithms with the potential to contribute to the fight against disinformation.
Finally, the EU Intelligence and Situation Centre handles disinformation, perceived primarily as a threat coming from Russia.
Broader issues of influence within European agencies arise as well, for the positions of number 2 and 3 in these agencies, although quite crucial , are often sought and held by British officials.
In the eyes of INTCEN agents, the efficacy of malicious foreign influence actions—of which information manipulation is but one dimension—lies in a strategy of “trial balloons/wide targeting” which, by definition, can never lose as it is essentially cost-free.
INTCEN’s strategy consists first and foremost of exchanges of information, awareness-raising and the removal of barriers between relevant agents.
INTCEN established an information exchange network which enables it to raise awareness and refine the interpretative framework used by implicated actors so that they can better detect influence attempts.
The Hybrid Threat Fusion Cell emphasizes the need to anticipate with as few preconceptions as possible.
Its priority is to protect the vital infrastructures of Member States, hence the importance of having national correspondents.
As for the means at its disposal, INTCEN’s Hybrid Threat Fusion Cell comprises seven members and its approach is chiefly political.
The European Parliament against hostile propaganda The European Parliament “1.
Even today, the Alliance continues to view this phenomenon through the lens of the Russian threat.
Other attempts to spread rumors about the alleged crimes perpetrated by German soldiers in Lithuania under NATO command have been detected.
Within the NATO structure, there is the NATO Strategic Communication Excellence Centre , created in 2014 in Riga.
Their work centers on doctrines, operations and training, and they publish a large number of analyses.
A team of around a dozen people in PDD monitors disinformation operations with specialized tools.
Fault lines nevertheless arise between allies on the question of what sort of response is most appropriate.
They also disagree over whether or not to try to “beat Russia at its own game,” including within Russian-speaking communities, by spreading doubt about Moscow’s activities and goals or by offering a revised version of some chapters of history.
Such an approach is highly contentious within NATO, where there are diverging views on the severity of the threat that pardy reflect different perspectives on Russia’s role and the adequate NATO response to Moscow.
The OSCE The OSCE must take into account the positions of all participating States, including Russia, along with the very different situations of countries within the OSCE zone in terms of respect for the rule of law, fundamental rights and freedom of expression.
At the OSCE level, there have been no specific actions taken against propaganda within the framework of the commitment to freedom of expression.
The OSCE Representative for Media Freedom, Harlem Desir, approaches these questions from the angle of freedom of speech, by emphasizing the transparency of sources, media pluralism and media literacy.
This position has led him, for example, to reject the penalization of disinformation, which can be used as a means of repressing the freedom of expression in authoritarian regimes.
In March 2017, the OSCE representative for media freedom, the UN Special Rapporteur on freedom of opinion and expression, the OAS Special Rapporteur on the freedom of expression and the Special Rapporteur on freedom of expression and access to information for the African Commission for Human and People’s Rights published a joint declaration on freedom of expression and “fake news,” disinformation and propaganda.
The objective of this text was to remind States that the fight against disinformation must not be used as a pretext to limit 64.
Joint Declaration on Freedom of Expression and ‘Fake News, ” Disinformation and Propaganda , 3 March 2017.
THE RESPONSES civil liberties, in particular freedom of expression, above the level of that which is permitted by international law.
Civil society Civil society is at the front line with information manipulation.
Whatever measures States may implement, the resilience capacity of any society depends primarily on the mobilization of its citizens.
Responses by civil society were initially sporadic and reactive, mostly through factchecking.
Given the intrinsic shortcomings of this approach, civil society has developed complementary initiatives, involving either a longer time frame, a normative dimension or research.
Fact-checking Checking the veracity of facts is the most natural response to fake news and hence the most common one.
Among the oldest ones is the American site Snopes, which was launched in 1994 and has since become a reference.
The proliferation of fact-checking mechanisms is on the rise all over world.
The efficacy of government-led verification is, however, debatable as people who are prone to believing or disseminating fake news are often the very same ones who distrust public institutions.
There is a rather widely held belief that the State should, to as great an extent possible, refrain from directly engaging in fact-checking activities.
The one obvious exception is when crises that threaten public order occur.
It is, therefore, within civil society that a proliferation of initiatives can be found.
The Poynter Insistute’s international network for factchecking also adopted a “code” of common principles for guaranteeing transparent and unbiased verification.
Civil society also demonstrated its resilience in Ireland, during the referendum on the 8 th amendment and for the purposes of responding to the activities of automated anti-abortion accounts.
The tool was used by more than 4,500 users during the referendum campaign.
The power of fact-checking lies principally in its potential to embarrass those who disseminate false information, that is, in reputation-related reasons, rather than in ideological reasons.
These people like to be the first to disclose new information, and would be embarrassed in their communities if the disclosed piece of news turns out to be false.
Fact-checking works better on novel topics, to which we attach no preconceptions.
Rachel Lavin and Roland Adorjani, “L’lrlande a deja trouve la parade aux fake news (mais on ne pourra pas la reproduire),” op. cit.
The act of fact-checking may challenge the falsity, partiality or forgery of a piece of information, while also serving a pedagogical role—but it does not erase the significant psychological impact associated with the consumption of fake news.
Moreover, the results of the fact-check do not always, or even often, hit the target audience, in that the correction is seldom read by those who need to be convinced that a given story was untrue.
In particular, commercial objectives and/ or the desire to appear virtuous can sometimes take precedence over the search for truth.
All these limitations do not invalidate the importance of fact-checking.
It is a palliative measure, which must be complemented by other measures.
Normative initiatives We consume information the same way we consume food.
In this regard, the fight against information manipulation can draw inspiration from nutrition labelling.
This is what some have called the “Michelin model:” 72 labels, indexes and rankings can help to distinguish reliable media from untrustworthy sources.
Rather than relying on the identification and condemnation of the agents of disinformation, the initiative aims at “reversing the logic by giving an actual advantage to all those who produce reliable information, notwithstanding their status,” explains RSF Secretary General, Christophe Deloire.
Accordingly, the idea is to grant a quality label to those media who deserve it, that is, who respect a certain number of criteria, such as editorial independence, transparency, and professional ethics.
The media would thus be encouraged to meet these criteria so as to reassure advertisers who seek stable and non-contentious environments.
Digital platforms could, in the longer-run, decide to highlight quality content by putting forward certified media in their algorithms.
To cite a few examples, the Czech think tank European Values has been convening an annual StratCom Summit in Prague since 2016, which has become one of the sector’s most important gatherings.
In the United States, the Atlantic Council has set up a dedicated structure, the “Digital Forensic Research Lab” which has quickly become a benchmark in the field.
Working in partnership with the Bellingcat team, an online investigation platform, this lab performs an important role in detecting and investigating major 73.
Francois Bougon, “Un label pour redonner confiance dans le journalisme,” Ue Monde, 3 April 2018.
We can also add to the list the Alliance for Securing Democracy , a bi-partisan transatlantic organization whose goal is to respond to Russian interference attempts in democratic processes in the United States and Europe.
Created in July 2017 by former senior officials in the American intelligence services and the State Department, the ASD is part of the German Marshall Fund.
It is indeed possible to collect a great volume of verifiable information in open source and to then use this information in order to deconstruct distorted news.
This was demonstrated in a report by the CSIS, using the example of Syria.
The White Helmets published photos of incendiary cluster bomb fragments which Moscow denies using.
GMF Alliance for Security Democracy, Hamilton 68, Tracking Russian Influence Operations on Twitter.
This section of the video was cut out and is, therefore, missing from the version now available on YouTube.
Grassroots initiatives There are individual initiatives, such as the hashtag campaign #KpeMA£HaniyHCTopmoHeiiepemimemb (#Kremlin you will not falsify our history) launched by the Lithuanian writer and TV presenter Andrius Tapinas, as well as collective initiatives, such as the group acting as “elves,” in contrast to trolls—an online community of Lithuanian origin of around 4,000 activists.
Journalists Journalists are of course on the frontline of the struggle against information manipulation and they often participate, or even initiate, some of the aforementioned actions.
Two such journalists are Jessikka Aro, a Finn who studied troll factories, and the German Julian Ropcke, who investigates Russian influence in Germany.
Journalists also act collectively, through the creation of groups such as the Baltic Center for Media Excellence, whose objective is to raise journalistic standards and improve the overall media environment in the Eastern partnership countries; or the Re:Baltica portal and the Toneboard start-up, which received a grant from Google to create a platform for verifying fake news.
Private actors Large digital platforms have long shown a lack of interest in the struggle against information manipulation, which they presented as irrelevant in light of their “non-editorial status” and obligation to guarantee freedom of expression and trade freedom.
From a non-subject to a matter of serious concern The issue of the platforms’ responsibility for the nature of the content disseminated through them first became a matter of serious consideration in the context of the fight against terrorism.
In particular, digital platforms were publicly accused of permitting communication among terrorists as well as the circulation of shocking content aimed at unsettling users and/ or mobilizing their support.
It is of course necessary to distinguish terrorist content from information manipulation.
While in the case of the fight against terrorism, the principle of freedom of expression does not take precedence over the security imperatives, the terms of the debate are not as clear cut when it comes to information manipulation.
However, in both cases it is important to recognize that digital platforms are in a position to monitor the information circulated and exchanged through them.
They are equally capable of taking action to ensure that certain content is less visible or even eliminated completely.
In this regard, the 2016 American presidential campaign can be viewed as a double catalyst.
Firstly, the significant amount—in rubles—spent on political advertisement aimed at damaging Hillary Clinton’s campaign sparked a wave of increased awareness in large segments of the American political elite.
Accustomed to the vigilance of traditional media toward the sale and broadcasting of advertisements, especially when purchased from a foreign seller, United States political representatives were disconcerted by the small interest in the matter displayed by digital actors.
Their unwillingness to enforce any control or verification mechanisms for advertisements, despite being in a position to implement precise targeted advertising, was seen by many to be unacceptable and a blatant lack of responsibility.
A few months later, the Cambridge Analytica scandal strengthened this perception.
The issue at stake was no longer the sale of advertisements but the handling of personal data collected through Facebook, without the users’ prior consent.
Through this illegal harvesting, the company was able to implement particularly sophisticated micro-targeted advertising, aimed at shifting election results in favor of Republican candidate Donald Trump either by promoting his ideas and campaign promises or through denigrating those endorsed by the Democratic candidate Hillary Clinton.
Facebook is now criticized for failing to protect its users’ personal data.
Interestingly, the European model—usually derided in the United States Congress— has been explicitly commended for its attention toward this issue.
The roots of that realization do not just stem from the American Presidential election.
European States—who have often been criticized for being defensive actors with a preference for regulating innovation—saw a majority of Senators and the public agree with their cautious position.
On the other hand, digital platforms were cast in a less flattering light.
They were seen as indifferent towards privacy concerns and the operation of democratic institutions, as well as reliant on a questionable economic model.
THE RESPONSES What the Cambridge Analytica Affair reveals about tomorrow’s persuasion tactics “The recent so-called Cambridge Analytica case reveals that tomorrow’s persuasion tactics could be nothing like the old strategies of spreading rumors, and more like the targeting of each individual voter.
Indeed, this new method consists of ‘using data to change behavior,’ or in other words acquiring such a deep understanding of each citizen by combining a multitude of information on his behavior, personal ties, habits, desires, fears, etc., that the computer will be able to incite them to vote or to buy that which perfectly matches their needs.
I believe A, therefore I receive a message telling me that candidate Y thinks so as well.
Mark Zuckerberg’s hearing in April 2018 is without a doubt the acme of this collective realization, because it was the first time that the CEO of a major digital corporation found himself obliged to publicly answer for the functioning and responsibility of his firm.
Although many commentators continue to debate the long-term consequences of this recent wave of protest, for the moment Zuckerberg’s hearing has not resulted in the withdrawal of a significant number of users or a decline in the market value of these digital platforms.
It must be noted that platforms have hyped up publicity around their efforts to counter information manipulation.
The response of large online platforms to information manipulation Online platforms have developed a significant array of mechanisms against information manipulation, in response to—and hence in accordance with—the criticisms they face.
The intensification of these critiques has compelled platforms to put forward a great number of measures within a very short time span, without always having previously articulated a genuine response strategy.
For each of these examples, the stated goal is to give internet users “the tools” that will enable them to identify and respond to information manipulation themselves.
Moreover, a number of platforms—including Facebook—have reached out direcdy to various candidates in the presidential election so as to make them aware of the risks and encourage them to develop good internet practices.
More broadly, online platforms have also strengthened the protection of data privacy, for it appears that information manipulation campaigns are often based on the exploitation of personal data—either by stealing it or by tailoring it to their narratives.
Finally, through public hearings, large online platforms contribute to raising public awareness of the need for increased vigilance against information manipulation.
Improve the detection of information manipulation Information manipulation campaigns often rely on automated accounts , networks of automated accounts and anonymous accounts.
Whereas platforms used to be reluctant to identify and deactivate the latter—for a number of reasons (economic model, editorial neutrality)—they have recently shifted their approach.
They started by taking a closer look at their users’ accounts so as to suspend accounts that were fake, automated, and/or suspected of participation in an information manipulation campaign.
Twitter announced that it had suspended over 50,000 accounts “connected to Russian interference,” to quote the company’s spokesperson.
However it is difficult to assess the effectiveness of these account suspension campaigns.
Facebook had adopted a similar approach in 2012, by launching a large hunt against fake “likes,” though with only limited success.
Since 2016, the fight against “the fake” has become the object of a war of figures, the effectiveness of which is difficult to assess.
In cooperation with the United States government, Facebook, Twitter and Google have also set up an initiative aimed at creating a common database listing fake accounts and the strategies developed by trolls to escape identification.
The goal is to optimize information manipulation detection by exchanging information on the models and actors behind it.
Other measures, relying on artificial intelligence, have been implemented to detect and suspend these accounts, sometimes even before they are activated.
Along the same lines, Facebook also announced that they had developed a tool enabling them to detect the serial publication of similar messages and comments.
Contain the dissemination and impact of information manipulation campaigns Online platforms have developed several measures to speed up the removal of malicious content.
While techniques based on artificial intelligence are used as preventive measures (before the content is published online), platforms continue to rely on human involvement to monitor—and sometimes erase—exchanged content and dubious advertising.
Facebook has also increased the teams dedicated to verifying dubious content by over 60%, with a total staff of 8,000 people globally.
While this reinforcement of human involvement is significant, it must be noted that it primarily concerns the monitoring of content deemed illegal and/or related to terrorism.
In July 2018, however, Facebook announced the implementation of a “new policy” of deleting content susceptible of causing violence, starting first with countries where disinformation has triggered violence, 78 such as Sri Lanka, for example, where messages claiming that Muslims were poisoning Buddhist food were erased from social networks.
Twitter also sped up the cleaning process, through the introduction in May-June 2018 of new mesures to combat trolling and hateful and extremist comments, 79 and the suspension of at least 70 million accounts in only two months—twice as high as the suspension rate in October 2017.
Google has recently introduced tools enabling its users to report “misleading and false” content.
Facebook now grants greater attention to the feedback and comments of web users who have identified fake information.
Yoel Roth and Del Harvey, “How Twitter is fighting spam and malicious automation,” blog.twitter.com, 26 June 2018.
THE RESPONSES this framework is to objectify response thresholds (when and how to respond), coordinate the work of various teams and enforce a standard response procedure.
The latter includes, in particular, recommending fact-checking articles (which deal with the same facts as the dubious content posted online and enable users to take a step back from the fake information); notifying the number of other users who deemed the information to be false or misleading; and an alert that the user is likely to relay false information.
In the same vein, YouTube chose to display, next to some conspiracy videos, a link to a Wikipedia article directly challenging the conspiracy narrative.
Finally, online platforms also developed tools to detect the “deep fake,” i.e. fake news that can very convincingly reproduce the effects of reality.
Google has announced that it has created a tool capable of detecting such videos and of removing such content before it is posted online.
Regulate and cooperate For a number of reasons , online platforms are wary of regulation.
They tend to favor informal cooperation with public authorities and the media.
Google reported having done the same during the American and French presidential campaigns.
Facebook also declared that it had collaborated directly with the German government during the most recent general elections.
Other countries are also considering implementing legislative mechanisms that would oblige platforms to act more decisively against contentious content.
Promote good practices and institutional actors Platforms very often choose to promote constructive approaches to counter information manipulation.
A policy favored by many of them consists in reinforcing the visibility of reliable content and/or those produced by trustworthy media sources in their search engines and news feeds.
These various elements of information, which appear as tabs, seek to highlight the ethical standards and the trustworthiness of these various media sources.
Twitter has undertaken to develop, for example, indicators that make it possible to monitor the diversity of exchanged opinion, the receptivity of users and media awareness of the issue.
Analyze the mechanisms of information manipulation campaigns In the face of stark criticism for their naivety and lack of discernment towards the impact and the scale of information manipulation campaigns, the platforms have emphasized their need to better understand the phenomenon.
To achieve this, they have put in place various partnerships and exchange policies with the world of research.
Likewise, the platforms contribute to the funding of initiatives aimed at developing a better awareness of ethical issues linked to platform usage, including in the field of information.
While the social platforms have come to grips with the fight against information manipulation, there remains a lot of work to be done.
As The Wall Street Journal recalls, Twitter CEO himself, Jack Dorsey, shared at least 17 tweets from a Russian troll between late 2016 and mid-2017.
The contribution of the field of advertising and marketing research The field of advertising is often presented as a stronghold of disinformation, in that it seeks to manipulate the mind for profit and commercial purposes.
Such questions are at the very root of not only information manipulation techniques, but also effective responses to this manipulation.
The tools developed by advertising and marketing research arguably present at least two advantages as regards the study of information manipulation.
First, by analyzing the response of target groups to particular campaigns, these tools enable us to better grasp the impact— visual, emotional, rational, and intellectual—of any given message on a particular audience.
Furthermore, by highlighting the weaknesses and “voids” of published messages, as well as the reasons why a particular user is attracted to one particular message over another, these tools inform us on the manner in which conventional media, that are perceived as reliable, can better their appeal and capture the attention of those audiences who bypass them.
Georgia Wells, Rob Barry and Shelby Holliday, “Russian Trolls Weigh In on Roseanne Barr and Donald Trump Jr.”, The Wall Street Journal, 19 June 2018.
Studies aim to predict the effectiveness that an advert is likely to have on the market by analyzing the audience’s attention levels, their connection to the brand, the entertainment generated by the ad for the user, drops in attention levels as well as the emotions produced.
These tools also make it possible to identify the weak points of a given campaign.
Studies are also conducted ex-post so as to monitor the evolution of preferences among various populations—in particular, among younger generations—untapped consumers to be ensnared.
Such tools include interviews with sample groups as well as the monitoring of the user’s perceptions of an advertising campaign.
Were they to be applied to information manipulation, these various techniques would arguably provide a strong explanatory dimension.
One could imagine conducting a similar study to compare the respective trajectories of a conventional article and a “distorted” one.
An analysis of the attractive power of the false information might thus allow us to strengthen the influence of reliable media on a variety of audiences.
Devices such as “eye tracking,” which measures the user’s eye movements, also enable us to better grasp users’ responses to fake news in order to better implement counter-measures.
Marketing studies, for their part, focus on analyzing market availability to particular products.
Those who manipulate information already use a marketing technique called the “A/B test,” which consists in comparing the impact of two variables, in this case two messages.
For example, manipulating actors would start by circulating two messages stating that “black people are terrorists” and “black people are criminals” and, realizing that the latter works better than the former, they would then bank on this second message and continue to refine it so as to improve its potential to go 83.
Paurav Shukla, Essentials of Marketing Research, Ventus Publishing ApS, 2008.
Despite the bad press often associated with it, advertising and marketing research can offer interesting perspectives in the fight against information manipulation.
The fight against manipulation also requires us to strengthen reliable conventional media sources, through a better understanding of their shortcomings and the efforts that are required to make them attractive to publics who are turning away from them.
For all of these reasons, we should expect information manipulation to expand and involve an ever-increasing number of actors.
Had someone said ten years ago that the recently created social networks (Facebook in 2004, Twitter in 2008, Instagram did not exist yet) would play such a tremendous role in the lives of billions of people and be implicated in a massive information problem threatening our democratic life, hardly anyone would have believed them.
It is, therefore, difficult to imagine what it is that will—ten years from now—shape our social interactions and pose the most serious challenges.
For instance, it is possible to imagine that the use of currently open networks will decrease and the use of closed networks will increase.
This scenario would pose different types of challenges to public authorities, particularly in terms of encryption.
Artificial intelligence will make bots more human and, therefore, harder to detect.
It will also progressively erode linguistic and cultural barriers (which remain a shield against foreign influence attempts for some countries), thanks to the enhancement of translation software.
Photo, audio and video editing software, for example, will render it possible in the near future (some of them already do so today) to make anyone say anything.
The United States Department of Defense also identified these altered videos as an issue in the midterm elections of 2018.
As such, the United States Defense Advanced Research Projects Agency even provided funding for the Media Forensics Project, whose goal is the development of technologies capable of automatically identifying and targeting these Deepfake videos.
An even greater danger, far more subtle than the creation of a fake video, arises from the slight alteration of only a part of an audio or video clip, such as a recording of a speech.
Another peril lies in the possible creation of a great number of variations of that speech—e.g. the circulation of twenty different versions of a single speech, so as to hide the authentic version in the confusion.
Artificial intelligence will enhance the sophistication of fictional personalities and make them less readily detectable.
These personalities will be able to give interviews and write columns in the press before they are uncovered.
Ben Collins, Joseph Cox, “Jenna Abrams, Russia’s Clown Troll Princess, Duped the Mainstream Media and the World,” The Daily Beast , 11 February 2017.
Even progress in social psychology research, in particular in terms of the way in which we make decisions, can be “arsenalized,” allowing us to carry out micro-targeting in a more precise and efficient manner.
The strength of these three combined ingredients—knowledge in social psychology, big data and artificial intelligence—can be used to create a weapon of mass division.
Future trends in Russia's ""information warfare"" It is, by definition, difficult to anticipate the next move by actors with a distinctive capacity to tailor-make their actions and learn from their mistakes.
Kinetization We already observe a growing interest by Russian actors on the physical level, that is, the communications infrastructure.
While this interest did not appear during the annexation of Crimea, it was undoubtedly reinforced by this operation, during which Moscow intervened directly in the information flow received by the population of the peninsula by literally cutting some internet and phone cables.
The Crimean case nevertheless remains a unique case study, due to the peculiar geography of the region and Russian intelligence services’ prior knowledge of the territory.
In the long-run, the two physical layers that are of greatest interest to Moscow are the submarine—for the cables which, as we have known for years, can be pirated—and the spatial—for the satellites around which some maneuvers have on occasion been observed.
We can, therefore, expect a greater overlap between the kinetic and non-kinetic dimensions of Russian operations.
Personalization There is a trend towards the personalization of attacks.
This technique is not new, as is evidenced by the Soviet and Russian services’ use of the “kompromaf method,e. compromising a target who can thus be controlled and manipulated.
In the field of information, the focus of this report, this trend could take the following guises in the years to come.
Then, several minutes later, their families receive a message announcing the death of their son, brother or father, at the hands of the enemy—which then brings families to call the soldiers, and allows through the concentration of signals to detect their location and bomb them, 3 in a tragic self-fulfilling prophecy.
A resurgence of Russian activity aimed at Western soldiers in external operations has also been observed.
For example, military forces deployed in Baltic States in the context of NATO’s Enhanced Forward Presence have been targeted.
This activity involves traditional methods (using a physical approach) but also some more innovative approaches.
The latter rely, for example, on the exploitation of soldiers’ personal data via social networks.
Such personalized attacks could also target civilians, be they politicians, senior officials or prominent public figures.
Massive leaks, all too obvious in the wake of the “DNC Leaks” and the “Macron Leaks” are, therefore, less of a risk.
Mainstreamization The range of media which spread the Kremlin’s doctrine, sometimes inadvertenly, continues to expand.
Those media sources which appear closely tied to the Russian government and/or who too obviously defend its positions are now clearly identified as propaganda organs.
Liam Collins, “Russia Gives Lessons in Electronic Warfare,” Association of the United States Army, 26 July 2018.
FUTURE CHALLENGES audiences are widening, they could face greater competition by other forms of information dissemination.
The Kremlin is likely to invest more intensely in “converting” personalities who are not known to be proRussian.
The Kremlin may also try to work some of its messages into the larger, more traditional media outlets.
This would result in a Russia’s information warfare going mainstream, which will be more difficult to counter.
Proxyzation With Europe and North America becoming both obvious targets as well as spaces saturated with counter-measures, with highly educated populations with high levels of awareness regarding this phenomenon, we expect that the battlefield will expand to include new fronts as previously identified , particularly in Africa and Latin America.
Their communities are also ripe with passions that are easily exploited, such as ethnic and religious tensions as well as resentment towards old colonial powers.
As such, in its effort to weaken Europe, Russia may use these populations as proxies.
This phenomenon is already at play in the Maghreb, where there are massive Russian investments, not only in the energy sector.
Populations in the Maghreb are largely exposed to propaganda from the Russia media in Arabic, which conveys anti-European messages.
These populations serve only as an indirect target, or a vector; the real objective is for these populations, who interact on a daily basis with family and friends living in Europe, to relay these messages back to Europe and convince others that the European media is lying and that the Europeans are hostile towards them.
The anti-immigration propaganda in Europe today, which works to agitate nationalist communities, is therefore only one facet of the operation.
To create division and pit communities against each other, it is also necessary to convince immigrant populations that they are mistreated.
These tendencies, which are only likely to increase in coming years, are of serious concern.
In the context of the “citizen consultations” on Europe, which began last April and will end in October 2018, coordinated actions are deployed, combining online manipulation—a massive dissemination of false information and posts via bots—and the sponsoring of physical “Trojan horses”—identified in advance as proponents of opinions favorable to the manipulating interests—so as to foster a radicalization of the debate and/or jeopardize the credibility of the consultations.
In a second phase, the content of the reports to the European Economic and Social Committee could also become the target of an information manipulation and/or a denigration campaign, through the automated propagation of the most subversive posts.
As the post-Brexit negotiations promise to be long and difficult, there are attempts at email hacking which would reveal the contact details of the political representatives and officials in charge of the negotiations, including their confidential correspondences.
If successful, these attacks could result in the selective dissemination of—potentially falsified—content so as to discredit the negotiation process and/or spread the seeds of discord between European partners, and between the EU and the United Kingdom.
One-off information manipulation campaigns are also likely to target specific points of the negotiations, so as to spark emotional responses from the British 4.
FUTURE CHALLENGES and European public, and thus reinforce mistrust between the people and the European institutions.
Several information manipulation campaigns are launched in order to exacerbate tensions between EU Member States.
One of these is targeted at the content of the Visegrad group meetings, crediting the Central European States with intentions they do not actually have—in particular, on issues of “illiberal” democratic governance or on differences in foreign policy matters—and thus reinforcing mistrust between Eastern and Western European States ahead of the 2019 European elections.
Another campaign could target the EU leadership and the reform ambitions borne by the Franco-German partnership, and reinvigorate the intra-European divisions generated by the Eurozone crisis by spreading news of purported projects aimed at bringing to heel the Southern Member States (Italy, Spain, Portugal and Greece), notably on issues of monetary governance and renationalization of public spending.
Attacks specifically targeted at France take place in order to undermine the government by creating one or several major political scandals.
Define and clearly distinguish the terms, as we sought to do in the introduction.
This should help counter widespread relativism, in other words, the claim that “everything is propaganda” and that all the media spreads disinformation.
We must not condemn the defense of national interests—Russian media have a legitimate right to defend Russian viewpoints, including those of the Russian government—but the information manipulation.
Do not underestimate the threat, even though it may not be perceptible on an everyday basis.
The Finnish Security Strategy for Society insists that a good preparation against information manipulation depends on an accurate evaluation of the threat.
INFORMATION MANIPULATION threatening scenarios and planning for the potential risks and conflicts that they would involve.
The short-term goals relate to specific events, often an election, an armed conflict, a social protest, a natural disaster, an assassination or an attempted assassination , a plane crash , etc.
Fake internet accounts and hoaxes are thus more conspicuous, more aggressive, and less subtle because they have an inherently limited lifespan and are bound to be exposed or suppressed once the goal has been achieved.
Long-term operations, on the other hand, are aimed at undermining certain ideas and opinions, or at exacerbating tensions and divisions within targeted communities.
They have insidious, incremental subversive effects, steered by more subtle and discreet actors, and with consequences that are more difficult to assess.
Hence it is important to go beyond short-term approaches, often through the prism of electoral cycles (i.e. that tackle only those informational threats that arise during elections), in order to understand the daily nature of the challenge.
Information manipulation feeds off of divisions and tensions that run through the fabric of our societies.
Hence, we cannot fight back effectively, or durably, against these forms of manipulation without the political will to increase resilience within our societies.
From this point of view, we have much to learn from certain States, in particular Finland, who has made resilience against so-called “hybrid” threats into a national concept.
Conspiracy theories prosper all the more easily if they are not contradicted.
Lessons From Recent Finnish History,” Carnegie Endowment for International Peace, 8 February 2018.
Every correction indirectly increases the circulation of the false information.
This propagation effect cannot be avoided and it is therefore important to pick one’s battles, that is, to focus on counteracting those instances of information manipulation that are most dangerous.
Civil society (journalists, the media, online platforms, NGOs, etc.) must remain the first shield against information manipulation in liberal, democratic societies.
The most important recommendation for governments is that they should make sure they retain as light a footprint as possible—not just in keeping with our values, but also out of a concern for effectiveness.
As one of the roots of the problem is distrust of elites, any “top down” approach is inherently limited.
It is preferable to champion horizontal, collaborative approaches, relying on the participation of civil society.
Those that have not should establish a national entity responsible for the detection and countering of information manipulation.
This entity can take various forms—from the network of competent people presently scattered across distinct services to the creation of a dedicated center endowed with its own staff.
As it may involve bureaucratic rivalries, one crucial aspect relates to the issue of institutional affiliation.
In the present international landscape, some entities are supervised by an inter or supra-ministerial body, while others are hosted within a particular ministry.
The nature of the link (executive powers or merely a secretariat role) also varies.
Extracts from a lecture on counter-propaganda given by the Head of Information Research Department in a secret series of lectures on Communism, SECRET , no.
PR 89/45 G, TNA FCO 141/7460, September 1952, on psywar.org, 30 April 2012.
When the group is too large or too hierarchically heterogeneous, discussions tend to diminish in quality and efficiency.
The interdisciplinary team should also include information system experts who tend to be confined to crisis resolution while they should in fact take part in the strategic thinking.
In those countries that are most exposed to foreign pressure, in Eastern and Central Europe and, more particularly, in the Baltic States, the role of the security and military forces is emphasized.
How to Tailor National Strategy Using Lessons Learned from Countering Kremlin ’r Hostile Subversive Operations in Central and Eastern Europe, European Values, Kremlin Watch Report, 30 April 2018, 3 and 28.
In Canada, the brunt of responsibility in the fight against disinformation falls unto the Ministry of Democratic Institutions—insofar as information manipulation threatens elections, and thus the integrity of democratic processes.
Notwithstanding the institutional affiliation of the dedicated entity, the Ministry of Foreign Affairs has an important role to play in monitoring and providing early warning, especially in instances of malign campaigns targeting national interests abroad.
Nevertheless, they can be detected and the goal is to do so as early as possible.
These probes can be passive accounts, which only listen, or active ones, which take part in discussions.
Clandestine operations, aiming for instance at manipulating the manipulators, are risky because, if exposed (and it is becoming increasingly difficult to prevent this in the long-run), they can jeopardize the very credibility of the source and invigorate conspiratorial actors—which would end up strengthening the very actors one aimed at undermining.
Such painstaking work is essential in order to understand the channels of propagation, but also to enable anticipation and adequate action.
In order to win this war, it is not only necessary to ensure a continuous presence on the web, to have a communication strategy, disseminate targeted messages, and be able to refute false information.
It is equally important to be proactive by drawing the adversary out of their comfort zone.
For example, whenever government services detect trolls or dormant bots, they should be exposed publicly before they are even used.
It is possible to condemn an attack without revealing its source and then leave it to the media to do their work.
This was also the approach the Germans adopted during their pre-electoral period.
Proactive communication is now widely recognized as the strategy to follow.
For States who do not have English as their official language, it is also important to communicate information in English about their doctrine, national strategy and experience.
The public has a right to know who speaks, similar to the logic that prevails in matters of food safety—the traceability of information must be a measure of its quality.
Develop our legal system “I have decided that we would make changes to our legal system so as to protect democratic life from fake news.
During the electoral period, platforms will be required to meet obligations of increased transparency regarding all sponsored content so as to make public the identity of advertisers and those who control them.
The regulating powers, which will be thoroughly reshaped in 2018 , will be increased to manage attempts at destabilization by television services controlled or influenced by foreign States.
This will allow the reworked CSA [French media regulatory authority] , in particular, to refuse to conclude agreements with such services by assessing the content published by said services, including on the internet.
It will also enable the regulator, in the event of an act likely to affect the outcome of the ballot—whether in the pre-election or election period—to suspend or cancel an agreement.
In other words, we must preserve the equilibrium between protecting the population and respecting civil liberties, which are the foundations of our liberal democracies.
We must be mindful of the risk of our actions having such unintended effects.
The American and British examples show that public inquiries offer many benefits in terms of raising citizens’ awareness, accumulating knowledge, and providing deterrence.
The role of social networks in information manipulation is now widely recognized.
They have become the principal source of information, and hence of disinformation, for a majority of the population.
Although information manipulation is costly for their reputation and despite the self-regulation pledges these platforms have made in recent times, it is unclear whether digital platforms actually want to curb these practices.
It is up to legislators to strike the right balance between freedom of expression and the need for a greater accountability when it comes to digital platforms in the fight against information manipulation.
We cannot, on the one hand, wait for digital platforms to do more in the fight against information manipulation while, on the other hand, not providing them with information that is sometimes necessary for them to move forward.
Public-private cooperation is of capital importance and demands knowledge-sharing in both directions.
This is one of the recommendations made to the Trump administration by two former senior officials of the Obama administration, in the context of the midterm elections of 2018.
Central, Eastern, and Northern European States alongside the U.K. and the United States.
France and Spain are in the process of stepping up their international presence because they too have been the target of attacks.
Other States should not wait to be attacked; they should become more active now.
Not only are authorities proactive and outward-looking, as is demonstrated by the parliamentary hearings and the fact that the Ministry of Defense will soon be sending a resident expert to the NATO Excellence Center in Riga, but so too has civil society been actively involved.
The Centre of Excellence for National Security at the S Rajaratnam School of International Studies organizes an annual seminar on disinformation which is one of the very rare meeting points between research and practitioner communities from Europe, North America, Asia and Africa.
This diversity is quite refreshing for those accustomed to the Euro-Atlantic scene, which tends to only view the subject through the Russian lens.
Each situation is, of course, unique (information manipulation in India, Burma or Indonesia are concerning but endogenous, and thus far removed from the Russian interferences in Europe and North America), but as China presents itself as an ever-increasing threat in the region, such as the Australian case illustrates, there are interesting parallels with Russia to be explored, including to find out what these two countries are learning from each other. c) innovate through the creation of new mechanisms.
An international early warning mechanism could be established, connecting all of the networks, centers and agencies of the EU and NATO Member States.
Somegroups, mostly in the United States, have suggested the creation of an international coalition.
Democratic United States Senators recommended the creation of “an international coalition against hybrid threats,” which would be spearheaded by the United States.
They urged the American President to convene an annual world summit on hybrid threats, modelled on the summits of the Global Coalition against Daesh or against violent extremism, which have been 50 RECOMMENDATIONS held annually since 2015.
Representatives from civil society and private actors would be invited to take part.
However, articulated in these terms, it appears problematic, not just because it excludes Canada, but because such a transatlantic alliance already exists and also because it would require an explanation to Moscow.
The coalition would run the risk of looking like an anti-Russian—rather than an anti-disinformation—alliance.
Existing structures, within the EU or NATO, are less susceptible to such criticism.
This Commission is a new actor worth watching, even though it is too soon to assess the role it will play.
Finally, the G7 offers an obvious platform from which to share best practices and formulate common approaches to countering information manipulation.
Canada made the issue one of the priorities of its Presidency of the G7 in 2018, by proposing various mechanisms for exchanges and joint action.
France, which will take over the G7 Presidency in 2019, should build on these initial results in order to carry out the joint efforts begun within this forum, which are predicated on the preservation and defense of democracy.
The promotion of media literacy in schools stands as one of the most widely agreed-upon recommendations, despite its unequal application by governments, as can be demonstrated by the Open Society 14.
However, if we strictly limit ourselves to media-literacy obtained through schooling, as is often the case, it is a long term measure whose effects will only be visible once the children reach adulthood.
It is important to consider media literacy and, more broadly, the development of critical thinking, for the whole population, at all stages of life.
The education of teenagers and students is particularly important because they tend to be the most vulnerable to information manipulation for a variety of reasons (lack of experience, the need to assert independence, socio-cultural environment) and they have not necessarily benefited from media literacy training in their early years.
Offering a core curriculum first-year course in university (text and image analysis, identification of sources) would be useful and easy to implement, at least in social sciences programs.
This is a public hygiene measure—just as people in the 19* century learned to wash their hands.
One possibility would be to follow the Swedish model and publish a “digital hygiene guide” for use by politicians and political parties.
In other words, it is crucial to educate the general public from a very early age but also at different stages of life, about image, audiovisual media, critical thinking and rational argumentation.
Courses in critical thinking and rational argumentation are widely available in some countries and even considered an indispensable prerequisite in university.
These courses teach students how to recognize a paralogism or a sophism and to detect fallacious reasoning.
In school, children should be taught how to construct as well as deconstruct false information and conspiracy theories.
Resilience to Post-Truth’ and its Predictors in the New Media Literacy Index 2018 , March 2018.
Normand Baillargeon, Petit Cours d'autodefense intellectuelle , Montreal, Lux, 2005.
Children should also learn to use Google image so as to verify the source of any given image.
There could be awareness-raising messages played before YouTube videos or sent by digital platforms as private messages, e.g. on Snapchat or Instagram. e) It is possible to reach out to adults through public campaigns around particular events or through training programs.
In that regard, the activities of the NGO Baltic Centre for Media Excellence, which trains journalists and teachers across the region, provide an interesting example.
In public service and, in particular, in the Ministries and services most concerned, it is crucial to train staff members so as to reinforce overall “digital hygiene” and develop an internal expertise enabling them to act in an autonomous manner.
This involves new recruitment criteria as well as a new range of training programs, public-private partnerships and mobility programs enabling civil servants to acquire new skills from innovative companies.
Institutions similar to the French Institute for Higher National Defence Studies could offer training sessions dedicated to informational threats.
In this way, games, such as the ones developed for Facebook by the NATO Strategic Communications Centre of Excellence, can be quite effective at garnering the interest of young and old alike.
Our immune system against information infection is not only grounded in a capacity to monitor and analyze the information space—which requires us to allocate more intelligence resources to these activities—but also in an ability to comprehend those who manipulate information and, above all, Russia.
Therefore, it is necessary to support research on Russia and the post-Soviet sphere at large.
This does not mean reviving “sovietology,” but acknowledging that it is possible to respond adequately only to that which we understand well.
In concrete terms, this means that States must increase research funding and introduce calls for tenders aimed at studies on predetermined topics or even fund PhDs and/or postdoctoral research projects as well as events and publications on the subject.
Russia Today and Sputnik have been organs of influence during this campaign that have, on several occasions, produced untruthfull statements about myself and my campaign It is a matter of serious concern that we have foreign news organizations—under whatever influence, I do not know—interfering in a democratic campaign by spreading serious lies.
Emmanuel Macron during a joint press conference with President Vladimir Putin at Versailles, 29 May 2017.
Counter-measures are often criticized for not being entertaining and, for this reason, missing their target audience.
On the other hand, stories involving false information are usually amusing.
RT and Sputnik practice “infotainment,” a combination of information and entertainment, compared to which corrective measures can appear very stern.
Yet experience in Europe and North America tells us that humor, satire, jokes and mockery work remarkably well against information manipulation.
Information manipulation exploits the vulnerabilities of our democratic societies.
For this reason, it is necessary to map out, locate and understand these vulnerabilities in order to anticipate and try to prevent hostile actions.
The ability to put ourselves in the shoes of the adversary is, therefore, essential in order to better predict their next moves.
To this end, we must not only study then by research and intelligence but also test our procedures through “red teams,”e. teams that play the part of the opponent by trying to identify and manipulate our weaknesses.
Information manipulation tries to systematically instill doubt in the values and principles of the communities it targets.
The best way to combat these manipulation attempts are, firstly, to have a clear idea of what we wish to protect.
Acknowledge the unavoidable reversal and diversion of our counter-measures.
It is important to recognize that our countermeasures will, in turn, be manipulated by the enemy.
Therefore, it is necessary to encourage positive approaches that promote the free circulation of high quality information, in contrast to the fragmentation that currently dominates the internet.
Establishing a regular and open dialogue between journalists and policymakers can help to fight against information manipulation.
In Sweden, a Media Council meets on a regular basis, bringing together media leaders and politicians to identify the challenges they face and, crucially, to coordinate their factchecking efforts.
Information manipulation is but one element in a complex system; it feeds off of other forms of influence.
In the case of Russia, targeted States should reduce their energy dependence on Russia as well as target corruption and the Russian financial circuits that contribute to the funding of influence operations.
In external operations, nurture relationships with the local population.
It is important to never forget that “every action projects an image, generates a perception for the adversary, for local populations 20.
Europe’s Counter to Fake News and Cyber Attacks,” Carnegie Endowment for International Peace, May 23 2018.
Information manipulation is both a cause and a symptom of the crisis of confidence in the digital arena.
Effectively fighting against these manipulations will have the end result of increasing confidence.
At the same time, this first requires having an understanding of the psychological mechanisms that underpin trust, by placing oneself in the users’ position, and promoting good practices that will build trust.
In this way, it would be useful to seek enhanced cooperation which would allow the establishment of reliability indices for online content.
Secondly, the correction must entail an explanation of why and how disinformation was spread.
Develop normative initiatives while recognizing that a proliferation of competing norms and standards will only weaken the overall effort.
Therefore, the objective should be to put forward a small number of tools of reference, possibly in connection with reputable NGOs.
The Reporters Without Borders initiative is, in this respect, very promising.
The majority of major media platforms have charters of good editorial practices and ethics.
The 1971 Munich Charter can provide a useful foundation, but it needs to be adapted to the contemporary media landscape and, notably, the rise of digital media.
Train journalists to better understand the risks of information manipulation, in journalism schools and throughout their careers.
There are concrete answers to these questions, 26 which may serve as a basis for teaching material.
Thetrustproject.org 50 RECOMMENDATIONS the profiles of the journalists, proof of their expertise on the subject matter, providing clear a distinction between an opinion, an analysis, or sponsored content, how the sources were accessed, why the journalist chose a particular hypothesis over another, etc.
The idea is that the readers want to know how journalists work, and how they know what they know.
This transparency in terms of practices, methods, and journalistic procedures can help to build trust.
Develop tools with which to counter “trolling,” such as Perspective by Jigsaw, which uses machine-learning and self-learning as tools to identify toxic messages that can then be isolated, stopped before publication and then submitted to moderators.
Another method consists of the publication of lists of accounts identified as trolls.
Use artificial intelligence and automatic language processing tools in the detection of manipulation attempts and fact-checking.
Detection software, such as Storyzy, are continuously multiplying and being perfected.
With respect to fact-checking, certain software can automatically compare the suspicious news story with all others that were already “debunked” so as to avoid repeating the same work for nothing.
This assumes that there is shared access to databases— hence the need for verification networks.
Automated verification saves time, but nevertheless still requires, for the time being, a human at the end of the process to validate its results.
Develop surveys and polls aimed at assessing public sensitivity to information manipulation.
Collecting precise data on a regular basis would improve the effectiveness of counter-measures.
Rethink the economic model behind journalism, so as to reconcile the preservation of freedom of expression, free market competition and the fight against information manipulation.
In line with this exercise of disseminating research, higher education institutions must also organize media training courses, to teach the specific skills needed to best interact with the media.
Moreover, the dissemination of research must be increasingly valorized in the career, as well as constitute a major criterion for evaluation, in order to incite academics to practice this exercise.
The possibility of an anti-trust regulation proposed by the European Commission expert group (see above) also deserves consideration.
Demand the establishment of a new contract with users that is founded on new digital rights.
The terms of reference must be reassessed so as to make them intelligible to all and more explicit as regards issues of access to and management of personal data.
In the aftermath of the Cambridge Analytica scandal, wishful appeals for more transparency are no longer good enough.
Internet users must be informed of the campaigns that can affect them and the reasons for such targeting.
Given the challenge this poses for democratic life, political advertising connected to the exploitation of big data must be subjected to specific regulation.
In this context, the possibility has been raised of establishing a public mediator who would be granted access to algorithms under the condition of strict confidentiality.
Increase the cost of information manipulation while ensuring the protection of vulnerable individuals and movements.
More systematic action must be undertaken against the agents of manipulation, drawing on the concept of “threat actor,” a term that comes from the field of cybersecurity.
Rather than censoring contentious content one by one [a “whack-a-mole approach”], platforms could conduct inquiries that lead to the identification of a hostile actor and then suppress all of those actor’s online outlets.
Whistle-blowers and organizations that are targeted by an information manipulation campaign must, on the other hand, be warned in advance through a special detection system.
They must also benefit from protective procedures that will enable them to defend themselves.
Digital platforms have appropriated the bulk of the advertising revenue, which used to be allocated to the funding of traditional media.
These platforms have also capitalized on these media’s primary content without remunerating them.
It is important to think about new methods of redistribution of information from digital platforms to quality media.
Require platforms to contribute to the funding of quality journalism, by requiring them to provide funding for fact-checking, for example.
It is, therefore, necessary to devise new forums in which platforms’ intellectual property rights would be guaranteed, in exchange for easier access to their data, software and algorithms.
This entails, particularly in the wake of the Cambridge Analytica scandal, the establishment of a preliminary framework for ethical research based on the model by which doctors access their patients’ medical files.
Explore redirection methods so as to ensure that those who seek fake news also come across debunking.
The idea is to apply such methods to other cases of information manipulation.
Responding to objections In many countries, responses to information manipulation raise concerns—sometimes sincere and other times feigned and calculated.
In all cases, however, these responses are legitimate objects of democratic debate.
In the following pages, we list the principal criticisms and provide some answers to these objections.
The role of disinformation in recent crises (Brexit, American electionsj has been overstated.
There is no conclusive research demonstrating that fake news has a direct and tangible impact on internet users.
Conversely, by responding to disinformation in a conspicuous manner, we risk granting the stories undue importance. —* Recent experience has demonstrated, on the contrary, that it is important to not underestimate the seriousness of information manipulation.
The Lisa Case has had very real consequences on the rise of anti-migrant sentiment in Germany and such effects are often irreversible, despite later efforts to restore the truth.
The Obama Administration chose, for a variety of reasons, not to alert the public to the information manipulation campaign targeting the country, thereby easing the course of an ongoing democratic destabilization effort.
On the other hand, the German Chancellor referred publicly to the manipulation threats in the wake of the 2015 attack on the Bundestag.
Digitalplatforms are the ideal scapegoats to blame for the evils of society.
This very neutrality requires strong principles and clear rules to prevent it from being diverted towards malicious goals or from serving projects that are hostile to our democracies and our citizens’ welfare.
It is high time that platforms take responsibility and that governments draw all the lessons from this type of scandals.
The proposed solutions will only impact those who are already convinced and will have no impact on those audiences who are most exposed to disinformation . —> Contemporary information manipulation campaigns succeed in sowing seeds of doubt in a variety of audiences—not just conspiracy theorists, alternative and radical communities.
Measures that support media literacy, fact-checking and quality journalism reinforce the resilience and immunity of the wider public to manipulation threats.
We are conscious of the fact that the most radical or pro-conspiracy theory segments of public opinion will not be convinced, but they are a minority and must remain so.
Initiatives led by non-governmental and independent organizations such as RSF, that seek to create consensus 50 RECOMMENDATIONS within the profession on objective criteria for quality journalism (working methods, cross-checking information, error correction procedures, media governance, etc.) are very valuable in this context.
In order to avoid counterproductive effects, ranking and labeling schemes must offer guarantees of the transparency of the process, the quality of the criteria, and demonstrate the inclusivity and diversity of those assessing these criteria.
The French President, in his New Year’s address to the press, on 4 January 2018, referenced the issue of conflicts of interest between shareholders and editorial boards and suggested some possible courses of action by which to guarantee the full editorial independence of the media.
Among these censored sites was MadaMisr, an independent, progressive newspaper who had voiced opposition towards the current regime.
The cure is therefore worse than the disease. —> In France, the parliamentary bill against information manipulation currently under consideration offers many guarantees.
The bill also relies on a reinforcement of the powers of the ordinary judge, the guardian of liberty, and the powers of the CSA, the independent public authority responsible for ensuring freedom of audiovisual expression.
The fundamental goal of this legislative proposal is simply to protect the honesty and integrity of the ballot, so that it faithfully reflects the popular will.
The fake news anathema has become a convenient tool with which dictators and illiberal regimes justify censorship. —> This is a real risk and one that we take very seriously.
We made a conscious decision to respond to information manipulation in a transparent and democratic manner, by cooperating with civil society and the media.
Grounded as it is in the rule of law and in the values of open societies, our response is by nature more difficult to flip around in an authoritarian setting.
In tackling information manipulation, we turn either to the ordinary judge, who is the guardian of liberty, or to the CSA, an independent regulation authority whose mission is to protect freedom of audiovisual expression.
France will remain vigilant, at every stage of the response, to ensure that the potential risks to civil liberties in an illiberal/ authoritarian context are duly taken into account.
The various initiatives mentioned in this report aim at fostering quality content, not at censoring biased or false content.
Furthermore, they do not resort to the methods frequently used by RT and Sputnik, such as the fabrication of facts and the falsification of documents, translations and interviews, the use of edited photos, or fake experts.
It is these instances of information manipulation, and these alone, that we denounce; not the fact that these outlets have a particular point of view.
The scapegoat argument:you blame Moscow for all of the Western world’s evils. —* Those actors who are behind information manipulation campaigns—and who are oftentimes easily identifiable—are not the source of our societies’ evils, but they do amplify them.
They deliberately identify the fault-lines intrinsic to each society (religious and linguistic minorities, historical issues, inequality, separatist tendencies, racial tensions, etc.) and then seek to further polarize public opinion around these divisive issues. —> The fight against information manipulation must also take into account other actors, potential or known, who are likely to undertake information manipulation campaigns.
Your response proves that you take your citizens forfools who are unable to “think correctly.
Nevertheless, our duty is to protect our democratic institutions and our national interests from hostile information manipulation as well as to foster the development of programs by civil society and public institutions, enabling citizens and young people, in particular, to fully exercise their critical thinking in the field of information.
Western nations, and Trance in particular, did not hesitate to resort to state propaganda in the colonial context.
This is the INFORMATION MANIPULATION remit of historians who study and shall continue to study all the chapters of our national history.
Today, we are faced with a new, specific challenge, which we must tackle not only by drawing upon the lessons of the past, but also by looking towards the future.
ADEMSKY Dima, Cross-Domain Coercion, Institut fran^ais des relations internationales, November 2015.
Propaganda and Trolling as Warfare Tools,” European Vieiv, 10 May 2016.
Resources for Searching and Analysing Online Information, CreateSpace Independent Publishing Platform, 2016.
BENASAYAG Miguel and AUBENAS Florence, La Fabrication de I'information, La Decouverte, 2007.
Security Implications of False Information Online, NATO Strategic Communications Centre of Excellence, November 2017.
Ideas, Domestic Politics and External Relations, Palgrave Mcmillan, 2015.
CENTRE FOR INTERNATIONAL RELATIONS, Information Warfare in the Internet.
CHEKINOV Sergei and BOGDANOV Sergei, “Asymmetrical Actions to Maintain Russia’s Military Security,” Military Thought, Vol., 2010.
CHOMSKY Noam and HERMAN Edward, Manufacturing Consent, The Political Economy of the Mass Media , Random House, 2002.
COMMUNICATIONS SECURITY ESTABLISHMENT, Cyber Threats to Canada’s Democratic Processes, Government of Canada, 2017.
CONNELL Mary Ellen and EVANS Ryan, “Russia’s Ambiguous Warfare and Implications for the United States Marine Corps,” MCU Journal, Vol., 2016.
CRAWFORD Krysten, “Stanford study examines fake news and the 2016 presidential election,” Stanford Neu’s, 2017.
DAMARAD Volha and YELISEYEU Andrei, Disinformation Resilience in Central and Eastern Europe, Disinformation Resilience Index , 2018.
DIEGUEZ Sebastian, Total Bullshit l An caur de lapost-verite, PUF, 2018.
ELKJER NISSEN Thomas, Social Media's Role in “Hybrid Strategies’,’ Riga NATO Strategic Communications Centre of Excellence, 2016.
EL-OIFI Mohammed, “Disinformation a 1’israelienne,” Le Monde diplomatique, 2005.
EUROPEAN COMMISSION, A Multi-Dimensional Approach to Disinformation, Report of the Independent High Level Group on Fake Neu’s and Online Disinformation, March 2018.
How to tailor national strategy using lessons learned from countering Kremlin’s hostile subversive operations in Central and Eastern Europe, Kremlin Watch Report, 30 April 2018.
FARWELL James R, “Countering Russian Meddling in United States Political Processes,” Parameters, 48:1, Spring 2018.
A Catalog of Modi Operandi, FOI Totalforsvarets forskningsinstitut, March 2013.
FRIED Daniel and POLYAKOVA Alina, Democratic Defense Against Disinformation, Atlantic Council, Eurasia Center, 2018.
GAGLIANO Giuseppe, Disinformation, desobeissance civile etguerre cognitive, VA Press, 2016.
Bikers and the Kremlin ,” The Moscow Times, 19 May 2015. —, “The Gerasimov Doctrine and Russian Non Linear War,” Blog in Moscow’s Shadows, July 2014.
GARRIGOU Alain, “Ce que nous apprennent les ‘fake news’,” Te Monde diplomatique, February 2018.
GASTINEAU Pierre and VASSET Philippe, Amies de destabilisation massive.
GILES Keir, Handbook of Raissian Information Warfare, Fellowship Monograph 9, NATO Defense College Research Division, November 2016. —, The Next Phase of Russian Information Warfare, NATO Strategic Communications Centre of Excellence, 2016.
Ecole de guerre economique, 2015. — and LUCAS Didier, La Guerre cognitive, Lavauzelle, 2002.
Understanding Russian Propaganda in Eastern Europe, RAND Corporation, 2018.
IRELAND , First Report of the Interdepartmental Group on Security of Ireland's Electoral Process and Disinformation, prepared by the Department of the Taoiseach, June 2018.
JANDA Jakub, “Why the West is Failing to Counter Kremlin Disinformation Campaigns,” The Observer, 30 December 2016.
JULIEN Claude , “L’art de la desinformation,” Ee Monde diplomatique, includes 13 articles, May 1987.
Kremlin’s traditional agenda and the export of values to Central Europe, Political Capital Institute, 4 August 2016.
LANGE-IONATAMISVILI Elina , Rjtssia’s Footprint in the Nordic-Baltic Information Environment, NATO Strategic Communications Centre of Excellence, 2016-2017.
Techniques and Counter-strategies to Rjtssian Propaganda in Central and Eastern Europe, CEPA’s Information Warfare Project/Legatum Institute, August 2016.
MACRON Emmanuel, President of French Republic, Discours dupresident de la RJpublique Emmanuel Macron a 1’occasion des vceux a la presse, 4 January 2018.
Caught between a rock and a hard place, ” European Council on Foreign Relations, March 2018.
MCGEEHAN Timothy P., “Countering Russian Disinformation,” Parameters, 48:1, Spring 2018.
MEGRET Maurice, Ea Guerrepsychologique, PUF, “Que sais-je,” 1963. —, E’Actionpsychologique, Artheme Fayard, 1953.
The Tolly of Technological Solutionism, PublicAffairs, 2014. —, “Les vrais responsables des fausses nouvelles He Monde diplomatique, 2017.
Alt-right attacks Macron in last ditch effort to sway French election,’ ’ Atlantic Council’s Digital Torensic Research Tab, 6 May 2017.
NOCETTI Julien, “Comment l’information recompose les relations internationales,” in MONTBRIAL Thierry de and DAVID Dominique , Ramses 2018.
NYSSEN Fran^oise, Minister of Culture, Discours prononce a 1’occasion des Assises internationales dujournalisme, Tours, 15 March 2018.
O’CARROLL Eoin, “How information overload helps spread fake news,” The Christian Science Monitor, 27 June 2017.
OH Sarah and ADKINS Travis L., Disinformation Toolkit, InterAction, June 2018.
How Big Data Increases Inequality and Threatens Demo crazy, Crown, 2016.
A Review of Relevant Psychological Research, Center for Asymmetric Threat Studies , Swedish National Defence College,d.
PAUL Christopher and MATTHEWS Miriam, The Russian ‘Firehose of Falsehood’ Propaganda Model—Why It Might Work and Options to Counter It, Expert insights on a timely policy issue, RAND Corporation, 2016.
Russia, The West, and the Coming Age of Global Digital Competition, Brookings, March 2018.
How the Kremlin Weaponi^es Information, Culture and Money, The Interpreter, a project of the Institute of Modern Russia, 2014.
RILEY Michael, ETTER Lauren and PRADHAN Bibhudatta, A Global Guide to State-Sponsored Trolling, Bloomberg, 19 July 2018.
ROSENFELD Louis, MORVILLE Peter and ARANGO Jorge, Information Architecture, O'Reilly, 2015.
SENECAT Adrien, “Le Decodex, un premier pas vers la verification de masse de l’information,” He Monde, 2017.
SZWED Robert, Framing of the Ukraine-Russia Conflict in Online and Social Media, NATO Strategic Communications Centre of Excellence, May 2016.
BIBLIOGRAPHY TATHAM Steve, The Solution to Russian Propaganda Is Not EU or NATO Propaganda but Advanced Social Science to Understand and Mitigate Its Effects in Targeted Populations, Policy paper, National Defence Academy of Latvia, Center for Security and Strategic Research, July 2015.
TENENBAUM Elie, Ee Pi'ege de la guerre hybride , Focus strategique 63, IFRI, October 2015.
THE INTEGRITY INITIATIVE, Framing Rjissian meddling in the Catalan question, October 2017.
THE HAGUE CENTRE FOR STRATEGIC STUDIES, Inside the Kremlin House of Mirrors.
Horn Liberal Democracies can Counter Ratssian Disinformation and Societal Interference, 2017.
TUFEKCI Zeynep, ‘YouTube, the Great Radicalizer, ” TheNeiv York Times, 10 March 2018.
TWOREK Heidi, “Responsible Reporting in an Age of Irresponsible Information,” Alliance for Securing Democracy Brief 2018, 009, March 2018.
US DEPARTMENT OF JUSTICE, Report of the Attorney General's Cyber Digital Task Force, July 2018.
VAISSIE Cecile, Les RJseaux du Kremlin en France, Les Petits Matins, 2016.
Finfluence des reseaux sociaux durant une election presidentielle,” in TAILLAT Stephane, CATTARUZZA Amael and DANET Didier , La Cyberdefense.
How to Tailor National Strategy Using Lessons Learned from Countering Kremlin’s Hostile Subversive Operations in Central and Eastern Europe, European Values, Kremlin Watch Report, 30 April 2018.
VOLKOFF Vladimir, Desinformation, flagrant delit, Ed. du Rocher, 1999. —, Petite Histoire de la desinformation, Du chevalde Troie a Internet, Ed. du Rocher, 1999. —, La Desinformation vue de I’Est, Ed. du Rocher, 2007.
VOSOUGHI Soroush, ROY Deb and ARAI Sinan, “The spread of true and false news online,” Science, 359:6380, 9 March 2018, 1146-1151.
WALTZMAN Rand, The Weaponigation of Information—The Need for Cognitive Security, RAND Corporation, 2017.
Toimrd an interdisciplinary framework for research andpolicy making, European Council, 2017.
Confusion, desinformation, communication, Ed. du Seuil, “Points,” 1978.
Mensonge on verite, comment reperer les “fake newi ’ ?, Xenius, Arte, 2017.
Runet, la bataille de I'lnternet russe, Arte, web serie, 10 episodes, 2018.
Marine Guillaume is a Policy officer on “Cybersecurity and Digital Affairs” at the Policy Planning Staff of the Ministry for Europe and Foreign Affairs and a lecturer at the Ecole Polytechnique.
A graduate from Sciences Po and alumnus from the Ecole Nationale d’Administration , she founded an independent think tank in Beirut dedicated to research on the Arab Spring , after having notably worked as the First Secretary at the French embassy in Lebanon, in charge of Regional and Multilateral issues , and as a desk officer for the Department for the EU Common Foreign Security Policy and for the Departement of the United Nations and International Organizations on environmental issues.
Acknowledgments We would like to warmly thank all of our interlocutors, in France and abroad, as well as our colleagues who were eager to help us in the preparation of this report, through specific contributions or careful proofreading (Jean-Pierre Bat, Emmanuel Bloch, Lucie Delzant, Emmanuel Dreyfus, Jean Dubose, Emilien Legendre, Kevin Limonier, Benjamin Pajot, Maud Quessard, Marie Robin, Boris Toucas), as well as Elodie Ternaux for the cover, Chantal Dukers for the mockup, and Mickaela Churchill, Aziliz Gouez and Diana Reisman for the English translation.
This phenomenon has manifested itself in recent years through various electoral interferences which threaten national security.
The Policy Planning Staff and the Institute for Strategic Research have thus joined forces to study this issue.
This report is the product of field research in order to develop a better understanding of the nature of the problem and identify good practices put in place by States and civil society.
Our research is equally based on the abundant scientific literature on the subject.
Precluding the vague and controversial notion of fake news, among others which are often either too narrow or too broad to apply to this specific issue, this report uses the term “information manipulation” to describe the intentional mass dissemination of false or biased news for hostile political ends.
We then proceed to identify the beneficiaries of these activities,e. the actors carrying out information manipulation.
We focus specifically on States that manipulate information outside their territory or, in other words, who interfere in the domestic affairs of other States.
This report then proceeds to examine the consequences of information manipulation, by exploring the distinctive features of recent information manipulation campaigns and identifying some common characteristics—both in terms of vulnerability factors and the methods employed.
We begin by looking at the interference attempts in the latest French presidential election and the lessons learned from this event.
To conclude, this report attempts to anticipate future challenges—technological challenges, future trends in Russian “information warfare” and possible future scenarios.
We propose 50 recommendations, operating on the assumption that information manipulation will remain a problem in the future and that it will constitute a long-term challenge for our democracies.
The Policy Planning Staff is the Center for analysis, prevision and strategy of the Ministry for Europe and Foreign Affairs.
The IRSEM is the Institute for Strategic Research of the Ministry for the Armed Forces.

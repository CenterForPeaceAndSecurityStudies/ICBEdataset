---
title: "Synthetic Narrative Cuban Missiles"
output: html_document
date: '2022-04-26'
---

#Create a Cuban Missile Corpus

```{python, eval=F}
fromscratch=False
if fromscratch:
  from internetarchive import download
  #help(download)
  destdir="/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/internetarchive/"
  from internetarchive import search_items
  search_results=search_items(query="Cuban Missile Crisis Khrushchev Kennedy Cuba nuclear 1962 Castro October", full_text_search=True)
  search_results=[q for q in search_results]
  import random
  random.shuffle(search_results)
  len(search_results)
  #for i in search_items('Cuban Missile Crisis'):
  #    print(i['identifier'])
  #download(identifier=search_results[0]['fields']['identifier'][0] ,destdir=destdir , verbose=True, ignore_existing=True, glob_pattern="**.txt") #search_results[0]['fields']['identifier'][0]
  for q in search_results:
    try:
      download(identifier=q['fields']['identifier'][0] ,destdir=destdir , verbose=True, ignore_existing=True, glob_pattern="*.txt") #if you don't remove pdfs, it'll crash later because there's a 1.7gb pdf in there lol
    except:
      pass

```

```{r}

library(reticulate)

#pip install -U pip setuptools wheel
#pip install -U spacy
#python -m spacy download en_core_web_sm
library(tidyverse)
library(rvest)

screen_rex <- function(x){
  return( x %>%
                    filter(text %>% str_detect("Cuban")) %>%
                    filter(text %>% str_detect("Missile")) %>%
                    filter(text %>% str_detect("Crisis")) %>%
                    filter(text %>% str_detect("Kennedy")) %>%
                    filter(text %>% str_detect("nuclear")) %>%
                    filter(text %>% str_detect("Khrushchev")) %>%
                    filter(text %>% str_detect("1962")) 
  )
}
#delete duplicate files from the directory just to be sure
#rdfind -deleteduplicates true  

library(googlesheets4) #Do this high up so you get the prompt early
cuban_missile_doc <- read_sheet("https://docs.google.com/spreadsheets/d/1lnFqrnCwjWWd_rdWLVHGkxSEcFpw-5XrV7MiQ6rA7KY/edit?usp=sharing", 
                                sheet="CubanMissilesNarratives") %>% janitor::clean_names()
dim(cuban_missile_doc)



#library(spacyr)
#spacy_install()
#spacy_download_langmodel("en")
#spacy_initialize()
#cuban_missile_clean_sentences <- spacy_parse(cuban_missile_clean$text, full_parse=F)
#dim(cuban_missile_clean_sentences)

#install.packages("pdftools")
#sudo add-apt-repository -y ppa:cran/poppler
#sudo apt-get update
#sudo apt-get install -y libpoppler-cpp-dev

#install.packages('striprtf')
#Surprisingly slow, faster to open and save as plain text and load from there
#cuban_missile_news <- striprtf::read_rtf("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/Factiva-25April20221633.rtf")

files <- list.files("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/loose_txt/", full.names = T, recursive = T,pattern="*.txt")
length(files)
loose_list_text <- list()
for(file in files){
  try({
    df <- data.frame(text=file %>% readLines() )  %>% 
          mutate(url=file) 
    loose_list_text[[file]] <- df
  })
}
cuban_missile_loose_list_text <- bind_rows(loose_list_text) %>% 
  group_by(url) %>%
  summarise(text=paste(text, collapse="\n"))  %>% screen_rex()
dim(cuban_missile_loose_list_text) #958
cuban_missile_loose_list_text$short <- NA

files <- list.files("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/loose_html/", full.names = T, recursive = T,pattern="*.htm*")
length(files)
loose_list <- list()
for(file in files){
  try({
    df <- data.frame(text=file %>% read_html %>% html_text )  %>% 
          mutate(url=file) %>% 
          screen_rex()
    loose_list[[file]] <- df
  })
}
cuban_missile_loose <- bind_rows(loose_list) %>% 
  group_by(url) %>%
  summarise(text=paste(text, collapse="\n"))  %>% screen_rex()
dim(cuban_missile_loose) #958
cuban_missile_loose$short <- NA

#IA
#install.packages('readtext')
#https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html#introduction
library(readtext)
#install.packages('textreadr')
text <- textreadr::read_dir("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/internetarchive/", recursive=T, pattern="txt", verbose=T)
cuban_missile_ia <- text %>% dplyr::select(url=document, text=content) %>% group_by(url) %>% summarise(text=text %>% paste(collapse=" \n ")) %>%  screen_rex()
cuban_missile_ia$short <- NA

#C4
files <- list.files("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/c4/", full.names = T)
c4_list <- list()
for(file in files){
  df <- data.frame(text=file %>% read_html %>% html_text )  %>% 
        mutate(text = strsplit(as.character(text), "\n")) %>% 
        unnest(text) %>%
        mutate(text=text %>% trimws()) %>%
        filter(text!='') %>%
        mutate(url=ifelse(text %>% str_detect('http'), text, NA )) %>%
        fill(url) %>%
        filter(text!=url) %>% filter(!is.na(url)) %>% screen_rex()
  c4_list[[file]] <- df
}
cuban_missile_c4 <- bind_rows(c4_list) %>% 
  group_by(url) %>%
  summarise(text=paste(text, collapse="\n"))  %>% screen_rex()
dim(cuban_missile_c4) #958
cuban_missile_c4$short <- NA

#Ok this is trash, it only pulls 300 ish unique URLS and then dupes
#Bing search
fromscratch=FALSE
if(fromscratch){
  library(httr)
  server="https://api.bing.microsoft.com/v7.0/search"
  token = "cb169dcd50584785a82dd8852f283cf2"
  search_term = "'Cuban Missile Crisis' +Khrushchev +Cuba +Kennedy +Castro +nuclear' +1962 -Ukraine"
  url = paste0(server) #, "search"
  mkt = 'en-US'
  iter = seq(0, 10000, 50)
  result_list <- list()
  for(i in iter){ #[0:3]
    print(i)
    response = GET(url = url, 
                   query = list(q = search_term, mkt=mkt, count=50, offset=i), 
                   add_headers(`Ocp-Apim-Subscription-Key` = token)
                   )
    result_list[[as.character(i)]] <- response
    #Sys.sleep(1)
    #res = content(response, encoding = "json")
    result_list_parsed <- result_list %>% lapply(content, encoding = "json")
    #res
    result_list_df <- result_list_parsed  %>% lapply(FUN=function(x) { x$webPages$value %>% lapply(as.data.frame)  } ) %>% bind_rows() 
    #dim(result_list_df)
    #Parse the whole thing each loop both as a sleep and to make sure it's adding unique urls
    result_list_df$url %>% duplicated() %>% table() #11 dupes #failed horribly and most were dupes only 335 unique URLS

  }
  saveRDS(result_list_df, "/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/bind_searches.Rds")
  result_list_df <- result_list_df %>% filter(!duplicated(url))
  library(rvest)
  #result_list_df
  #temp=result_list_df$url[1] %>% read_html() %>% html_text()
  result_list_df$text <- NA
  for(i in 1:nrow(result_list_df)){
    print(i) #273 hung
    try({result_list_df$text[i] <- result_list_df$url[i] %>% read_html %>% html_text}) #not fast
  }
  saveRDS(result_list_df, "/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/bind_searches_df.Rds")
}

result_list_df <- readRDS( "/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/bind_searches_df.Rds")  %>% screen_rex()


icb_wide_clean <- readRDS(file=paste0(here::here(), "/replication_data/temp/ICBe_V1_wide_clean.Rds"))

cuban_missile_icb <- icb_wide_clean %>% filter(crisno==196) %>% dplyr::select(sentence_number_int_aligned, text=sentence) %>% distinct() %>% mutate(url=paste("ICB_196_",sentence_number_int_aligned)) %>% select(-sentence_number_int_aligned)
cuban_missile_icb$short <- NA

cuban_missile_news <- read_lines("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/Factiva-25April20221633.txt")
length(cuban_missile_news)

"Document SLMO000020040704dk9400o8v" %>% str_detect("^Document ")
cuban_missile_news_df <- data.frame(text=cuban_missile_news) %>% mutate(url=ifelse(text %>% str_detect("^Document "), text, NA )) %>% fill(url, .direction="up") %>% filter(!is.na(text) & text!='')
cuban_missile_news_df$short=NA

files <- list.files(path = "/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban/",
                    pattern = "*.pdf", all.files = FALSE,
           full.names = T, recursive = T,
           ignore.case = FALSE, include.dirs = FALSE, no.. = FALSE)
       
library(pdftools)
pdf_text_list <- list()
for(file in files){
  #print(file)
  try({ pdf_text_list[[file]] <- pdftools::pdf_text(file) %>% unlist() %>% paste(collapse="\n ")   } )
}
cuban_missile_pdfs <- data.frame(url=names(pdf_text_list)) 
cuban_missile_pdfs$text <- as.vector(unlist(pdf_text_list))
cuban_missile_pdfs$short <- NA

#cuban_missile_pdfs %>% filter(url %>% str_detect("NewsBank Multiple Articles.pdf")) %>% 
#    mutate(text = strsplit(as.character(text), "OpenURL Link")) %>% 
#    unnest(text) %>% group_by(url) %>% mutate(url=url %>% paste0(row_number()))
cuban_missile_pdfs <- cuban_missile_pdfs %>% 
    mutate(text = strsplit(as.character(text), "OpenURL Link")) %>% 
    unnest(text) %>% group_by(url) %>% mutate(url=url %>% paste0(row_number()))  %>% screen_rex()



"/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/cuban//zotero/7LU24CRX/filename.pdf" %>% str_replace(".*/(.*?.pdf)","\\1")


cuban_missile <- bind_rows(
  cuban_missile_pdfs %>%
  mutate(url=url %>% str_replace(".*/(.*?.pdf)","\\1")) %>% 
           filter(!duplicated(url %>% tolower())) %>% filter(!duplicated(text %>% tolower() %>% trimws())),
  cuban_missile_doc,
  cuban_missile_news_df,
  cuban_missile_icb,
  result_list_df %>% dplyr::select(text, url) %>% mutate(short=NA),
  cuban_missile_c4,
  cuban_missile_ia,
  cuban_missile_loose,
  cuban_missile_loose_list_text
)   %>% screen_rex() 
dim(cuban_missile) #2067 #1548 #16819 #9093

"Second, the case is relatively well documented:in additionto the famous ExComm tapes (May and Zelikow 1997), declassification of the written recordhas been extensive." %>% str_replace_all("\\(.*?\\)","")

"asdfasdf (sdfasdf) asdadsasd" %>% str_replace_all("\\(.*?\\)","")
"asdfasdf [1-10] asdadsasd" %>% str_replace_all("\\[.*?\\]","")
"John F. Kennedy" %>% str_replace_all("( [A-Z])\\.","\\1")
"Cuban armed assistance to subversion in other parts of the Western Hemisphere. f." %>% str_replace_all(" [a-z]\\.","")
"Cuban armed assistance to subversion in other parts of the Western Hemisphere.1" %>% str_replace_all("([a-z]\\.) *[0-9]{1,2}","\\1")
"U.S. president Dwight D. Eisenhower (1890–1969; served 1953–61) halted U.S. importation of Cuban sugar" %>%
                                    str_replace_all("( [A-Z])\\. ","\\1 ")

"U.S. president Dwight D. Eisenhower (1890–1969; served 1953–61) halted U.S. importation of Cuban sugar" %>%
                                    str_replace_all("U\\.S\\. | US "," United States ") 

"sasadfasdffds.1 asdfasdf" %>% str_replace_all("\\.[0-9] ","\\. ")
library(cld3); #install.packages('cld3')
cuban_missile_clean <- cuban_missile %>% ungroup() %>%
                       janitor::clean_names() %>%
                       mutate(text= text %>% 
                                    str_replace_all("\\(.*?\\)","") %>% #parentheticals
                                    str_replace_all("\\[.*?\\]","") %>% #brackets citations
                                    str_replace_all("U\\.S\\. | US "," United States ") %>%
                                    str_replace_all("U\\.N\\. "," United Nations ") %>%
                                    str_replace_all("( [A-Z])\\. ","\\1 ") %>% #middle names
                                    str_replace_all(" [a-z]\\.","") %>% 
                                    str_replace_all("([a-z]\\.) *[0-9]{1,2}","\\1")  %>% 
                                    str_replace_all("Mr\\.","Mr ")  %>% 
                                    str_replace_all(" {3,}","\n")  %>%  
                                    str_replace_all("\\.[0-9] ","\\. ")  %>% 
                                    str_replace_all("  "," ") 
                        ) %>%
                        mutate(langauge=cld3::detect_language(text)) %>% 
                        filter(langauge=='en') %>%
                        mutate(text_nchar= text %>% nchar() ) %>%
                        ungroup() %>% filter(!duplicated(text_nchar)) %>% #there should never be an exact character count
                        filter(text_nchar>1000) 
table(cuban_missile_clean$langauge) %>% sort()
hist(cuban_missile_clean$text_nchar %>% log(), breaks=50)
table(cuban_missile_clean$text_nchar) %>% sort()
dim(cuban_missile_clean) #1,636

#https://github.com/dselivanov/LSHR
library(devtools)
#install_github("dselivanov/LSHR")
library(LSHR) #install.packages('LSHR')
#devtools::install_github('dselivanov/text2vec')
library(text2vec)
it <- itoken(cuban_missile_clean$text, preprocess_function = tolower, tokenizer = word_tokenizer)
dtm <- create_dtm(it, hash_vectorizer())
dtm = as(dtm, "RsparseMatrix")
hashfun_number = 120
s_curve <- get_s_curve(hashfun_number, n_bands_min = 5, n_rows_per_band_min = 5)
s_curve %>% ggplot(aes(x=similarity, y=probability_become_candidate, color=paste(n_bands,n_rows_per_band))) + geom_line()
seed = 1
pairs = get_similar_pairs(dtm, bands_number = 6, rows_per_band = 20, distance = 'cosine', seed = seed)
pairs[order(-N)]
temp <- pairs %>% filter(N==5) %>%
  mutate(id1_new =  ifelse(id1<id2, id1, id2) ) %>%
  mutate(id2_new =  ifelse(id1<id2, id2, id1) )
dim(temp) #now only 18 dupes
temp$a <- substr( cuban_missile_clean$text[temp$id1_new],  1, 200)
temp$b <- substr( cuban_missile_clean$text[temp$id2_new],  1, 200)



"https://doi.org/10.2307/2009577"
"Document: President Kennedy's statement on Soviet military" %>% str_replace_all("^.*?: ","")
"The discussion of the crisis by ExComm members shows awareness of and sensitivity to domestic political considerationsin the selection of the initial United States response to the Soviet missiles." %>% str_detect("[a-z0-9]\\.$")
'It can use them and it must not be treated lightly"; Khrushchev, cited in Abel, 191-92.' %>% str_detect(" [0-9]{3}(\\–|\\-)")
"On the Turkish issue, he faults Kennedy's refusal of the quid pro quo, since Kennedy had known for some time that our missiles in Turkey were obsolete; Stone, 21-22.
" %>% str_detect(" [0-9]{2,3}(\\–|\\-)[0-9]{2,3}")
"On April 12, 1961, the Soviets again led the way with the launch of Yuri Gagarin, a Russian cosmo¬ naut, into space to become the first human to leave Earth." %>% str_replace_all("- |¬ ","")
#install.packages('tidytext')
library(tidytext)
index <- setdiff(1:nrow(cuban_missile_clean), temp$id2_new %>% unique())
cuban_missile_clean_sentences <- cuban_missile_clean[index,] %>% 
                                 ungroup() %>%
                                 mutate(text= text %>% str_replace_all("\n","  ") ) %>% #because pdfs are often parsed line by line
                                 mutate(text= text %>% str_replace_all(" {1,}"," ") ) %>%
                                 unnest_tokens(sentence, text, token="sentences", drop=T, to_lower=F) %>%
                                 mutate(sentence= sentence %>% str_replace_all("^.*?: ","") ) %>%
                                 mutate(sentence= sentence %>% str_replace_all("- |¬ ","") ) %>% #words broken across pages
                                 mutate(sentence= sentence %>% str_replace_all(" {1,}"," ") %>% trimws() ) %>%
                                 mutate(sentence_nchar=sentence %>% nchar()) %>%
                                 mutate(sentence= sentence %>% trimws()) %>%
                                 filter(sentence_nchar>70 & sentence_nchar<600) %>%
                                 filter(!str_detect(sentence, "downloaded|Downloaded|Table|Figure|University|pp\\.|New York| file | [0-9]{3} | [0-9]{3}(\\–|\\-)|Box |IEEE|Memorandum|Transcript|Telegram|Kew Gardens| [0-9]{2,3}(\\–|\\-)[0-9]{2,3}|CHAPTER| cited|=|Cuban Missile Crisis|Book|Copyright|FRUS|http|JSTOR|researchers|archival|scholarship|recordings|File|Foreign Relations of the United States|Printing Office|Document|Minutes of|Summary|Record|vol\\.|No\\.|Issue|Paper|à| et | est |United States Government|American Foreign Policy|FO |PREM| box|Part| et | est |Acknowledgements|website|cookies|Privacy|©")) %>%
                                 mutate(sentence= sentence %>% str_replace_all(" {1,}"," ") %>% trimws() ) %>%
                                 filter(sentence %>% str_detect("^[A-Z]")) %>% #has at least one capital letter
                                 filter(sentence %>% str_detect("[a-z0-9]\\.$")) %>% #ends in a letter/number and a period no questions
                                 #add_count(sentence) %>%
                                 ungroup() %>%
                                 mutate(dupe= sentence %>% tolower() %>% str_replace_all("[^a-z]","") %>% duplicated() ) %>%
                                 filter(!dupe) %>% #restrict to unique sentences
                                 group_by(url) %>%  
                                   mutate(sentence_n = n() ) %>% 
                                   mutate(sentence_number= row_number() ) %>%
                                   mutate(position= row_number()/n()) %>%
                                 ungroup() %>%
                                 filter(sentence_n<=20000 ) %>%
                                 filter(sentence_n>=10 | url %>% str_detect("ICB")) #require at least 10 unique sentences per source

cuban_missile_clean_sentences$sentence %>% tolower() %>% duplicated() %>% table()
dim(cuban_missile_clean_sentences) #600206 #124,808 #125018 #72576 #74385 #79851 #80529 #73,817

cuban_missile_clean_sentences %>% filter(url %>% str_detect("ICB"))

#Less than 600 and greater than 50
hist(cuban_missile_clean_sentences$sentence_nchar[cuban_missile_clean_sentences$sentence_nchar], 60)
summary(cuban_missile_clean_sentences$sentence_nchar) #Interquartile is 111 to 200

cuban_missile_clean_sentences  %>% write_tsv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences.tsv")

```

```{r}

library(tidyverse)
cuban_missile_clean_sentences <- read_tsv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences.tsv")

cuban_missile_clean_sentences$url %>% unique() %>% length() #1,218 #993 #1568 #881 #999 #464 #558 unique sources #646 sources so far
cuban_missile_clean_sentences$url %>% table() %>% as.vector() %>% summary() #median of 37 sentences per

cuban_missile_clean_sentences_unique <- cuban_missile_clean_sentences$sentence %>% unique()
length(cuban_missile_clean_sentences_unique) #600,148 #179,217 #1002249 #125018 #82,280 #78,225 #81621 #73817 #There are 73,991 unique sentences

#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
#    71     102     136     154     185     599     338 
summary(cuban_missile_clean_sentences$sentence_nchar %>% as.numeric())
nchar("The Soviet commander in Cuba was authorised to use tactical nuclear weapons against United States forces should they invade, and as Soviet missiles were already installed, any attack on Cuba could have led to the outbreak of nuclear war.")
nchar("Soviet commander in Cuba authorised to use tactical nuclear weapons against U.S. forces if they invaded.") #104
nchar("Any attack on Cuba could have led to the outbreak of nuclear war.")

#Write out a folder of sentences to iterate over and summaries

#embarassingly slow and parallelizable
library(digest)
for(q in cuban_missile_clean_sentences$url %>% unique() ){
  cuban_missile_clean_sentences %>% filter(url==q) %>% pull(sentence) %>% #forgot to make it just long sentences, actually worked out I think
    writeLines(glue::glue("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/sentences_original/{digest(q,'crc32')}.txt"))
}

```

Create additional simplified sentences

This takes the 3090 Ti more than all night to run alone

```{python}

import torch
use_cuda = torch.cuda.is_available()
if use_cuda:
    print('__CUDNN VERSION:', torch.backends.cudnn.version())
    print('__Number CUDA Devices:', torch.cuda.device_count())
    print('__CUDA Device Name:',torch.cuda.get_device_name(0))
    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)
    print('__CUDA Device Name:',torch.cuda.get_device_name(1))
    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(1).total_memory/1e9)    

import os
from tqdm.auto import tqdm
import pandas as pd
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
summarizer_distilbart = pipeline("summarization", device=0, model="sshleifer/distilbart-cnn-12-6") #1 for small gpu 0 for big
from datasets import load_dataset
#What if we did it by url? Then we don't need to keep track of which sentences worked and didn't and if it fails we just lose one doc
directory="/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/sentences_original/"
out_directory="/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/sentences_sum/"
from os.path import exists
from datasets import Dataset
for filename in os.listdir(directory):
    f_in = os.path.join(directory, filename)
    f_out=os.path.join(out_directory, filename)
    if not exists(f_out): #I'm eventually going to have to write a try for the one that fails on us
        my_file = open(f_in, "r")
        df = pd.DataFrame({"text": my_file.readlines() })
        dataset = Dataset.from_pandas(df ) #.head(1)
        results=[out for out in summarizer_distilbart(KeyDataset(dataset, "text"), num_beams=4, min_length=10, max_length=60, length_penalty=100.0, batch_size=32)] #32 was fastest
        #dataset = load_dataset('text', data_files={'train': [f_in]}, split="train") #same speed either way
        with open(f_out, 'w') as f:
          _ = f.write('\n'.join([q[0]['summary_text'] for q in results]))



```

Load the simplified sentences and add them to the pile

```{r}

files <- list.files("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/sentences_sum/", full.names = T)
length(files)
sentences_sum_list <- list()
for(file in files){
  try({
    filename = basename(file)
    df=data.frame(sentence=readLines(file))
    df$url_hash <- filename %>% str_replace(".txt",'')
    sentences_sum_list[[file]] <- df
  })
}

library(digest)
url_hash_df <- cuban_missile_clean_sentences %>% dplyr::select(url) %>% distinct() %>% rowwise() %>% mutate(url_hash=digest(url,'crc32')) %>% ungroup()

library(tidytext)
sentences_sum_df <- bind_rows(sentences_sum_list) %>%
                    left_join(url_hash_df) %>%
                     mutate(sentence=sentence %>% str_replace_all(" {1,}\\.",".") ) %>% #after summarizing it puts a space before the final punctuation
                     ungroup() %>%
                     mutate(sentence= sentence %>% str_replace_all("\n","  ") ) %>% #because pdfs are often parsed line by line
                     mutate(sentence= sentence %>% str_replace_all(" {1,}"," ") ) %>% 
                     unnest_tokens(sentence, sentence, token="sentences", drop=T, to_lower=F) %>%
                     mutate(sentence= sentence %>% str_replace_all("^.*?: ","") ) %>%
                     mutate(sentence= sentence %>% str_replace_all("- |¬ ","") ) %>% #words broken across pages
                     mutate(sentence= sentence %>% str_replace_all(" {1,}"," ") %>% trimws() ) %>%
                     mutate(sentence_nchar=sentence %>% nchar()) %>%
                     mutate(sentence= sentence %>% trimws()) %>%
                     filter(sentence_nchar>70 & sentence_nchar<600) %>%
                     filter(!str_detect(sentence, "downloaded|Downloaded|Table|Figure|University|pp\\.|New York| file | [0-9]{3} | [0-9]{3}(\\–|\\-)|Box |IEEE|Memorandum|Transcript|Telegram|Kew Gardens| [0-9]{2,3}(\\–|\\-)[0-9]{2,3}|CHAPTER| cited|=|Cuban Missile Crisis|Book|Copyright|FRUS|http|JSTOR|researchers|archival|scholarship|recordings|File|Foreign Relations of the United States|Printing Office|Document|Minutes of|Summary|Record|vol\\.|No\\.|Issue|Paper|à| et | est |United States Government|American Foreign Policy|FO |PREM| box|Part| et | est |Acknowledgements|website|cookies|Privacy")) %>%
                     mutate(sentence= sentence %>% str_replace_all(" {1,}"," ") %>% trimws() ) %>%
                     filter(sentence %>% str_detect("^[A-Z]")) %>% #has at least one capital letter
                     filter(sentence %>% str_detect("[a-z0-9]\\.$")) %>% #ends in a letter/number and a period no questions
                     #add_count(sentence) %>%
                     ungroup() %>%
                     mutate(dupe= sentence %>% tolower() %>% str_replace_all("[^a-z]","") %>% duplicated() ) %>%
                     filter(!dupe)
dim(sentences_sum_df) #796,090

cuban_missile_clean_sentences_withsum <- 
  bind_rows(cuban_missile_clean_sentences %>% dplyr::select(url, sentence),
            sentences_sum_df %>% dplyr::select(url, sentence) ) %>%
       mutate(dupe= sentence %>% tolower() %>% str_replace_all("[^a-z]","") %>% duplicated() ) %>%
       filter(!dupe)
dim(cuban_missile_clean_sentences_withsum) #1,238,303 #1267675

#
cuban_missile_clean_sentences_withsum %>% write_tsv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences_withsum.tsv")


```

```{r}

cuban_missile_clean_sentences <- read_tsv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences.tsv")

cuban_missile_clean_sentences_withsum <- read_tsv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences_withsum.tsv") %>%
  rowwise() %>%
    mutate(url_hash= digest(url, 'crc32') ) %>%
  ungroup() %>%
  mutate(url_int=url %>% as.factor() %>% as.integer())


```

```{python}

#Can't get this and spacy to work in the same run. Once reticulate loads once it sticks with the environment that came first and I can't figure out how to switch it.
#pip install -U sentence-transformers
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
fromscratch=True
if fromscratch:
  sentences=np.array(r.cuban_missile_clean_sentences_withsum.sentence) #sentences_unique_sanitized
  url_ints = np.array(r.cuban_missile_clean_sentences_withsum.url_int.values)
  sentence_chars = np.array([len(q) for q in sentences])
  #sentences=sentences[sentence_chars<1000] #cull anything with too many characters
  len(sentences) #648,266 #600148 #10383 12514
  n=len(sentences)
  chunk_length=200
  n_chunks = int(np.ceil(n/chunk_length)) #can only 
  sentences_chunks=[sentences[i:i + chunk_length] for i in range(0, n, chunk_length)]
  len(sentences_chunks)
  len(sentences_chunks[0])  
  #model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') 
  #model = SentenceTransformer('sentence-transformers/gtr-t5-xxl')
  #model = SentenceTransformer('sentence-transformers/gtr-t5-xl')
  model = SentenceTransformer('paraphrase-mpnet-base-v2')
  #model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')
  #
  #https://www.sbert.net/examples/applications/paraphrase-mining/README.html
  from sentence_transformers import SentenceTransformer, util
  #I don't know why but I had to set the query chunk size to smaller for it to work. I think it was silently failing.
  #sentences.shape 79,851  sentences, so 6,376,182,201 possible comparisons
  #We want a greedy matching across documents, pick a sentence, find its top pair in the next document, and so on
  #
  sentences_embeddings = np.vstack([model.encode(q) for q in sentences_chunks]).astype('float32')  #This is what takes a whle #[0:100] #cpu is possible just painfully slow #, device='0'
  sentences_embeddings.shape #1,267,675 #124,808 #78225
  d=sentences_embeddings.shape[1]
  import faiss
  faiss.omp_set_num_threads(64)
  faiss.normalize_L2(sentences_embeddings) #If you normalize then you do get the right
  # build a flat (CPU) index
  index_flat = faiss.IndexFlatIP(d) #make sure you use IP so you get cosine distance https://github.com/facebookresearch/faiss/issues/95
  # make it into a gpu index
  #https://github.com/facebookresearch/faiss/wiki/Running-on-GPUs
  res = faiss.StandardGpuResources()  # use a single GPU
  gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)  
  gpu_index_flat.add(sentences_embeddings)
  k = 30   #it runs out of memory with 600k times 50. 30 works. #You can do 25 and miss almost no pairs over 0.9
  D, I = gpu_index_flat.search(sentences_embeddings, k)  #With a million this is no longer instant but it is going hard
  np.quantile( D[:,0], [0.01,0.05,0.1,0.5,0.9,0.95,0.99] ) #should be all 1s ish
  #1568 articles. If they were all identical that would mean for each sentence there would be 1568 other matches and no others so 1568*number of sentences on average (37) 58,016
  #58016/124808 #0.46% is then the upper bound, it's actually much smaller. There are so many pairings it's like 0.0000037 of pairs would be matches if it was just the same doc over and over
  np.quantile( D[:,1], [0.01,0.05,0.1,0.5,0.9,0.95,0.99] ) #We have to decide something here about what our prior is on how many paraphrased sentences there should be in a corpus. 0.95 would mean 1 in 20
  condition=D[:,1:]>=0.87 #0.87 
  np.sum(condition) #56,447 #28,189 #7809 or 3904.5 pairs  #1460 #6,240,376 #2,496,146 #1,248,071 #624,036
  A=np.where(condition)[0]
  A_id=url_ints[A]
  B=I[:,1:][condition]
  B_id=url_ints[B]
  condition2 = A_id != B_id #add a second condition where we require both nodes to live in different url_hash which should be an integer
  sum(condition2) #170,404
  edges=np.transpose(np.vstack([A,B]))[condition2,:]
  weights=D[:,1:][condition][condition2]
  vertices=np.unique(np.hstack([edges[:,0],edges[:,1]]))
  #
  #ids_strongly = np.unique(np.hstack([paraphrases_strongly[:,1],paraphrases_strongly[:,2]] ) ).astype("int")
  #ids_strongly = np.setxor1d(ids_strongly,list(dead)).astype("int")
  #sentences_strongly = sentences[ids_strongly]
  #sentences_embeddings_strongly = sentences_embeddings[ids_strongly,:]
  #sentences_embeddings_strongly.shape #78,225 #78,474
  #paraphrases_strongly = util.paraphrase_mining(model, sentences_strongly, show_progress_bar=True, top_k=300, batch_size=128, query_chunk_size=1000, max_pairs=30000000) 
  #paraphrases_strongly_array=   np.vstack(paraphrases_strongly)
  #paraphrases_strongly_array = paraphrases_strongly_array[paraphrases_strongly_array[:,0]>0.70, :] #require a certain proximity
  #paraphrases_strongly_array.shape
  #subset=np.sort( np.unique( np.hstack([paraphrases_strongly_array[:,1],paraphrases_strongly_array[:,2]]) ) ).astype('int')
  #sentences_strongly=sentences_strongly[subset,]
  #sentences_embeddings_strongly=sentences_embeddings_strongly[subset,]
  #sentences_embeddings_strongly.shape
  #
  import igraph
  import pandas as pd
  edges_df=pd.DataFrame(edges.astype('int')) #
  edges_df['weights']=weights
  edges_df['from']=edges_df[0]
  edges_df['to']=edges_df[1]
  condition=edges_df[0]<edges_df[1]
  edges_df['from'][condition]=edges_df[1][condition]
  edges_df['to'][condition]=edges_df[0][condition]
  edges_df=edges_df[['from','to','weights']].drop_duplicates()
  #
  #  
  g = igraph.Graph.DataFrame(edges_df, directed=False) #.simplify()
  g.es.attribute_names()
  len(g.es) #92,800
  len(g.vs)  #55,831 #47,846 #32,258 #32,916 #47363 #32916 nodes
  clustered=igraph.Graph.community_fastgreedy(g, weights=edges_df.weights.values ) # #wow did pretty well #weights=paraphrases_strongly[:,0]
  clustered.optimal_count
  clustered_cuts=clustered.as_clustering()
  len(clustered_cuts.membership) #32916 #ok so it cut 2824 sentences into 1269 clusters
  import scipy
  tabs=scipy.stats.contingency.crosstab(clustered_cuts.membership)
  #with np.printoptions(threshold=np.inf):
  #  print(np.sort(tabs[1])) #ok so there are a few big ones but mostly very small
  #
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import genieclust
  import sklearn
  import umap
  x_train= sklearn.preprocessing.scale(sentences_embeddings[vertices,:]) #[0:10000,] #I think we have to scale it so euclidian distance is the same as cosine #_strongly
  x_train.shape
  x_train_umap = umap.UMAP().fit_transform(x_train) #
  #
  import fastcluster
  #clusters_ward = fastcluster.ward(x_train)
  from scipy.cluster.hierarchy import dendrogram
  #linkage = fastcluster.linkage_vector(x_train, method='ward', metric='euclidean')
  #labels = hierarchy.fcluster(linkage._Z, t=0.02, criterion='distance') - 1
  #dendrogram = hierarchy.dendrogram(linkage, no_plot=True, color_threshold=-np.inf)
  #                      
  #X=x_train                 
  #import scipy.cluster.hierarchy
  #g = genieclust.Genie(compute_full_tree=True)
  #g.fit(X)
  #linkage_matrix = np.column_stack([g.children_, g.distances_, g.counts_])
  #scipy.cluster.hierarchy.dendrogram(linkage_matrix,
  #    show_leaf_counts=False, no_labels=True)
  #plt.show()
  #g = genieclust.Genie(n_clusters=500, exact = True, compute_full_tree=False, affinity='cosinesimil', verbose=True) #, M=3
  #labels_genie = g.fit_predict(x_train) #it assigns non clusters to 0
  #np.max(labels_genie)
  #
  from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap
  import pandas as pd
  x_train_umap_df=pd.DataFrame(x_train_umap, columns=['X','Y'])
  #x_train_umap_df['labels_genie']=labels_genie
  x_train_umap_df['labels_community_fastgreedy']=clustered_cuts.membership
  #x_train_umap_df['labels_genie_pos']=labels_genie>0
  #x_train_umap_df['labels_genie_noise']=labels_genie<0
  x_train_umap_df['sentence']=sentences[vertices]
  x_train_umap_df['labels']=x_train_umap_df['labels_community_fastgreedy']
  (ggplot(x_train_umap_df, aes('X', 'Y', color='factor(labels_community_fastgreedy)' )  ) + geom_point(size=0.1, show_legend=False)   ) #it just runs out of colors
  #
  #genieclust.plots.plot_scatter(x_train_umap, labels=labels_genie, alpha=0.5)
  #plt.title("(n=%d, true n_clusters=%d)" % (X.shape[0], n_clusters))
  #plt.axis("equal")
  #plt.show()
  #
  #(ggplot(x_train_umap_df, aes('X', 'Y', color='factor(labels_genie_noise)' )  ) + geom_point(size=0.1, show_legend=False)   )
  #
  #paraphrases_array.shape
  #sentences
  #sentences_test=['That is a happy person','That is a happy dog','That is a not happy person','That person is happy','This person is happy']
  #temp = model.encode(sentences_chunks[0])
  #clusters = util.community_detection(sentences_embeddings, min_community_size=25, threshold=0.75) #if this is working, it's too slow to tell
  #
  #with open("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/temp/x_train_umap_df.pickle", "wb") as outfile:
  # 	# "wb" argument opens the file in binary mode
  #pickle.dump(x_train_umap_df, outfile)
#
#


```

```{r}

#x_train= py$x_train
#library('fastcluster')
#hc <- hclust.vector(x_train, method="ward", members=NULL, metric='euclidean', p=NULL)
#g = genieclust.Genie(compute_full_tree=True)

#install.packages('genieclust')
#library(genieclust)
#?gclust

#install.packages('gputools')
library(reticulate)
library(tidyverse)
cuban_missile_clean_sentences_withsum_clustered <- cuban_missile_clean_sentences_withsum %>% left_join(py$x_train_umap_df)
dim(cuban_missile_clean_sentences_withsum_clustered) #1,267,675 #648493
cuban_missile_clean_sentences_withsum_clustered %>% pull(url) %>% unique()  %>% length() #1995 #1,998

cuban_missile_clean_sentences_withsum_clustered %>% filter(!is.na(labels) & labels>=0) %>% pull(labels) %>% table()  %>% table()

cuban_missile_clean_sentences_withsum_clustered %>% filter(!is.na(labels) & labels>=0) %>% pull(labels) %>% table() %>%  sort() %>% tail(10)

cuban_missile_clean_sentences_withsum_clustered %>% filter(!is.na(labels) & labels>0) %>% pull(labels) %>% table() %>% as.vector() %>% summary()

cuban_missile_clean_sentences_withsum_clustered %>% filter(!is.na(labels) & labels>0) %>% pull(labels) %>% table() %>% as.vector() %>% table() 

cuban_case_study_events <- cuban_missile_clean_sentences_withsum_clustered  %>%
                            filter(!is.na(labels) & labels>=0) %>% #-1 are the noise ones
                            group_by(labels) %>%
                            summarise(
                              n_sources=url %>% unique() %>% length(),
                              sentence= sentence %>% paste(collapse="\n")#,
                              #position=median(position) %>% round(2) #we lost position but I was coding date by hand anyway now so
                              ) %>%
                            #arrange(n_sources %>% desc() ) %>%
                            arrange(n_sources) %>%
                            filter(n_sources>=5)
dim(cuban_case_study_events) #410

table(cuban_case_study_events$n_sources)

cuban_case_study_events$sentence %>% str_detect("defcon|DEFCON") %>% table()

table(cuban_case_study_events$n_sources)

```

```{python}

import pandas as pd
from transformers import pipeline
#summarizer = pipeline("summarization", device=0, model="facebook/bart-large-cnn")
#summarizer = pipeline("summarization", device=0, model="sshleifer/distilbart-xsum-12-6") #noooooo
txt="All major United States mi]itary commands were placed on DEFCON-3 on 22 October, with SAC moving to DEFCON-2 on 24 October. It now appears that the commander of United States Air Forces Europe, General Truman Landon, ordered numerous operational activities commensurate with DEFCON-3 alert status, including increasing the number of nuclear- armed aircraft on Quick Reaction Alert .73 Additionally, the alert status  of the IRBMs in Britain and Italy was at fifteen minutes readiness and the British V-bomber force was brought to a high state of ground alert. In times of peace the United States armed forces are usually on DefCon5. Following this meeting Harkness presented Diefenbaker with a report showing that the United States military had moved to DEFCON 2, meaning the Americans were making immediate preparations for war. United States forces went from \"peace alert to Defcon 3 (war alert), and strategic air command Defcon 2\" , five divisions of armed strategic reserves were placed on alert. Tactical strike aircraft in Florida maintained high levels of alert, anywhere from pilots in their cockpits ready to launch to as long as a thirty- minute alert. TAC forces in Florida assumed a one-hour alert and prepared to go to a fifteen-minute alert, which involved pilots waiting in aircraft for launch orders. On October 22 United States armed forces moved to DEFCON-3, a height-  ened state of alert, upgraded to DEFCON-2, only one level short of war,  on October 25. On October 26, he learned that the Pentagon had moved United States forces from DEFCON 5, peacetime status, to DEFCON 2, just one away from war, and that United States hospitals had been ordered to prepare to receive casualties. Military forces of both nations were placed on maximum alert and United States nuclear bombers were ordered into the air. For the first and only time in its existence, the Strategic Air Command had been ordered to Defcom 2 status, one step short of all-out war. Harkness returned to his office and learned that parts of the United States military had raised their alert status to DEFCON 2, meaning that an imminent attack was now expected. Squadrons of B-52 nuclear bombers lifted off from their bases to join the Strategic Air Command's airborne alert as a precaution against surprise attack. On 24 October the United States' Strategic Air Command was placed on DEFCON- 2, a state of alert denoting readiness for combat. A high alert was immediately announced for air forces to bomb the exile troop's ships. Until  20 November, the United States Strategic Air Command remained on alert at  Defence Condition 2 , other  forces were held at DefCon 3, and the naval quarantine was maintained in  place. DEFCON ranged from DEFCON 5, which was simple preparedness, to DEFCON 1, which was the most serious alert of all, the DEFCON for war. A massive airborne alert was begun by U.S.- based B-52 bombers which were loaded with nuclear weapons and by KC-135 tankers. Bombers, especially the new B-52s, provided a much more reliable way of delivering nuclear weapons against Soviet targets than ICBMs, but they are slow and—if detected as they approached Soviet air space—would allow the Soviets to launch air defense aircraft as well as launch or disperse their nuclear forces. On the morning of October 24, the United States quarantine went into place, and the United States military went to DEFCON 2, the last level before nuclear war. U.S. military forces worldwide, with the exception of the United States Air Forces in Europe , are placed on DEFCON 3. ICBM missile crews are alerted and Polaris nuclear submarines in port are dispatched to preassigned stations at sea. DEFCON 3 means an increase in readiness above normal levels, specifically that air forces are ready to deploy in 15 minutes. The United States Strategic Air Command placed all its B-52 intercontinental bombers on 15-minute takeoff alert on October 20; on October 22, it placed them on a revolving airborne alert, with a percentage of bombers airborne at all times, ready to head over the North Pole toward the Soviet Union. ICBM crews were also placed on highest alert, ready to launch, and nuclear-armed Polaris submarines moved to their pre-assigned war stations at sea. United States ground and air forces were put on full alert, with B-52s of the Strategic Air Command ready to attack on a moment's notice. THE QUARANTINE On October 22, in anticipation of a Cuban and/or Soviet reaction to the quarantine, the joint chiefs of staff placed United States military forces worldwide on DEFCON 3 alert. The Joint Chiefs of Staff announced a military readiness status of DEFCON 3 as United States naval forces began implementation of the quarantine and plans accelerated for a military strike on Cuba. With no apparent end to the crisis in sight, United States forces were placed at DEFCON 2—meaning war involving the Strategic Air Command was imminent. ICBMs were prepared for launch, Polaris submarines were dispatched, and B-52 bombers were placed on alert. US Forces Go to DEFCON 2 In light of the latest U-2 photos, and with no peaceful end to the crisis in sight, the Joint Chiefs of Staff placed United States forces at readiness level DEFCON 2; an indication that war involving the Strategic Air Command was imminent. B52 nuclear bombers were deployed, so that one-eighth of them were airborne all the time. United States nuclear-armed bombers were placed on airborne alert, and some of the Soviet missiles and bombers in Cuba were not under the direct control of senior leadership in Moscow and thus could have been launched by less cautious military officers. Hundreds of nuclear bombers were in the air, and intercontinental ballistic missiles were put on a short fuse."
#len(txt)

txt= "All major United States mi]itary commands were placed on DEFCON-3 on 22 October, with SAC moving to DEFCON-2 on 24 October."
txt= "The Soviet commander in Cuba was authorised to use tactical nuclear weapons against United States forces should they invade, and as Soviet missiles were already installed, any attack on Cuba could have led to the outbreak of nuclear war."
summarizer_distilbart = pipeline("summarization", device=0, model="sshleifer/distilbart-cnn-12-6") #This is actually better than bart someone
txt_summarized_distilbart = summarizer_distilbart(txt, min_length=10, max_length=40,length_penalty=3.0, truncation=False) #embarassingly slow #max_length=50, #, length_penalty=1500.0
txt_summarized_distilbart


#summarizer_distilbart = pipeline("summarization", device=0, model="sshleifer/distilbart-cnn-12-6") #This is actually better than bart someone
#txt_summarized_distilbart = summarizer_distilbart(txt, min_length=10, max_length=20, length_penalty=200) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#txt_summarized_distilbart

#summarizer_bart = pipeline("summarization", device=0, model="facebook/bart-large-cnn")
#txt_summarized_bart = summarizer_bart(txt, min_length=10, max_length=25, length_penalty=2.0) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#txt_summarized_bart

#Ok this is great. It splits long confusing sentences into individual ones. Perfect.
#summarizer_bart = pipeline("summarization", device=0, model="facebook/bart-large-cnn")
#txt_summarized_bart = summarizer_bart(txt, num_beams=4, min_length=10, max_length=40, length_penalty=20.0) #Maybe it breaks it into individual sentences which we can use
#txt_summarized_bart
#df= pd.DataFrame.from_dict(txt_summarized)


#summarizer_mt5 = pipeline("summarization", device=0, model="csebuetnlp/mT5_multilingual_XLSum") #lol just continues to hallucinate
#txt_summarized_mt5 = summarizer_mt5(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#txt_summarized_mt5

#T5 just absolutely hallucinates things.
#summarizer = pipeline("summarization", device=0, model="csebuetnlp/mT5_multilingual_XLSum")
#txt_summarized_mT5_multilingual_XLSum = summarizer(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#df_mT5_multilingual_XLSum= pd.DataFrame.from_dict(txt_summarized_mT5_multilingual_XLSum)


txt=[q[0:2000] for q in r.cuban_case_study_events['sentence'].values]  #first 2k characters
#Summarize with distilbart
summarizer = pipeline("summarization", device=0, model="sshleifer/distilbart-cnn-12-6") #This is actually better than bart someone
txt_summarized = summarizer(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
len(txt_summarized[0]['summary_text'])
#txt_summarized[0]['summary_text']
df= pd.DataFrame.from_dict(txt_summarized)

#Summarize with bart-large-cnn
summarizer = pipeline("summarization", device=0, model="facebook/bart-large-cnn")
txt_summarized_bart = summarizer(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
df_bart= pd.DataFrame.from_dict(txt_summarized_bart)



#summarizer = pipeline("summarization", device=0, model="google/pegasus-xsum") #requires a short 512 tokens
#txt_summarized_pegasus_xsum = summarizer(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#df_pegasus_xsum= pd.DataFrame.from_dict(txt_summarized_pegasus_xsum)

#summarizer = pipeline("summarization", device=0, model="google/pegasus-multi_news")
#txt_summarized_pegasus_multi_news = summarizer(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#df_pegasus_multi_news = pd.DataFrame.from_dict(txt_summarized_pegasus_multi_news)

#summarizer = pipeline("summarization", device=0, model="google/pegasus-cnn_dailymail")
#txt_summarized_pegasus_cnn_dailymail = summarizer(txt, min_length=50, max_length=100) #embarassingly slow #max_length=50, #, length_penalty=1500.0
#df_pegasus_cnn_dailymail = pd.DataFrame.from_dict(txt_summarized_pegasus_cnn_dailymail)




#Put on GPU
#https://github.com/huggingface/transformers/issues/2704

#txt=r.temp['sentence'].values[200]
#summarizer(txt[0:2], min_length=10, max_length=100) #crashes if I pass more than 1

#Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). Running this sequence through the model will result in indexing errors
#txt=[q[0:2000] for q in r.temp['sentence'].values] #limit to first 4k
#summaries = [summarizer(q, min_length=10, max_length=100)  for q in txt] #

```

```{r}

cuban_case_study_events$summary_text_distilbart_cnn_12_6 = py$df$summary_text
cuban_case_study_events$summary_text_bart_large = py$df_bart$summary_text
#cuban_case_study_events$summary_text_mT5_multilingual_XLSum = py$df_mT5_multilingual_XLSum$summary_text
#
cuban_case_study_events  %>% write_csv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_case_study_events.csv")
dim(cuban_case_study_events) #486   4
cuban_case_study_events$n_sources %>% summary()

```

```{python, eval=F}
  	
else:
  with open("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/temp/cuba_embeddings.pickle", "rb") as f:
    unpickler = pickle.Unpickler(f)
    # if file is not empty scores will be equal
    # to the value unpickled
    sentences_embeddings = unpickler.load()
    
#More extras

  #res = faiss.StandardGpuResources()
  quantizer = faiss.IndexFlatL2(d)  # the other index
  nlist = 100
  #index = faiss.IndexIVFFlat(quantizer, d, nlist)
  #index.train(sentences_embeddings.astype('float32'))
  #index.is_trained
  index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
  index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
  #index.add(sentences_embeddings.astype('float32'))
  #gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  
  #index.parallel_mode = 1
  #index.nprobe = 10
  distance, results = index.search(sentences_embeddings.astype('float32'), 5) #takes a while too
  #distance[:,1]
  #is it giving distance or similarity now?
  np.quantile( distance[:,1], [0.01,0.05,0.1,0.5,0.9,0.95,0.99] ) #you can see how the approximation perverts the distance #the median distance is .73 so that's good evidence we don't want to use that
  
  #It's fast because it's using torch.topk in the back
  #mining out high cor relationships is weird, crank top k and max pairs high and then threshold min cos later
  #This is a lot but it's still b
  #No matter what I do this takes about 2 min
  #paraphrases = paraphrase_mining_rex(model, sentences, show_progress_bar=True, top_k=500, batch_size=128*2, query_chunk_size=1000, max_pairs=60000000,min_score=0.8) #[0:2000] [0:10000] #60000000 400
  #len(paraphrases) #25,859,178 #19,490,957 #13,014,587
  #paraphrases_array=   np.vstack(paraphrases)
  #paraphrases_array.shape #25859178 #9,767,894
  #np.min(paraphrases_array[:,0])
  #condition= paraphrases_array[:,0]>0.8 #np.logical_and( ) #, paraphrases_array[:,0]<0.98
  #sum(condition) #2,038,562 #556,196 #545,160 #508242 #473,463
  #paraphrases_strongly = paraphrases_array[condition, :] #require a certain proximity
  #paraphrases_strongly.shape
  #np.min(paraphrases_array[:,0]) #the min it found was 0.7 already which is interesting
  #paraphrases_array.shape
  #We want to drop IDs that are too close
  #paraphrases_array_tooclose = paraphrases_array[paraphrases_array[:,0]>0.98, :] #thin any that are too close
  #paraphrases_array_tooclose.shape
  #protected=set()
  #dead=set()
  #for i in range(0,paraphrases_array_tooclose.shape[0]):
  #  a=int(paraphrases_array_tooclose[i,1])
  #  b=int(paraphrases_array_tooclose[i,2])
  #  if not a in protected and not a in dead and not b in protected and not b in dead: #if neither is assigned protect A
  #    protected.add(a)
  #  if a in protected and not a in dead and not b in protected and not b in dead: #
  #    dead.add(b)
  #  if not a in protected and not a in dead and b in dead: #if a isn't assigned but b is in dead protect a
  #    protected.add(a)
  #
  #
  #
#    
#Parallel version
#pool = model.start_multi_process_pool()
#emb = model.encode_multi_process(sentences, pool) #Compute the embeddings using the multi-process pool
#model.stop_multi_process_pool(pool) #Optional: Stop the proccesses in the pool
#
#sentences_embeddings.shape
#from sentence_transformers import SentenceTransformer, util
#cosine_scores = util.cos_sim(sentences_embeddings, sentences_embeddings)
#xxl
#  tensor([[1.0000, 0.7225, 0.8346, 0.9151, 0.9066],
#        [0.7225, 1.0000, 0.5697, 0.6481, 0.6713],
#        [0.8346, 0.5697, 1.0000, 0.7935, 0.7862],
#        [0.9151, 0.6481, 0.7935, 1.0000, 0.9403],
#        [0.9066, 0.6713, 0.7862, 0.9403, 1.0000]])
#xl
#tensor([[1.0000, 0.7127, 0.8432, 0.9232, 0.9157],
#        [0.7127, 1.0000, 0.5873, 0.6428, 0.6830],
#        [0.8432, 0.5873, 1.0000, 0.8099, 0.7972],
#        [0.9232, 0.6428, 0.8099, 1.0000, 0.9516],
#        [0.9157, 0.6830, 0.7972, 0.9516, 1.0000]])  
#sentences_embeddings = model.encode(sentences[0:1000]) #
#
#https://towardsdatascience.com/billion-scale-semantic-similarity-search-with-faiss-sbert-c845614962e2
from numba import cuda 
device = cuda.get_current_device()
device.reset()
import faiss
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
#faiss.write_index(index, 'abc_news')
#index = faiss.read_index('abc_news')
#import time
#def search(query):
# t=time.time()
# query_vector = model.encode([query])
# k = 5
# top_k = index.search(query_vector, k)
# print('totaltime: {}'.format(time.time()-t))
# return [sentences[_id] for _id in top_k[1].tolist()[0]]
#
n=sentences_embeddings.shape[0]
niter = 20 #doesn't improve after 20
verbose = True
d = sentences_embeddings.shape[1]

kmeans_125 = faiss.Kmeans(d, 125, niter=niter, verbose=verbose, gpu=True)
kmeans_125.train(sentences_embeddings)
D, cluster_125 = kmeans_125.index.search(sentences_embeddings, 1)

device.reset() #free up memory 
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
kmeans_250 = faiss.Kmeans(d, 250, niter=niter, verbose=verbose, gpu=True)
kmeans_250.train(sentences_embeddings)
D, cluster_250 = kmeans_250.index.search(sentences_embeddings, 1)

device.reset() #free up memory
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
kmeans_500 = faiss.Kmeans(d, 500, niter=niter, verbose=verbose, gpu=True)
kmeans_500.train(sentences_embeddings)
D, cluster_500 = kmeans_500.index.search(sentences_embeddings, 1)

device.reset() #free up memory
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
kmeans_1000 = faiss.Kmeans(d, 1000, niter=niter, verbose=verbose, gpu=True)
kmeans_1000.train(sentences_embeddings)
D, cluster_1000 = kmeans_1000.index.search(sentences_embeddings, 1)

device.reset() #free up memory
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
kmeans_2000 = faiss.Kmeans(d, 2000, niter=niter, verbose=verbose, gpu=True)
kmeans_2000.train(sentences_embeddings)
D, cluster_2000 = kmeans_2000.index.search(sentences_embeddings, 1)

device.reset() #free up memory
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
kmeans_4000 = faiss.Kmeans(d, 4000, niter=niter, verbose=verbose, gpu=True)
kmeans_4000.train(sentences_embeddings)
D, cluster_4000 = kmeans_4000.index.search(sentences_embeddings, 1)

device.reset() #free up memory
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
kmeans_8000 = faiss.Kmeans(d, 8000, niter=niter, verbose=verbose, gpu=True)
kmeans_8000.train(sentences_embeddings)
D, cluster_8000 = kmeans_8000.index.search(sentences_embeddings, 1)

device.reset() #free up memory 
res = faiss.StandardGpuResources()
index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) #Ip stands for inner product which is cosine on normalized vectors
index.add_with_ids(sentences_embeddings, np.array(range(0, len(sentences_embeddings))))
kmeans_16000 = faiss.Kmeans(d, 16000, niter=niter, verbose=verbose, gpu=True)
kmeans_16000.train(sentences_embeddings)
D, cluster_16000 = kmeans_16000.index.search(sentences_embeddings, 1)

device.reset() #free up memory 

```

```{r, eval=F}

paraphrases_df <- as.data.frame(py$paraphrases_array) %>% arrange(V2, desc(V1)) %>% setNames(c('similarity', 'v1', 'v2'))
paraphrases_df$sentence1 = as.vector(unlist(py$sentences)[paraphrases_df$v1+1]) #remember to account for indexing
paraphrases_df$sentence2 = as.vector(unlist(py$sentences)[paraphrases_df$v2+1]) #remember to account for indexing
dim(paraphrases_df) #112,793

#We want to go further and also drop dupes of any sentences that are near exacts of each other, keep just one copy of them
paraphrases_df_dupes <- paraphrases_df %>% filter(similarity>0.93) %>% 
                        mutate(sentence1b = ifelse(sentence1>sentence2, sentence1, sentence2)) %>% 
                        mutate(sentence2b = ifelse(sentence1>sentence2, sentence2, sentence1)) %>% 
                        mutate(sentence1 = sentence1b) %>% 
                        mutate(sentence2 = sentence2b) %>% 
                        arrange(sentence1) %>%
                        mutate(dupe=  duplicated(sentence1))
glimpse(paraphrases_df_dupes)

todrop <- c(paraphrases_df_dupes$sentence1[paraphrases_df_dupes$dupe], paraphrases_df_dupes$sentence2 ) %>% unique()
#length(todrop)

paraphrases_df_urls <- paraphrases_df %>%
                       filter(!sentence1 %in% todrop) %>%
                       filter(!sentence2 %in% todrop) %>%
                       left_join( cuban_missile_clean_sentences %>% dplyr::select(url1=url, sentence1=sentence) ) %>%
                       left_join( cuban_missile_clean_sentences %>% dplyr::select(url2=url, sentence2=sentence) ) %>%
                       filter(url1!=url2) %>% dplyr::select(-v1,-v2) %>% 
                       mutate(V1_sentence_ordered=ifelse(sentence1<sentence2,sentence1,sentence2)) %>%
                       mutate(V2_sentence_ordered=ifelse(sentence1<sentence2,sentence2,sentence1)) %>%
                       dplyr::select(similaritya=similarity,sentence1=V1_sentence_ordered,sentence2=V2_sentence_ordered,url1,url2 ) %>% 
                       filter(similaritya<0.93  & similaritya>0.5 & trimws(tolower(sentence1))!=trimws(tolower(sentence2)) ) 
dim(paraphrases_df_urls) #106,875 #the good news is most are from different documents

#This is actually really tricky, we want to do greedy clustering without ever looping back into the first document
#I think we can actually do this with a series of joins

agg1 <- paraphrases_df_urls %>% 
        group_by(sentence1) %>% filter(similaritya==max(similaritya)) %>% ungroup() %>% 
        group_by(sentence2) %>% filter(similaritya==max(similaritya)) %>% ungroup()

agg2 <- agg1 %>% 
        left_join(paraphrases_df_urls %>%
                     rename(similarityb=similaritya,
                            sentence2=sentence1,
                            sentence3=sentence2,
                            url2=url1,
                            url3=url2
                            )
                  ) %>%
        filter(is.na(url3) | (url3!= url1 & url3!=url2)) %>%
        mutate(similarity_total = similaritya + similarityb) %>% 
        group_by(sentence1) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence2) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence3) %>% filter(similarity_total==max(similarity_total)) %>% ungroup()

#On 26 October, in one of several letters he exchan
agg3 <- agg2 %>% 
        left_join(paraphrases_df_urls %>%
                     rename(similarityc=similaritya,
                            sentence3=sentence1,
                            sentence4=sentence2,
                            url3=url1,
                            url4=url2
                            )
                  ) %>%
        filter(url4!= url1 & url4!=url2 & url4!=url3 ) %>%
        mutate(similarity_total = similaritya + similarityb + similarityc) %>% 
        group_by(sentence1) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence2) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence3) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence4) %>% filter(similarity_total==max(similarity_total)) %>% ungroup()

agg4 <- agg3 %>% 
        left_join(paraphrases_df_urls %>%
                     rename(similarityd=similaritya,
                            sentence4=sentence1,
                            sentence5=sentence2,
                            url4=url1,
                            url5=url2
                            )
                  ) %>%
        filter(url5!= url1 & url5!=url2 & url5!=url3 & url5!=url4 ) %>%
        mutate(similarity_total = similaritya + similarityb + similarityc + similarityd)  %>% 
        group_by(sentence1) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence2) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence3) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence4) %>% filter(similarity_total==max(similarity_total)) %>% ungroup()

agg5 <- agg4 %>% 
        left_join(paraphrases_df_urls %>%
                     rename(similaritye=similaritya,
                            sentence5=sentence1,
                            sentence6=sentence2,
                            url5=url1,
                            url6=url2
                            )
                  ) %>%
        filter(url6!= url1 & url6!=url2 & url6!=url3 & url6!=url4  & url6!=url5 ) %>%
        mutate(similarity_total = similaritya + similarityb + similarityc + similarityd + similaritye) %>% 
        group_by(sentence1) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence2) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence3) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence4) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>% 
        group_by(sentence5) %>% filter(similarity_total==max(similarity_total)) %>% ungroup() %>%
        mutate(cluster=row_number())

agg5 %>% dplyr::select(sentence6) %>% View()

temp <- agg5 %>% dplyr::select(cluster, starts_with('sentence')) %>% pivot_longer(-c(cluster)) %>%
  left_join(cuban_missile_clean_sentences %>% dplyr::select(value=sentence, position))

temp_collapse <- temp %>%
  group_by(cluster) %>%
  summarise(sentence=paste(value, collapse="\n"), position=mean(position)) %>%
  arrange(position)

temp_collapse  %>% as.data.frame() %>% write_csv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/temp_collapse.csv")


library(igraph)
library(lsa) ; #install.packages('lsa')
#g = make_graph("Zachary")
g <- graph_from_data_frame(paraphrases_df_subset[,c('V2','V3')], directed = FALSE, vertices = NULL)
#coords = layout_with_fr(g)
# plot the graph
#plot(g, layout=coords, vertex.label=NA, vertex.size=10)
c1 = cluster_fast_greedy(g)
#modularity(c1)
m <- membership(c1)
vertices <- bind_rows(
  paraphrases_df_subset %>% dplyr::select(v=V2, sentence=V2_sentence),
  paraphrases_df_subset %>% dplyr::select(v=V3, sentence=V3_sentence)
) %>% distinct() %>% arrange(v)
vertices$cluster <- as.vector(m)

paraphrases_df_subset  %>% as.data.frame() %>% write_csv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/paraphrases_df_subset.csv")

```

```{r, eval=F}

df = data.frame(
  sentence=cuban_missile_clean_sentences_unique,
  cluster_125=py$cluster_125,
  cluster_250=py$cluster_250,
  cluster_500=py$cluster_500,
  cluster_1000=py$cluster_1000,
  cluster_2000=py$cluster_2000,
  cluster_4000=py$cluster_4000,
  cluster_8000=py$cluster_8000,
  cluster_16000=py$cluster_16000
) %>%
  #add_count(cluster_1000) %>% 
  arrange(cluster_125, cluster_250, cluster_500, cluster_1000, cluster_2000,cluster_4000, cluster_8000,cluster_16000) #
#df$cluster_1000 %>% table() %>% sort()


#df %>% filter(cluster_1000==257)

cuban_missile_clean_sentences_clustered <- cuban_missile_clean_sentences %>% left_join(df) %>%
        arrange(cluster_125, cluster_250, cluster_500, cluster_1000, cluster_2000,cluster_4000, cluster_8000,cluster_16000)
saveRDS(cuban_missile_clean_sentences_clustered, "/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences_clustered.Rds" )
```

It gets a little crash so it's best to do the above and then load a save from earlier, faster too

```{r, eval=F}

library(tidyverse)
cuban_missile_clean_sentences_clustered <- readRDS( "/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_missile_clean_sentences_clustered.Rds" )


temp <- cuban_missile_clean_sentences_clustered  %>%
        #mutate(icb=url %>% str_detect("ICB")) %>%
        #mutate(sentence= ifelse(icb, paste("ICB- ", sentence) , sentence)) %>%
        #arrange(cluster_125, cluster_250, cluster_500, cluster_1000, cluster_2000,cluster_4000, cluster_8000,cluster_16000) %>% #somehow this arrange crashed it?
        group_by(cluster_16000) %>%
        summarise(
          n_sources=url %>% unique() %>% length(),
          sentence= sentence %>% paste(collapse="\n"),
          position=median(position)
          ) %>%
        arrange(n_sources %>% desc() ) %>%
        filter(n_sources>3)

temp  %>% write_csv("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/out/cuban_case_study_events.csv")
dim(temp) #1019
temp$n_sources %>% summary()

```


```{python, eval=F}
  #print(out)
  
len(txt_summarized_distilbart)
#txt_summarized_bart
df_distilbart= pd.DataFrame.from_dict(txt_summarized_distilbart)

txt= "The Soviet commander in Cuba was authorised to use tactical nuclear weapons against United States forces should they invade, and as Soviet missiles were already installed, any attack on Cuba could have led to the outbreak of nuclear war."
summarizer_distilbart = pipeline("summarization", device=0, model="sshleifer/distilbart-cnn-12-6") #This is actually better than bart someone
txt_summarized_distilbart = summarizer_distilbart(txt, min_length=10, max_length=20,length_penalty=3.0, truncation=False) #embarassingly slow #max_length=50, #, length_penalty=1500.0
txt_summarized_distilbart
#' Soviet commander in Cuba authorised to use tactical nuclear weapons against U.S. forces if they invaded . Any attack on Cuba could have led to the outbreak of nuclear war .'



#https://github.com/vaibhavad/python-wrapper-OpenIE5
import pandas as pd
from pyopenie import OpenIE5
extractor = OpenIE5('http://localhost:8000')
extractions = extractor.extract("The U.S. president Barack Obama gave his speech to thousands of people.")
with open("/mnt/8tb_a/rwd_github_private/ICBEdataset/replication_paper/data/in/case_studies/openie_sentences.txt", 'a', encoding = 'utf-8') as f:
  for k in range(0,r.cuban_missile_clean_sentences.shape[0] ):
    extraction=extractor.extract(r.cuban_missile_clean_sentences.sentence.values[k])
    df=pd.json_normalize(extraction)
    for i in range(0,df.shape[0]):
      url=r.cuban_missile_clean_sentences.url.values[k]
      txt=" ".join([df['extraction.arg1.text'].values[i], df['extraction.rel.text'].values[i], " ".join([q['text'] for q in df['extraction.arg2s'].values[i] ])  ])
      f.write(url + '\t' + txt + '\n')
    
    

import spacy                                                                                                                                               
import claucy                                                                                                                                               
nlp = spacy.load("en_core_web_sm")
claucy.add_to_pipe(nlp)                                                                                                                                     
doc = nlp("AE died in Princeton in 1955.")    
doc = nlp("Senior Kennedy administration officials, with the exception of independent CIA director John Mc-Cone, had assumed Moscow would never put long-range missiles into Cuba.")    
doc = nlp("The Soviet commander in Cuba was authorised to use tactical nuclear weapons against United States forces should they invade, and as Soviet missiles were already installed, any attack on Cuba could have led to the outbreak of nuclear war.")    

doc._.clauses                                                                                                                                               
Out[6]: [<SV, AE, died, None, None, None, [in Princeton, in 1955]>]
propositions = doc._.clauses[0].to_propositions(as_text=True)                                                                                               
propositions                   

from allennlp.predictors.predictor import Predictor
import allennlp_models.tagging

predictor = Predictor.from_path("https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz")
predictor.predict(sentence="In December, John decided to join the party.")

```

